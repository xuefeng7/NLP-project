{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import string\n",
    "import numpy as np; np.random.seed(7)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import re\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_map = {}\n",
    "with open('data/glove.6B.200d.txt', 'r') as src:\n",
    "    src = src.read().strip().split('\\n')\n",
    "    for line in src:\n",
    "        wv = line.strip().split(' ')\n",
    "        word = wv.pop(0)\n",
    "        w2v_map[word] = np.array(list(map(float, wv)))\n",
    "        \n",
    "w2i_map = {}\n",
    "w2v_matrix = np.zeros(( len((w2v_map.keys())), 200 ))\n",
    "for i, (key, val) in enumerate(w2v_map.items()):\n",
    "    w2i_map[key] = i\n",
    "    w2v_matrix[i] = val\n",
    "\n",
    "def w2v(w):\n",
    "    return w2v_matrix[w2i_map[w]]\n",
    "    \n",
    "def sen2w(sen):\n",
    "    processed = []\n",
    "    sen = sen.strip().split()\n",
    "    if len(sen) > 100:\n",
    "        sen = sen[:100]\n",
    "    for w in sen:\n",
    "        #ignore date\n",
    "        if re.match(r'\\d{1,}-\\d{1,}-\\d{1,}', w):\n",
    "            continue\n",
    "        if re.match(r'\\d{1,}:\\d{1,}', w):\n",
    "            continue\n",
    "        \n",
    "        if w in w2i_map:\n",
    "            processed += [w]\n",
    "        else:\n",
    "            separated = re.findall(r\"[^\\W\\d_]+|\\d+|[=`%$\\^\\-@;\\[&_*>\\].<~|+\\d+]\", w)\n",
    "            if len(set(separated)) == 1:\n",
    "                continue\n",
    "            if separated.count('*') > 3 or separated.count('=') > 3:\n",
    "                continue\n",
    "            for separate_w in separated:\n",
    "                if separate_w in w2i_map:\n",
    "                    processed += [separate_w]\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_context_repre(path):\n",
    "    context_repre = {}\n",
    "    with open('data/' + path, 'r') as src:\n",
    "        src = src.read().strip().split('\\n')\n",
    "        for line in src:\n",
    "            context = line.strip().split('\\t')\n",
    "            qid = context.pop(0)\n",
    "            if len(context) == 1:\n",
    "                context_repre[int(qid)] = {'t': sen2w(context[0]), 'b': None}\n",
    "            else:\n",
    "                context_repre[int(qid)] = {'t':sen2w(context[0]), 'b': sen2w(context[1])}\n",
    "    return context_repre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_set_pair_with_idx(df):\n",
    "    idx_set = {}\n",
    "    for idx, row in df.iterrows():\n",
    "        idx_set[row['Q']] = {'pos': np.array(list(map(int, row['Q+'].split(' ')))), \\\n",
    "                             'neg': np.array(list(map(int, row['Q-'].split(' '))))}\n",
    "    return idx_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_android_set(pos_path, neg_path):\n",
    "    \n",
    "    idx_set = {}\n",
    "    \n",
    "    pos_file = open('data/' + pos_path, 'r')\n",
    "    pos_src = pos_file.read().strip().split('\\n')\n",
    "    \n",
    "    neg_file = open('data/' + neg_path, 'r')\n",
    "    neg_src = neg_file.read().strip().split('\\n')\n",
    "    \n",
    "    for pos in pos_src:\n",
    "        pos = list(map(int, pos.split(' ')))\n",
    "        if pos[0] in idx_set:\n",
    "            idx_set[pos[0]]['pos'] += [pos[1]]\n",
    "        else:\n",
    "            idx_set[pos[0]] = {}\n",
    "            idx_set[pos[0]]['pos'] = [pos[1]]\n",
    "            idx_set[pos[0]]['neg'] = []\n",
    "         \n",
    "    for neg in neg_src:\n",
    "        neg = list(map(int, neg.split(' ')))\n",
    "        idx_set[neg[0]]['neg'] += [neg[1]]\n",
    "\n",
    "    \n",
    "    pos_file.close()\n",
    "    neg_file.close()\n",
    "    \n",
    "    return idx_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def contxt2vec(title, body=None):\n",
    "    \n",
    "    if body == None:\n",
    "        body = []\n",
    "    \n",
    "    title_v = np.zeros( (len(title), 200) )\n",
    "    \n",
    "    for i, t in enumerate(title):\n",
    "        title_v[i] = w2v(t)\n",
    "    \n",
    "    if len(body) > 0:\n",
    "        body_v = np.zeros( (len(body), 200) )\n",
    "        for i, b in enumerate(body):\n",
    "            body_v[i] = w2v(b)\n",
    "    \n",
    "        return title_v, body_v\n",
    "    \n",
    "    return title_v, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create random batch\n",
    "def sample_contxt_batch(context_repre, sample_size=128, batch_first=False):\n",
    "    \n",
    "    sampled_qids = np.random.choice(list(context_repre.keys()), sample_size)\n",
    "    \n",
    "    batch_title, batch_body = [], []\n",
    "    max_title_len, max_body_len = 0, 0\n",
    "    title_len, body_len = [], []\n",
    "    \n",
    "    for qid in sampled_qids:\n",
    "        \n",
    "        title, body = context_repre[qid]['t'], context_repre[qid]['b']\n",
    "        \n",
    "        title_len += [len(title)]\n",
    "        batch_title += [ title ]\n",
    "        max_title_len = max(max_title_len, len(title))\n",
    "        \n",
    "        if not body:\n",
    "            body_len += [len(title)]\n",
    "            batch_body += [ title ]\n",
    "        else:\n",
    "            batch_body += [ body ]\n",
    "            body_len += [len(body)]\n",
    "            max_body_len = max(max_body_len, len(body))\n",
    "        \n",
    "    if batch_first:\n",
    "        # for CNN\n",
    "        padded_batch_title = np.zeros(( len(batch_title), max_title_len, 200)) \n",
    "        padded_batch_body = np.zeros(( len(batch_body),  max_body_len, 200))\n",
    "        for i, (title, body) in enumerate(zip(batch_title, batch_body)):\n",
    "            title_repre, body_repre = contxt2vec(title, body)\n",
    "            padded_batch_title[i, :title_len[i]] = title_repre\n",
    "            padded_batch_body[i, :body_len[i]] = body_repre\n",
    "    else:\n",
    "        # for LSTM\n",
    "        # (max_seq_len, batch_size, feature_len)\n",
    "        padded_batch_title = np.zeros(( max_title_len, len(batch_title), 200)) \n",
    "        padded_batch_body = np.zeros(( max_body_len, len(batch_body),  200))\n",
    "        for i, (title, body) in enumerate(zip(batch_title, batch_body)):\n",
    "            title_repre, body_repre = contxt2vec(title, body)\n",
    "            padded_batch_title[:title_len[i], i] = title_repre\n",
    "            padded_batch_body[:body_len[i], i] = body_repre\n",
    "\n",
    "    return padded_batch_title, padded_batch_body, \\\n",
    "                np.array(title_len).reshape(-1,1), np.array(body_len).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create batch with order\n",
    "def process_contxt_batch(qids, idx_set, context_repre, batch_first=False):\n",
    "    \n",
    "    batch_title, batch_body = [], []\n",
    "    max_title_len, max_body_len = 0, 0\n",
    "    title_len, body_len = [], []\n",
    "    \n",
    "    for qid in qids:\n",
    "        \n",
    "        q_title, q_body = context_repre[qid]['t'], context_repre[qid]['b']\n",
    "        q_pos = idx_set[qid]['pos']\n",
    "        \n",
    "        if len(q_pos) > 20:\n",
    "            q_pos = q_pos[:20]\n",
    "\n",
    "        for qid_pos in q_pos:\n",
    "            # query Q\n",
    "            title_len += [len(q_title)]\n",
    "            batch_title += [ q_title ]\n",
    "            max_title_len = max(max_title_len, len(q_title))\n",
    "            if not q_body:\n",
    "                body_len += [len(q_title)]\n",
    "                batch_body += [ q_title ]\n",
    "            else:\n",
    "                batch_body += [ q_body ]\n",
    "                body_len += [len(q_body)]\n",
    "                max_body_len = max(max_body_len, len(q_body))\n",
    "                \n",
    "            # pos Q\n",
    "            title, body = context_repre[qid_pos]['t'], context_repre[qid_pos]['b']\n",
    "            title_len += [len(title)]\n",
    "            batch_title += [ title ]\n",
    "            max_title_len = max(max_title_len, len(title))\n",
    "            if not body:\n",
    "                body_len += [len(title)]\n",
    "                batch_body += [ title ]\n",
    "            else:\n",
    "                batch_body += [ body ]\n",
    "                body_len += [len(body)]\n",
    "                max_body_len = max(max_body_len, len(body))\n",
    "            # neg Q\n",
    "            q_neg = idx_set[qid]['neg']\n",
    "            q_neg_sample_indices = np.random.choice(range(100), size=20)\n",
    "            q_random_neg = q_neg[q_neg_sample_indices]\n",
    "            \n",
    "            for qid_neg in q_random_neg:\n",
    "                title, body = context_repre[qid_neg]['t'], context_repre[qid_neg]['b']\n",
    "                title_len += [len(title)]\n",
    "                batch_title += [ title ]\n",
    "                max_title_len = max(max_title_len, len(title))\n",
    "                if not body:\n",
    "                    body_len += [len(title)]\n",
    "                    batch_body += [ title ]\n",
    "                else:\n",
    "                    batch_body += [ body ]\n",
    "                    body_len += [len(body)]\n",
    "                    max_body_len = max(max_body_len, len(body))\n",
    "    \n",
    "    if batch_first:\n",
    "        # for CNN\n",
    "        padded_batch_title = np.zeros(( len(batch_title), max_title_len, 200)) \n",
    "        padded_batch_body = np.zeros(( len(batch_body),  max_body_len, 200))\n",
    "        for i, (title, body) in enumerate(zip(batch_title, batch_body)):\n",
    "            title_repre, body_repre = contxt2vec(title, body)\n",
    "            padded_batch_title[i, :title_len[i]] = title_repre\n",
    "            padded_batch_body[i, :body_len[i]] = body_repre\n",
    "    else:\n",
    "        # for LSTM\n",
    "        # (max_seq_len, batch_size, feature_len)\n",
    "        padded_batch_title = np.zeros(( max_title_len, len(batch_title), 200)) \n",
    "        padded_batch_body = np.zeros(( max_body_len, len(batch_body),  200))\n",
    "        for i, (title, body) in enumerate(zip(batch_title, batch_body)):\n",
    "            title_repre, body_repre = contxt2vec(title, body)\n",
    "            padded_batch_title[:title_len[i], i] = title_repre\n",
    "            padded_batch_body[:body_len[i], i] = body_repre\n",
    "\n",
    "    return padded_batch_title, padded_batch_body, \\\n",
    "                np.array(title_len).reshape(-1,1), np.array(body_len).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LAMDA = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradReverse(torch.autograd.Function):\n",
    "    def forward(self, x):\n",
    "        return x.view_as(x)\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        return (grad_output * -LAMDA) # need tune\n",
    "\n",
    "def grad_reverse(x):\n",
    "    return GradReverse()(x)\n",
    "\n",
    "class DomainClassifer(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        \n",
    "        super(DomainClassifer, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.domain_classifier = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, num_classes),\n",
    "            nn.LogSoftmax()\n",
    "        )\n",
    "\n",
    "    def forward(self, embedding):\n",
    "        embedding = grad_reverse(embedding)\n",
    "        return self.domain_classifier(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EmbeddingLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, layer_type, num_layer=1, kernel_size=3):\n",
    "        \n",
    "        super(EmbeddingLayer, self).__init__()\n",
    "        \n",
    "        self.num_layer = num_layer\n",
    "        \n",
    "        self.layer_type = layer_type\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.kernel_size = kernel_size\n",
    "        \n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "        if self.layer_type == 'lstm':\n",
    "            \n",
    "            self.embedding_layer = nn.LSTM(self.input_size, hidden_size, bidirectional=True)\n",
    "        \n",
    "        elif self.layer_type == 'cnn':\n",
    "\n",
    "            self.embedding_layer = nn.Sequential(\n",
    "                        nn.Conv1d(in_channels = self.input_size,\n",
    "                                  out_channels = self.hidden_size,\n",
    "                                  kernel_size = self.kernel_size),\n",
    "                        self.tanh)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return (Variable(torch.zeros(self.num_layer*2, batch_size, self.hidden_size)), \\\n",
    "                Variable(torch.zeros(self.num_layer*2, batch_size, self.hidden_size)))\n",
    "\n",
    "    def forward(self, title, body, title_len, body_len):\n",
    "        \n",
    "            \n",
    "        if self.layer_type == 'lstm':\n",
    "            \n",
    "            title_mask = Variable(torch.FloatTensor(build_mask3d(title_len, np.max(title_len))))\n",
    "            body_mask = Variable(torch.FloatTensor(build_mask3d(body_len, np.max(body_len))))\n",
    "            \n",
    "            \n",
    "            title_out, self.title_hidden = self.embedding_layer(title, (self.tanh(self.title_hidden[0]), \\\n",
    "                                                                   self.tanh(self.title_hidden[1])))\n",
    "            body_out, self.body_hidden = self.embedding_layer(body, (self.tanh(self.body_hidden[0]), \\\n",
    "                                                                   self.tanh(self.body_hidden[1])))\n",
    "        \n",
    "        if self.layer_type == 'cnn':\n",
    "            # batch first input\n",
    "            title_mask = Variable(torch.FloatTensor(build_mask3d(title_len - self.kernel_size + 1,\\\n",
    "                                                                 np.max(title_len) - self.kernel_size + 1)))\n",
    "            body_mask = Variable(torch.FloatTensor(build_mask3d(body_len - self.kernel_size + 1, \\\n",
    "                                                                np.max(body_len) - self.kernel_size + 1)))\n",
    "            \n",
    "            title = torch.transpose(title, 1, 2)\n",
    "            body = torch.transpose(body, 1, 2)\n",
    "\n",
    "            title_out =  self.embedding_layer(title)\n",
    "            body_out =  self.embedding_layer(body)\n",
    "\n",
    "            title_out = torch.transpose(title_out, 1, 2)\n",
    "            body_out = torch.transpose(body_out, 1, 2)\n",
    "        \n",
    "            title_out = torch.transpose(title_out, 0, 1)\n",
    "            body_out = torch.transpose(body_out, 0, 1)\n",
    "\n",
    "\n",
    "        title_embeddings = torch.sum(title_out * title_mask, dim=0) / torch.sum(title_mask, dim=0)\n",
    "        body_embeddings = torch.sum(body_out * body_mask, dim=0) / torch.sum(body_mask, dim=0)\n",
    "        embeddings = ( title_embeddings + body_embeddings ) / 2\n",
    "        \n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_mask3d(seq_len, max_len):\n",
    "    mask = np.zeros((max_len, len(seq_len), 1))\n",
    "    for i, s in enumerate(seq_len):\n",
    "        # only one word\n",
    "        if int(s) == -1:\n",
    "            mask[0, i] = 1\n",
    "        # only two word\n",
    "        elif int(s) == 0:\n",
    "            mask[:2, i] = np.ones((2, 1))\n",
    "        else: \n",
    "            mask[:int(s), i] = np.ones((int(s), 1))\n",
    "    return mask\n",
    "\n",
    "def multi_margin_loss(hidden, margin=0.50):\n",
    "    \n",
    "    def loss_func(embeddings):\n",
    "        # a batch of embeddings\n",
    "        blocked_embeddings = embeddings.view(-1, 22, hidden)\n",
    "        q_vecs = blocked_embeddings[:,0,:]\n",
    "        \n",
    "        pos_vecs = blocked_embeddings[:,1,:]\n",
    "        neg_vecs = blocked_embeddings[:,2:,:]\n",
    "\n",
    "        pos_scores = torch.sum(q_vecs * pos_vecs, dim=1) / (torch.sqrt(torch.sum(q_vecs ** 2, dim=1)) \\\n",
    "                                                   * torch.sqrt(torch.sum(pos_vecs ** 2, dim=1)))\n",
    "        neg_scores = torch.sum(torch.unsqueeze(q_vecs, dim=1) * neg_vecs, dim=2) \\\n",
    "        / (torch.unsqueeze(torch.sqrt(torch.sum(q_vecs ** 2, dim=1)),dim=1) * torch.sqrt(torch.sum( neg_vecs ** 2, dim=2)))\n",
    "        neg_scores = torch.max(neg_scores, dim=1)[0]\n",
    "        \n",
    "        diff = neg_scores - pos_scores + margin\n",
    "        loss = torch.mean((diff > 0).float() * diff)\n",
    "        return loss\n",
    "\n",
    "    return loss_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train( \n",
    "    embedding_layer, domain_classifier, \n",
    "    emb_batch_size=25, dc_batch_size=25,\n",
    "    num_epoch=100, lamda=1e-3,\n",
    "    id_set=None,train_from=None,sample_from=None,\n",
    "    eval=True\n",
    "    ):\n",
    "    \n",
    "    if embedding_layer.layer_type == 'lstm':\n",
    "        \n",
    "        margin_criterion = multi_margin_loss(hidden=embedding_layer.hidden_size * 2)\n",
    "    \n",
    "    elif embedding_layer.layer_type == 'cnn':\n",
    "        \n",
    "        margin_criterion = multi_margin_loss(hidden=embedding_layer.hidden_size)\n",
    "        \n",
    "    domain_criterion = torch.nn.NLLLoss()\n",
    "    \n",
    "    emb_optimizer = torch.optim.Adam(embedding_layer.parameters(), lr=0.001)\n",
    "    domain_optimizer = torch.optim.Adam(domain_classifier.parameters(), lr=0.001)\n",
    "    \n",
    "    qids = list(id_set.keys())\n",
    "    num_batch = len(qids) // emb_batch_size\n",
    "    \n",
    "    for epoch in range(1, num_epoch + 1):\n",
    "        \n",
    "        for batch_idx in range(1, num_batch + 1):\n",
    "            \n",
    "            batch_x_qids = qids[ ( batch_idx - 1 ) * emb_batch_size: batch_idx * emb_batch_size ]\n",
    "            \n",
    "            ## Minimize margin loss\n",
    "            if embedding_layer.layer_type == 'lstm':\n",
    "                batch_title, batch_body, title_len, body_len = process_contxt_batch(batch_x_qids, \\\n",
    "                                                                                id_set, train_from)\n",
    "                embedding_layer.title_hidden = embedding_layer.init_hidden(batch_title.shape[1])\n",
    "                embedding_layer.body_hidden = embedding_layer.init_hidden(batch_body.shape[1])\n",
    "            else:\n",
    "                batch_title, batch_body, title_len, body_len = process_contxt_batch(batch_x_qids, \\\n",
    "                                                                                id_set, train_from, batch_first=True)\n",
    "            \n",
    "            src_title_qs = Variable(torch.FloatTensor(batch_title))\n",
    "            src_body_qs = Variable(torch.FloatTensor(batch_body))\n",
    "            src_embeddings = embedding_layer(src_title_qs, src_body_qs, title_len, body_len) # class label = ubuntu\n",
    "            \n",
    "            margin_loss = margin_criterion(src_embeddings)\n",
    "            \n",
    "            ## Domain classification\n",
    "            if embedding_layer.layer_type == 'lstm':\n",
    "                batch_title, batch_body, title_len, body_len = sample_contxt_batch(sample_from, \\\n",
    "                                                                                   sample_size=dc_batch_size)\n",
    "                embedding_layer.title_hidden = embedding_layer.init_hidden(batch_title.shape[1])\n",
    "                embedding_layer.body_hidden = embedding_layer.init_hidden(batch_body.shape[1])\n",
    "            else:\n",
    "                batch_title, batch_body, title_len, body_len = sample_contxt_batch(sample_from, \\\n",
    "                                                                                   batch_first=True)\n",
    "            \n",
    "            # sample title, body from android dataset\n",
    "            target_title_qs = Variable(torch.FloatTensor(batch_title))\n",
    "            target_body_qs = Variable(torch.FloatTensor(batch_body))\n",
    "            # class label = android\n",
    "            target_embeddings = embedding_layer(target_title_qs, target_body_qs, title_len, body_len) \n",
    "            embedding_X = torch.cat((src_embeddings[:dc_batch_size], target_embeddings), 0)\n",
    "            \n",
    "            src_label = torch.zeros(dc_batch_size).type(torch.LongTensor)\n",
    "            target_label = torch.ones(dc_batch_size).type(torch.LongTensor)\n",
    "            embedding_Y = torch.cat((src_label, target_label), 0)\n",
    "            \n",
    "            # prepare for shuffle\n",
    "            train_loader = DataLoader(TensorDataset(embedding_X.data, embedding_Y), \\\n",
    "                                      batch_size=embedding_Y.size(0), shuffle=True)\n",
    "            \n",
    "            for x, y in train_loader:\n",
    "                domain_loss = domain_criterion(domain_classifier(Variable(x)), Variable(y))\n",
    "            \n",
    "            \n",
    "            loss = margin_loss - lamda * domain_loss\n",
    "            print ('epoch:{}/{}, batch:{}/{}, loss:{}'.format(epoch, num_epoch, \\\n",
    "                                                              batch_idx, num_batch, loss.data[0]))\n",
    "            \n",
    "            emb_optimizer.zero_grad()\n",
    "            domain_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            emb_optimizer.step()\n",
    "            domain_optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ubuntu_train_df = pd.read_csv('data/train_random.txt', header=None, delimiter='\\t', names=['Q','Q+','Q-'])\n",
    "ubuntu_train_idx_set = build_set_pair_with_idx(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "android_dev_idx_set = read_android_set('android/dev.pos.txt', 'android/dev.neg.txt')\n",
    "android_test_idx_set = read_android_set('android/test.pos.txt', 'android/test.neg.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ubuntu_context_repre = build_context_repre('text_tokenized.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "android_context_repre =  build_context_repre('android/corpus.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1/100, batch:1/508, loss:0.5144846439361572\n",
      "epoch:1/100, batch:2/508, loss:0.5101043581962585\n",
      "epoch:1/100, batch:3/508, loss:0.5098358392715454\n",
      "epoch:1/100, batch:4/508, loss:0.5008068680763245\n",
      "epoch:1/100, batch:5/508, loss:0.5049743056297302\n",
      "epoch:1/100, batch:6/508, loss:0.5006970167160034\n",
      "epoch:1/100, batch:7/508, loss:0.4971328377723694\n",
      "epoch:1/100, batch:8/508, loss:0.4975408613681793\n",
      "epoch:1/100, batch:9/508, loss:0.49646589159965515\n",
      "epoch:1/100, batch:10/508, loss:0.4962424337863922\n",
      "epoch:1/100, batch:11/508, loss:0.5003198981285095\n",
      "epoch:1/100, batch:12/508, loss:0.5003091096878052\n",
      "epoch:1/100, batch:13/508, loss:0.4962831437587738\n",
      "epoch:1/100, batch:14/508, loss:0.4965842366218567\n",
      "epoch:1/100, batch:15/508, loss:0.4998915493488312\n",
      "epoch:1/100, batch:16/508, loss:0.49837401509284973\n",
      "epoch:1/100, batch:17/508, loss:0.4994613230228424\n",
      "epoch:1/100, batch:18/508, loss:0.49879372119903564\n",
      "epoch:1/100, batch:19/508, loss:0.49664756655693054\n",
      "epoch:1/100, batch:20/508, loss:0.49708154797554016\n",
      "epoch:1/100, batch:21/508, loss:0.49826180934906006\n",
      "epoch:1/100, batch:22/508, loss:0.4989047646522522\n",
      "epoch:1/100, batch:23/508, loss:0.4974585771560669\n",
      "epoch:1/100, batch:24/508, loss:0.4965941607952118\n",
      "epoch:1/100, batch:25/508, loss:0.49753686785697937\n",
      "epoch:1/100, batch:26/508, loss:0.49714672565460205\n",
      "epoch:1/100, batch:27/508, loss:0.4969492554664612\n",
      "epoch:1/100, batch:28/508, loss:0.4965190887451172\n",
      "epoch:1/100, batch:29/508, loss:0.49690350890159607\n",
      "epoch:1/100, batch:30/508, loss:0.4957118332386017\n",
      "epoch:1/100, batch:31/508, loss:0.49897125363349915\n",
      "epoch:1/100, batch:32/508, loss:0.49809926748275757\n",
      "epoch:1/100, batch:33/508, loss:0.4954405725002289\n",
      "epoch:1/100, batch:34/508, loss:0.4952699840068817\n",
      "epoch:1/100, batch:35/508, loss:0.4982524514198303\n",
      "epoch:1/100, batch:36/508, loss:0.4957774877548218\n",
      "epoch:1/100, batch:37/508, loss:0.49418947100639343\n",
      "epoch:1/100, batch:38/508, loss:0.49666640162467957\n",
      "epoch:1/100, batch:39/508, loss:0.49380165338516235\n",
      "epoch:1/100, batch:40/508, loss:0.4935462772846222\n",
      "epoch:1/100, batch:41/508, loss:0.4903101623058319\n",
      "epoch:1/100, batch:42/508, loss:0.49419352412223816\n",
      "epoch:1/100, batch:43/508, loss:0.4959598183631897\n",
      "epoch:1/100, batch:44/508, loss:0.4893593192100525\n",
      "epoch:1/100, batch:45/508, loss:0.49084463715553284\n",
      "epoch:1/100, batch:46/508, loss:0.49439236521720886\n",
      "epoch:1/100, batch:47/508, loss:0.4904617667198181\n",
      "epoch:1/100, batch:48/508, loss:0.4923650920391083\n",
      "epoch:1/100, batch:49/508, loss:0.4915071725845337\n",
      "epoch:1/100, batch:50/508, loss:0.49108922481536865\n",
      "epoch:1/100, batch:51/508, loss:0.4765644669532776\n",
      "epoch:1/100, batch:52/508, loss:0.48184823989868164\n",
      "epoch:1/100, batch:53/508, loss:0.4902864396572113\n",
      "epoch:1/100, batch:54/508, loss:0.47256898880004883\n",
      "epoch:1/100, batch:55/508, loss:0.47407296299934387\n",
      "epoch:1/100, batch:56/508, loss:0.460207462310791\n",
      "epoch:1/100, batch:57/508, loss:0.5073303580284119\n",
      "epoch:1/100, batch:58/508, loss:0.45271822810173035\n",
      "epoch:1/100, batch:59/508, loss:0.48270317912101746\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = EmbeddingLayer(200, 120, 'lstm')\n",
    "domain_classifier = DomainClassifer(240, hidden_size=128, num_classes=2)\n",
    "train( \n",
    "    embedding_layer, \n",
    "    domain_classifier, \n",
    "    lamda = LAMDA,\n",
    "    id_set=ubuntu_train_idx_set,\n",
    "    train_from=ubuntu_context_repre,\n",
    "    sample_from=android_context_repre\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
