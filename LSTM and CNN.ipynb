{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import string\n",
    "import nltk\n",
    "import numpy as np; np.random.seed(7)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loadings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word 2 vec repre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_map = {}\n",
    "with open('data/vectors_pruned.200.txt', 'r') as src:\n",
    "    src = src.read().strip().split('\\n')\n",
    "    for line in src:\n",
    "        wv = line.strip().split(' ')\n",
    "        word = wv.pop(0)\n",
    "        w2v_map[word] = np.array(list(map(float, wv)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2i_map = {}\n",
    "for i, key in enumerate(w2v_map.keys()):\n",
    "    w2i_map[key] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pkl.dump(w2i_map, open('data/word_idx_map.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### map Q idx to context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_matrix = np.zeros(( len((w2v_map.keys())), 200 ))\n",
    "counter = 0\n",
    "for _, val in w2v_map.items():\n",
    "    w2v_matrix[counter] = val\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pkl.dump(w2i_map, open('data/w2v_matrix.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def w2v(w):\n",
    "    return w2v_matrix[w2i_map[w]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sen2w(sen):\n",
    "    processed = []\n",
    "    sen = re.sub(r'[!#\\'(),/:?\\{}]', ' ', sen).strip().split()\n",
    "    if len(sen) > 100:\n",
    "        sen = sen[:100]\n",
    "    for w in sen:\n",
    "        # ignore date\n",
    "        if re.match(r'\\d{1,}-\\d{1,}-\\d{1,}', w):\n",
    "            continue\n",
    "        if re.match(r'\\d{1,}:\\d{1,}', w):\n",
    "            continue\n",
    "        if w in w2i_map:\n",
    "            processed += [w]\n",
    "        else:\n",
    "            separated = re.findall(r\"[^\\W\\d_]+|\\d+|[=`%$\\^\\-@;\\[&_*>\\].<~|+\\d+]\", w)\n",
    "            for separate_w in separated:\n",
    "                if separate_w in w2i_map:\n",
    "                    processed += [separate_w]\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_repre = {}\n",
    "with open('data/text_tokenized.txt', 'r') as src:\n",
    "    src = src.read().strip().split('\\n')\n",
    "    for line in src:\n",
    "        context = line.strip().split('\\t')\n",
    "        qid = context.pop(0)\n",
    "        if len(context) == 1:\n",
    "            context_repre[int(qid)] = {'t': sen2w(context[0]), 'b': None}\n",
    "        else:\n",
    "            context_repre[int(qid)] = {'t':sen2w(context[0]), 'b': sen2w(context[1])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pair building --- (q, q+), (q, q-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/train_random.txt', header=None, delimiter='\\t', names=['Q','Q+','Q-'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>Q+</th>\n",
       "      <th>Q-</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>262144</td>\n",
       "      <td>211039</td>\n",
       "      <td>227387 413633 113297 356390 256881 145638 2962...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>491522</td>\n",
       "      <td>65911</td>\n",
       "      <td>155119 402211 310669 383107 131731 299465 1633...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>240299</td>\n",
       "      <td>168608 390642</td>\n",
       "      <td>368007 70009 48077 376760 438005 228888 142340...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>196614</td>\n",
       "      <td>205184</td>\n",
       "      <td>334471 163710 376791 441664 159963 406360 4300...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>360457</td>\n",
       "      <td>321532</td>\n",
       "      <td>151863 501857 217578 470017 125838 31836 42066...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Q             Q+                                                 Q-\n",
       "0  262144         211039  227387 413633 113297 356390 256881 145638 2962...\n",
       "1  491522          65911  155119 402211 310669 383107 131731 299465 1633...\n",
       "2  240299  168608 390642  368007 70009 48077 376760 438005 228888 142340...\n",
       "3  196614         205184  334471 163710 376791 441664 159963 406360 4300...\n",
       "4  360457         321532  151863 501857 217578 470017 125838 31836 42066..."
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_set_pair_with_idx(df):\n",
    "    idx_set = {}\n",
    "    for idx, row in df.iterrows():\n",
    "        idx_set[row['Q']] = {'pos': np.array(list(map(int, row['Q+'].split(' ')))), \\\n",
    "                             'neg': np.array(list(map(int, row['Q-'].split(' '))))}\n",
    "    return idx_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_idx_set = build_set_pair_with_idx(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def contxt2vec(title, body=None):\n",
    "    \n",
    "    if body == None:\n",
    "        body = []\n",
    "    \n",
    "    v = np.zeros( (len(title) + len(body), 200) )\n",
    "    counter = 0\n",
    "    \n",
    "    for t in title:\n",
    "        v[counter] = w2v(t)\n",
    "        counter += 1\n",
    "\n",
    "    for b in body:\n",
    "        v[counter] = w2v(b)\n",
    "        counter += 1\n",
    "    \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(qids, idx_set):\n",
    "\n",
    "    batch_x = []\n",
    "    batch_y = np.zeros((len(qids) * 20, 1))\n",
    "    counter = 0\n",
    "    \n",
    "    max_len = 0\n",
    "    for qid in qids:\n",
    "        \n",
    "        qid_v = contxt2vec(context_repre[qid]['t'], context_repre[qid]['b'])\n",
    "        \n",
    "        q_pos = idx_set[qid]['pos']\n",
    "        \n",
    "        # usually one sample\n",
    "        for qid_pos in q_pos:\n",
    "            #if counter == batch_size: break\n",
    "            title, body = context_repre[qid_pos]['t'], context_repre[qid_pos]['b']\n",
    "            max_len = max(max_len, len(title) + len(body))\n",
    "            batch_x += [(qid_v, contxt2vec(title, body))]\n",
    "            batch_y[counter] = 1\n",
    "            counter += 1\n",
    "        \n",
    "        q_neg = idx_set[qid]['neg']\n",
    "        q_neg_sample_indices = np.random.choice(range(100), size=20 - len(q_pos))\n",
    "        q_random_neg = q_neg[q_neg_sample_indices]\n",
    "        \n",
    "        for qid_neg in q_random_neg:\n",
    "            #if counter == batch_size: break\n",
    "            title, body = context_repre[qid_neg]['t'], context_repre[qid_neg]['b']\n",
    "            max_len = max(max_len, len(title) + len(body))\n",
    "            batch_x += [(qid_v, contxt2vec(title, body))]\n",
    "            batch_y[counter] = 0\n",
    "            counter += 1\n",
    "\n",
    "    # padding\n",
    "    seq_len = []\n",
    "    padded_batch_x = np.zeros(( len(qids) * 20, max_len, 400 ))\n",
    "    for i, (q, q_) in enumerate(batch_x):\n",
    "        seq_len += [ (q.shape[0], q_.shape[0]) ]\n",
    "        padded_batch_x[i,:q.shape[0],:200] = q\n",
    "        padded_batch_x[i,:q_.shape[0],200:] = q_\n",
    "    \n",
    "    return padded_batch_x, batch_y, seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qids = list(train_idx_set.keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_x, batch_y, seq_len = process_batch(qids, train_idx_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EmbeddingLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, layer_type):\n",
    "        \n",
    "        super(EmbeddingLayer, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        if layer_type == 'lstm':\n",
    "            self.embedding_layer = nn.LSTM(input_size, hidden_size)\n",
    "        elif layer_type == 'cnn':\n",
    "            pass\n",
    "        \n",
    "        self.tanh = nn.Tanh()\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return (Variable(torch.zeros(1, 1, self.hidden_size)), Variable(torch.zeros(1, 1, self.hidden_size)))\n",
    "\n",
    "    def forward(self, context):\n",
    "        lstm_out, self.hidden = self.embedding_layer(context, (self.tanh(self.hidden[0]), self.tanh(self.hidden[1])))\n",
    "        return lstm_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cos_sim(qv, qv_):\n",
    "    return torch.dot(qv, qv_, dim=0) / ( torch.sqrt(torch.sum(qv ** 2, dim=0)) * torch.sqrt(torch.sum(qv_ ** 2, dim=0) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss(qv, qv_, y):\n",
    "    cos_scores = cos_sim(qv, qv_)\n",
    "    pos_score = cos_scores * y\n",
    "    neg_score = torch.max(cos_scores * (1 - y))\n",
    "    diff = (neg_score - pos_score) * y \n",
    "    loss = (dff * ( diff > 0 )) / torch.sum(y)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = EmbeddingLayer(200, 240, 'lstm')\n",
    "q = Variable(torch.FloatTensor(batch_x[:,:,:200]))\n",
    "q_ = Variable(torch.FloatTensor(batch_x[:,:,200:]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
