{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import string\n",
    "import numpy as np; np.random.seed(7)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# global\n",
    "HIDDEN_DIM = 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_map = {}\n",
    "with open('data/vectors_pruned.200.txt', 'r') as src:\n",
    "    src = src.read().strip().split('\\n')\n",
    "    for line in src:\n",
    "        wv = line.strip().split(' ')\n",
    "        word = wv.pop(0)\n",
    "        w2v_map[word] = np.array(list(map(float, wv)))\n",
    "\n",
    "w2i_map = {}\n",
    "w2v_matrix = np.zeros(( len((w2v_map.keys())), 200 ))\n",
    "for i, (key, val) in enumerate(w2v_map.items()):\n",
    "    w2i_map[key] = i\n",
    "    w2v_matrix[i] = val\n",
    "\n",
    "def w2v(w):\n",
    "    return w2v_matrix[w2i_map[w]]\n",
    "\n",
    "def sen2w(sen):\n",
    "    processed = []\n",
    "    sen = sen.strip().split()\n",
    "    if len(sen) > 100:\n",
    "        sen = sen[:100]\n",
    "    for w in sen:\n",
    "        #ignore date\n",
    "        if re.match(r'\\d{1,}-\\d{1,}-\\d{1,}', w):\n",
    "            continue\n",
    "        if re.match(r'\\d{1,}:\\d{1,}', w):\n",
    "            continue\n",
    "        \n",
    "        if w in w2i_map:\n",
    "            processed += [w]\n",
    "        else:\n",
    "            separated = re.findall(r\"[^\\W\\d_]+|\\d+|[=`%$\\^\\-@;\\[&_*>\\].<~|+\\d+]\", w)\n",
    "            if len(set(separated)) == 1:\n",
    "                continue\n",
    "            if separated.count('*') > 3 or separated.count('=') > 3:\n",
    "                continue\n",
    "            for separate_w in separated:\n",
    "                if separate_w in w2i_map:\n",
    "                    processed += [separate_w]\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fixed context len = 125\n",
    "context_repre = {}\n",
    "with open('data/text_tokenized.txt', 'r') as src:\n",
    "    src = src.read().strip().split('\\n')\n",
    "    for line in src:\n",
    "        context = line.strip().split('\\t')\n",
    "        qid = context.pop(0)\n",
    "        if len(context) == 1:\n",
    "            context_repre[int(qid)] = {'t': sen2w(context[0]), 'b': None}\n",
    "        else:\n",
    "            context_repre[int(qid)] = {'t':sen2w(context[0]), 'b': sen2w(context[1])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_set_pair_with_idx(df):\n",
    "    idx_set = {}\n",
    "    for idx, row in df.iterrows():\n",
    "        idx_set[row['Q']] = {'pos': np.array(list(map(int, row['Q+'].split(' ')))), \\\n",
    "                             'neg': np.array(list(map(int, row['Q-'].split(' '))))}\n",
    "    return idx_set\n",
    "\n",
    "train_df = pd.read_csv('data/train_random.txt', header=None, delimiter='\\t', names=['Q','Q+','Q-'])\n",
    "train_idx_set = build_set_pair_with_idx(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>Q+</th>\n",
       "      <th>Q-</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>262144</td>\n",
       "      <td>211039</td>\n",
       "      <td>227387 413633 113297 356390 256881 145638 2962...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>491522</td>\n",
       "      <td>65911</td>\n",
       "      <td>155119 402211 310669 383107 131731 299465 1633...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>240299</td>\n",
       "      <td>168608 390642</td>\n",
       "      <td>368007 70009 48077 376760 438005 228888 142340...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>196614</td>\n",
       "      <td>205184</td>\n",
       "      <td>334471 163710 376791 441664 159963 406360 4300...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>360457</td>\n",
       "      <td>321532</td>\n",
       "      <td>151863 501857 217578 470017 125838 31836 42066...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Q             Q+                                                 Q-\n",
       "0  262144         211039  227387 413633 113297 356390 256881 145638 2962...\n",
       "1  491522          65911  155119 402211 310669 383107 131731 299465 1633...\n",
       "2  240299  168608 390642  368007 70009 48077 376760 438005 228888 142340...\n",
       "3  196614         205184  334471 163710 376791 441664 159963 406360 4300...\n",
       "4  360457         321532  151863 501857 217578 470017 125838 31836 42066..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def contxt2vec(title, body=None):\n",
    "    \n",
    "    if body == None:\n",
    "        body = []\n",
    "    \n",
    "    title_v = np.zeros( (len(title), 200) )\n",
    "    \n",
    "    for i, t in enumerate(title):\n",
    "        title_v[i] = w2v(t)\n",
    "    \n",
    "    if len(body) > 0:\n",
    "        body_v = np.zeros( (len(body), 200) )\n",
    "        for i, b in enumerate(body):\n",
    "            body_v[i] = w2v(b)\n",
    "    \n",
    "        return title_v, body_v\n",
    "    \n",
    "    return title_v, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_contxt_batch(qids, idx_set):\n",
    "    \n",
    "    batch_title, batch_body = [], []\n",
    "    max_title_len, max_body_len = 0, 0\n",
    "    title_len, body_len = [], []\n",
    "    counter = 0\n",
    "#     y = []\n",
    "    \n",
    "    for qid in qids:\n",
    "        \n",
    "        q_title, q_body = context_repre[qid]['t'], context_repre[qid]['b']\n",
    "        q_pos = idx_set[qid]['pos']\n",
    "        \n",
    "        if len(q_pos) > 20:\n",
    "            q_pos = q_pos[:20]\n",
    "\n",
    "        for qid_pos in q_pos:\n",
    "\n",
    "            # query Q\n",
    "            title_len += [len(q_title)]\n",
    "            batch_title += [ q_title ]\n",
    "            max_title_len = max(max_title_len, len(q_title))\n",
    "            if not q_body:\n",
    "                body_len += [len(q_title)]\n",
    "                batch_body += [ q_title ]\n",
    "            else:\n",
    "                batch_body += [ q_body ]\n",
    "                body_len += [len(q_body)]\n",
    "                max_body_len = max(max_body_len, len(q_body))\n",
    "#             y += [1]\n",
    "            # pos Q\n",
    "            title, body = context_repre[qid_pos]['t'], context_repre[qid_pos]['b']\n",
    "            title_len += [len(title)]\n",
    "            batch_title += [ title ]\n",
    "            max_title_len = max(max_title_len, len(title))\n",
    "            if not body:\n",
    "                body_len += [len(title)]\n",
    "                batch_body += [ title ]\n",
    "            else:\n",
    "                batch_body += [ body ]\n",
    "                body_len += [len(body)]\n",
    "                max_body_len = max(max_body_len, len(body))\n",
    "#             y += [1]\n",
    "            # neg Q\n",
    "            q_neg = idx_set[qid]['neg']\n",
    "            q_neg_sample_indices = np.random.choice(range(100), size=20)\n",
    "            q_random_neg = q_neg[q_neg_sample_indices]\n",
    "            \n",
    "            for qid_neg in q_random_neg:\n",
    "                title, body = context_repre[qid_neg]['t'], context_repre[qid_neg]['b']\n",
    "                title_len += [len(title)]\n",
    "                batch_title += [ title ]\n",
    "                max_title_len = max(max_title_len, len(title))\n",
    "                if not body:\n",
    "                    body_len += [len(title)]\n",
    "                    batch_body += [ title ]\n",
    "                else:\n",
    "                    batch_body += [ body ]\n",
    "                    body_len += [len(body)]\n",
    "                    max_body_len = max(max_body_len, len(body))\n",
    "#                 y += [0]\n",
    "    # (max_seq_len, batch_size, feature_len)\n",
    "    padded_batch_title = np.zeros(( max_title_len, len(batch_title), 200)) \n",
    "    padded_batch_body = np.zeros(( max_body_len, len(batch_body),  200))\n",
    "    \n",
    "    for i, (title, body) in enumerate(zip(batch_title, batch_body)):\n",
    "        title_repre, body_repre = contxt2vec(title, body)\n",
    "        padded_batch_title[:title_len[i], i] = title_repre\n",
    "        padded_batch_body[:body_len[i], i] = body_repre\n",
    "    #np.array(y).reshape(-1,1)\n",
    "    return padded_batch_title, padded_batch_body, \\\n",
    "                np.array(title_len).reshape(-1,1), np.array(body_len).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_annotations(path, K_neg=20, prune_pos_cnt=10):\n",
    "    lst = [ ]\n",
    "    with open(path) as fin:\n",
    "        for line in fin:\n",
    "            parts = line.split(\"\\t\")\n",
    "            pid, pos, neg = parts[:3]\n",
    "            pos = pos.split()\n",
    "            neg = neg.split()\n",
    "            if len(pos) == 0 or (len(pos) > prune_pos_cnt and prune_pos_cnt != -1): continue\n",
    "            if K_neg != -1:\n",
    "                np.random.shuffle(neg)\n",
    "                neg = neg[:K_neg]\n",
    "            s = set()\n",
    "            qids = [ ]\n",
    "            qlabels = [ ]\n",
    "            for q in neg:\n",
    "                if q not in s:\n",
    "                    qids.append(q)\n",
    "                    qlabels.append(0 if q not in pos else 1)\n",
    "                    s.add(q)\n",
    "            for q in pos:\n",
    "                if q not in s:\n",
    "                    qids.append(q)\n",
    "                    qlabels.append(1)\n",
    "                    s.add(q)\n",
    "            lst.append((pid, qids, qlabels))\n",
    "\n",
    "    return lst\n",
    "\n",
    "def cos_sim(qv, qv_):\n",
    "    return torch.sum(qv * qv_, dim=1) / (torch.sqrt(torch.sum(qv ** 2, dim=1)) * torch.sqrt(torch.sum(qv_ ** 2, dim=1)))\n",
    "    \n",
    "# create eval batch \n",
    "def process_eval_batch(qid, data):\n",
    "    qid_dict = data[qid]\n",
    "    qs = qid_dict['q']\n",
    "    max_title_len, max_body_len = 0, 0\n",
    "    title_len, body_len = [], []\n",
    "    batch_title, batch_body = [], []\n",
    "    for qid_ in [qid] + qs:\n",
    "        title, body = context_repre[qid_]['t'], context_repre[qid_]['b']\n",
    "        title_len += [len(title)]\n",
    "        batch_title += [ title ]\n",
    "        max_title_len = max(max_title_len, len(title))\n",
    "        if not body:\n",
    "            body_len += [len(title)]\n",
    "            batch_body += [ title ]\n",
    "        else:\n",
    "            batch_body += [ body ]\n",
    "            body_len += [len(body)]\n",
    "            max_body_len = max(max_body_len, len(body))\n",
    "        \n",
    "    padded_batch_title = np.zeros(( max_title_len, len(batch_title), 200)) \n",
    "    padded_batch_body = np.zeros(( max_body_len, len(batch_body),  200))\n",
    "    \n",
    "    for i, (title, body) in enumerate(zip(batch_title, batch_body)):\n",
    "        title_repre, body_repre = contxt2vec(title, body)\n",
    "        padded_batch_title[:title_len[i], i] = title_repre\n",
    "        padded_batch_body[:body_len[i], i] = body_repre\n",
    "    \n",
    "    return padded_batch_title, padded_batch_body, \\\n",
    "           np.array(title_len).reshape(-1,1), np.array(body_len).reshape(-1,1) \n",
    "    \n",
    "def evaluate(embeddings): # (n x 240)\n",
    "    qs = embeddings[0]\n",
    "    qs_ = embeddings[1:]\n",
    "    cos_scores = cos_sim(qs.expand(len(embeddings)-1, qs.size(0)), qs_)\n",
    "    return cos_scores\n",
    "\n",
    "def precision(at, labels):\n",
    "    res = []\n",
    "    for item in labels:\n",
    "        tmp = item[:at]\n",
    "        if any(val==1 for val in item):\n",
    "            res.append(np.sum(tmp) / len(tmp) if len(tmp) != 0 else 0.0)\n",
    "    return sum(res)/len(res) if len(res) != 0 else 0.0\n",
    "\n",
    "def MAP(labels):\n",
    "    scores = []\n",
    "    missing_MAP = 0\n",
    "    for item in labels:\n",
    "        temp = []\n",
    "        count = 0.0\n",
    "        for i,val in enumerate(item):\n",
    "            \n",
    "            if val == 1:\n",
    "                count += 1.0\n",
    "                temp.append(count/(i+1))\n",
    "            if len(temp) > 0:\n",
    "                scores.append(sum(temp) / len(temp))\n",
    "            else:\n",
    "                missing_MAP += 1\n",
    "    return sum(scores)/len(scores) if len(scores) > 0 else 0.0\n",
    "    \n",
    "def MRR(labels):\n",
    "    scores = []\n",
    "    for item in labels:\n",
    "        for i,val in enumerate(item):\n",
    "            if val == 1:\n",
    "                scores.append(1.0/(i+1))\n",
    "                break\n",
    "    return sum(scores)/len(scores) if len(scores) > 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_eval(eval_map, eval_data, embedding_layer, eval_name):\n",
    "    \n",
    "    labels = []\n",
    "    \n",
    "    for qid_ in eval_map.keys():\n",
    "        \n",
    "        eval_title_batch, eval_body_batch, eval_title_len, eval_body_len = eval_map[qid_] # process_eval_batch(qid_, eval_data)\n",
    "        embedding_layer.title_hidden = embedding_layer.init_hidden(eval_title_batch.shape[1])\n",
    "        embedding_layer.body_hidden = embedding_layer.init_hidden(eval_body_batch.shape[1])\n",
    "        eval_title_qs = Variable(torch.FloatTensor(eval_title_batch))\n",
    "        eval_body_qs = Variable(torch.FloatTensor(eval_body_batch))\n",
    "        embeddings = embedding_layer(eval_title_qs, eval_body_qs, eval_title_len, eval_body_len)\n",
    "        cos_scores = evaluate(embeddings)\n",
    "        labels.append(np.array(eval_data[qid_]['label'])[np.argsort(cos_scores.data.numpy())][::-1])\n",
    "\n",
    "    print (eval_name + ' Performance P@5', precision(5, labels))\n",
    "    print (eval_name + ' Performance P@1', precision(1, labels))\n",
    "    print (eval_name + ' Performance MAP', MAP(labels))\n",
    "    print (eval_name + ' Performance MRR', MRR(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEV SET\n",
    "dev = read_annotations('data/dev.txt')\n",
    "dev_data = {}\n",
    "for item in dev:\n",
    "    qid = int(item[0])\n",
    "    dev_data[qid] = {}\n",
    "    dev_data[qid]['q'] = list(map(int, item[1]))\n",
    "    dev_data[qid]['label'] = item[2]\n",
    "\n",
    "# TEST SET\n",
    "test = read_annotations('data/test.txt')\n",
    "test_data = {}\n",
    "for item in test:\n",
    "    qid = int(item[0])\n",
    "    test_data[qid] = {}\n",
    "    test_data[qid]['q'] = list(map(int, item[1]))\n",
    "    test_data[qid]['label'] = item[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dev_map = {}\n",
    "for qid_ in dev_data.keys():\n",
    "    dev_map[qid_] = process_eval_batch(qid_, dev_data)\n",
    "\n",
    "test_map = {}\n",
    "for qid_ in test_data.keys():\n",
    "    test_map[qid_] = process_eval_batch(qid_, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_mask(seq_len):\n",
    "    mask = []\n",
    "    for i, s in enumerate(seq_len):\n",
    "        s_mask = np.zeros((np.max(seq_len), 1))\n",
    "        s_mask[:int(s)] = np.ones((int(s), 1))\n",
    "        mask += [s_mask]\n",
    "    return mask\n",
    "\n",
    "def build_mask3d(seq_len):\n",
    "    mask = np.zeros((np.max(seq_len), len(seq_len), 1))\n",
    "    for i, s in enumerate(seq_len):\n",
    "        mask[:int(s), i] = np.ones((int(s), 1))\n",
    "    return mask\n",
    "\n",
    "def multi_margin_loss(margin=0.30):\n",
    "    \n",
    "    def loss_func(embeddings):\n",
    "        # a batch of embeddings\n",
    "        blocked_embeddings = embeddings.view(-1, 22, HIDDEN_DIM *2)\n",
    "        q_vecs = blocked_embeddings[:,0,:]\n",
    "        pos_vecs = blocked_embeddings[:,1,:]\n",
    "        neg_vecs = blocked_embeddings[:,2:,:]\n",
    "\n",
    "        pos_scores = torch.sum(q_vecs * pos_vecs, dim=1) / (torch.sqrt(torch.sum(q_vecs ** 2, dim=1)) \\\n",
    "                                                   * torch.sqrt(torch.sum(pos_vecs ** 2, dim=1)))\n",
    "\n",
    "        neg_scores = torch.sum(torch.unsqueeze(q_vecs, dim=1) * neg_vecs, dim=2) \\\n",
    "        / (torch.unsqueeze(torch.sqrt(torch.sum(q_vecs ** 2, dim=1)),dim=1) * torch.sqrt(torch.sum( neg_vecs ** 2, dim=2)))\n",
    "        neg_scores = torch.max(neg_scores, dim=1)[0]\n",
    "\n",
    "        diff = neg_scores - pos_scores + margin\n",
    "        loss = torch.mean((diff > 0).float() * diff)\n",
    "        return loss\n",
    "\n",
    "    return loss_func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EmbeddingLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, layer_type, num_layer=1, kernel_size=None):\n",
    "        \n",
    "        super(EmbeddingLayer, self).__init__()\n",
    "        \n",
    "        self.num_layer = num_layer\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.kernel_size = kernel_size\n",
    "        \n",
    "        if layer_type == 'lstm':\n",
    "            \n",
    "            self.layer_type = 'lstm'\n",
    "            #self.title_embedding_layer = nn.LSTM(input_size, hidden_size)\n",
    "            #self.body_embedding_layer = nn.LSTM(input_size, hidden_size)\n",
    "            self.embedding_layer = nn.LSTM(input_size, hidden_size, bidirectional=True)\n",
    "            self.tanh = nn.Tanh()\n",
    "        \n",
    "        elif layer_type == 'cnn':\n",
    "            self.layer_type = 'cnn'\n",
    "            self.embedding_layer = nn.Sequential(\n",
    "                        nn.Conv1d(in_channels = 200,\n",
    "                                  out_channels = self.hidden_size,\n",
    "                                  kernel_size = kernel_size),\n",
    "                        nn.Tanh())\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return (Variable(torch.zeros(self.num_layer*2, batch_size, self.hidden_size)), \\\n",
    "                Variable(torch.zeros(self.num_layer*2, batch_size, self.hidden_size)))\n",
    "\n",
    "    def forward(self, title, body, title_len, body_len):\n",
    "            \n",
    "        if self.layer_type == 'lstm':\n",
    "            \n",
    "            \n",
    "            title_lstm_out, self.title_hidden = self.embedding_layer(title, (self.tanh(self.title_hidden[0]), \\\n",
    "                                                                   self.tanh(self.title_hidden[1])))\n",
    "            body_lstm_out, self.body_hidden = self.embedding_layer(body, (self.tanh(self.body_hidden[0]), \\\n",
    "                                                                   self.tanh(self.body_hidden[1])))\n",
    "            \n",
    "            \n",
    "            title_mask = Variable(torch.FloatTensor(build_mask3d(title_len)))\n",
    "            title_embeddings = torch.sum(title_lstm_out * title_mask, dim=0) / torch.sum(title_mask, dim=0)\n",
    "            \n",
    "            body_mask = Variable(torch.FloatTensor(build_mask3d(body_len)))\n",
    "            body_embeddings = torch.sum(body_lstm_out * body_mask, dim=0) / torch.sum(body_mask, dim=0)\n",
    "            \n",
    "            embeddings = ( title_embeddings + body_embeddings ) / 2\n",
    "        \n",
    "            return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_model(mdl, path):\n",
    "    # saving model params\n",
    "    torch.save(mdl.state_dict(), path)\n",
    "\n",
    "def restore_model(mdl_skeleton, path):\n",
    "    # restoring params to the mdl skeleton\n",
    "    mdl_skeleton.load_state_dict(torch.load(path))\n",
    "    return mdl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(layer_type, embedding_layer, batch_size=25, num_epoch=100, id_set=train_idx_set, eval=True):\n",
    "    \n",
    "#     if layer_type == 'lstm':\n",
    "#         embedding_layer = EmbeddingLayer(200, 240, 'lstm')\n",
    "#     elif layer_type == 'cnn':\n",
    "#         embedding_layer = EmbeddingLayer(200, 240, 'cnn', kernel_size=3)\n",
    "        \n",
    "    optimizer = torch.optim.Adam(embedding_layer.parameters(), lr=0.001)\n",
    "    # criterion = torch.nn.MultiMarginLoss()\n",
    "    criterion = multi_margin_loss()\n",
    "    \n",
    "    qids = list(id_set.keys())\n",
    "    num_batch = len(qids) // batch_size\n",
    "    \n",
    "    for epoch in range(1, num_epoch + 1):\n",
    "        \n",
    "        for batch_idx in range(1, num_batch + 1):\n",
    "            \n",
    "            batch_x_qids = qids[ ( batch_idx - 1 ) * batch_size: batch_idx * batch_size ]\n",
    "            batch_title, batch_body, title_len, body_len = process_contxt_batch(batch_x_qids, train_idx_set)\n",
    "            \n",
    "            if layer_type == 'lstm':\n",
    "                embedding_layer.title_hidden = embedding_layer.init_hidden(batch_title.shape[1])\n",
    "                embedding_layer.body_hidden = embedding_layer.init_hidden(batch_body.shape[1])\n",
    "            \n",
    "            title_qs = Variable(torch.FloatTensor(batch_title))#, requires_grad=True)\n",
    "            body_qs = Variable(torch.FloatTensor(batch_body))#, requires_grad=True)\n",
    "            \n",
    "            embeddings = embedding_layer(title_qs, body_qs, title_len, body_len)\n",
    "#             blocked_embeddings = embeddings.view(-1, 22, HIDDEN_DIM * 2)\n",
    "#             q_vecs = blocked_embeddings[:,0,:]\n",
    "#             pos_neg_vecs = blocked_embeddings[:,1:,:]\n",
    "            \n",
    "#             # cosine similarity\n",
    "#             scores = torch.sum(torch.unsqueeze(q_vecs, dim=1) * pos_neg_vecs, dim=2) \\\n",
    "#                 / (torch.unsqueeze(torch.sqrt(torch.sum(q_vecs ** 2, dim=1)), dim=1) *\\\n",
    "#                    torch.sqrt(torch.sum( pos_neg_vecs ** 2, dim=2)))\n",
    "#             target = Variable(torch.zeros(scores.size(0)).type(torch.LongTensor)) \n",
    "            \n",
    "            loss = criterion(embeddings)\n",
    "            #loss = criterion(scores, target)\n",
    "            print ('epoch:{}/{}, batch:{}/{}, loss:{}'.format(epoch, num_epoch, batch_idx, num_batch, loss.data[0]))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "            if eval and batch_idx % 25 == 0: # lstm for now\n",
    "                print ('evaluating ....')\n",
    "                do_eval(dev_map, dev_data, embedding_layer, 'Dev')\n",
    "                print ('------------------')\n",
    "                do_eval(test_map, test_data, embedding_layer, 'Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1/10, batch:1/508, loss:0.5026151537895203\n",
      "epoch:1/10, batch:2/508, loss:0.502778172492981\n",
      "epoch:1/10, batch:3/508, loss:0.5006894469261169\n",
      "epoch:1/10, batch:4/508, loss:0.49986058473587036\n",
      "epoch:1/10, batch:5/508, loss:0.5012609362602234\n",
      "epoch:1/10, batch:6/508, loss:0.49954280257225037\n",
      "epoch:1/10, batch:7/508, loss:0.5005972981452942\n",
      "epoch:1/10, batch:8/508, loss:0.49912452697753906\n",
      "epoch:1/10, batch:9/508, loss:0.4989265203475952\n",
      "epoch:1/10, batch:10/508, loss:0.49831342697143555\n",
      "epoch:1/10, batch:11/508, loss:0.5000904202461243\n",
      "epoch:1/10, batch:12/508, loss:0.5000025629997253\n",
      "epoch:1/10, batch:13/508, loss:0.49849817156791687\n",
      "epoch:1/10, batch:14/508, loss:0.4986778497695923\n",
      "epoch:1/10, batch:15/508, loss:0.4995615482330322\n",
      "epoch:1/10, batch:16/508, loss:0.49869799613952637\n",
      "epoch:1/10, batch:17/508, loss:0.49957963824272156\n",
      "epoch:1/10, batch:18/508, loss:0.4992402493953705\n",
      "epoch:1/10, batch:19/508, loss:0.49848881363868713\n",
      "epoch:1/10, batch:20/508, loss:0.4981097877025604\n",
      "epoch:1/10, batch:21/508, loss:0.4989778399467468\n",
      "epoch:1/10, batch:22/508, loss:0.49776920676231384\n",
      "epoch:1/10, batch:23/508, loss:0.49804046750068665\n",
      "epoch:1/10, batch:24/508, loss:0.4982928931713104\n",
      "epoch:1/10, batch:25/508, loss:0.49791476130485535\n",
      "epoch:1/10, batch:26/508, loss:0.4994646906852722\n",
      "epoch:1/10, batch:27/508, loss:0.4978293478488922\n",
      "epoch:1/10, batch:28/508, loss:0.4972526729106903\n",
      "epoch:1/10, batch:29/508, loss:0.49834728240966797\n",
      "epoch:1/10, batch:30/508, loss:0.4964900612831116\n",
      "epoch:1/10, batch:31/508, loss:0.4994589388370514\n",
      "epoch:1/10, batch:32/508, loss:0.49660494923591614\n",
      "epoch:1/10, batch:33/508, loss:0.49228158593177795\n",
      "epoch:1/10, batch:34/508, loss:0.49425017833709717\n",
      "epoch:1/10, batch:35/508, loss:0.49447402358055115\n",
      "epoch:1/10, batch:36/508, loss:0.49862024188041687\n",
      "epoch:1/10, batch:37/508, loss:0.49317148327827454\n",
      "epoch:1/10, batch:38/508, loss:0.49262315034866333\n",
      "epoch:1/10, batch:39/508, loss:0.48059818148612976\n",
      "epoch:1/10, batch:40/508, loss:0.48819389939308167\n",
      "epoch:1/10, batch:41/508, loss:0.48319339752197266\n",
      "epoch:1/10, batch:42/508, loss:0.48573794960975647\n",
      "epoch:1/10, batch:43/508, loss:0.4831410050392151\n",
      "epoch:1/10, batch:44/508, loss:0.47279953956604004\n",
      "epoch:1/10, batch:45/508, loss:0.4652847945690155\n",
      "epoch:1/10, batch:46/508, loss:0.47648563981056213\n",
      "epoch:1/10, batch:47/508, loss:0.4581361413002014\n",
      "epoch:1/10, batch:48/508, loss:0.45249348878860474\n",
      "epoch:1/10, batch:49/508, loss:0.467847615480423\n",
      "epoch:1/10, batch:50/508, loss:0.45640483498573303\n",
      "epoch:1/10, batch:51/508, loss:0.4185739755630493\n",
      "epoch:1/10, batch:52/508, loss:0.3981330692768097\n",
      "epoch:1/10, batch:53/508, loss:0.40138953924179077\n",
      "epoch:1/10, batch:54/508, loss:0.4002557098865509\n",
      "epoch:1/10, batch:55/508, loss:0.38636845350265503\n",
      "epoch:1/10, batch:56/508, loss:0.3994416296482086\n",
      "epoch:1/10, batch:57/508, loss:0.4428117871284485\n",
      "epoch:1/10, batch:58/508, loss:0.38672277331352234\n",
      "epoch:1/10, batch:59/508, loss:0.44070881605148315\n",
      "epoch:1/10, batch:60/508, loss:0.37468332052230835\n",
      "epoch:1/10, batch:61/508, loss:0.42678385972976685\n",
      "epoch:1/10, batch:62/508, loss:0.4127173125743866\n",
      "epoch:1/10, batch:63/508, loss:0.44484665989875793\n",
      "epoch:1/10, batch:64/508, loss:0.4276929795742035\n",
      "epoch:1/10, batch:65/508, loss:0.4173497259616852\n",
      "epoch:1/10, batch:66/508, loss:0.4016376733779907\n",
      "epoch:1/10, batch:67/508, loss:0.3919973373413086\n",
      "epoch:1/10, batch:68/508, loss:0.3395753800868988\n",
      "epoch:1/10, batch:69/508, loss:0.35292473435401917\n",
      "epoch:1/10, batch:70/508, loss:0.3820555508136749\n",
      "epoch:1/10, batch:71/508, loss:0.3967892527580261\n",
      "epoch:1/10, batch:72/508, loss:0.38665342330932617\n",
      "epoch:1/10, batch:73/508, loss:0.29363101720809937\n",
      "epoch:1/10, batch:74/508, loss:0.3983496427536011\n",
      "epoch:1/10, batch:75/508, loss:0.42492032051086426\n",
      "epoch:1/10, batch:76/508, loss:0.42160481214523315\n",
      "epoch:1/10, batch:77/508, loss:0.45862525701522827\n",
      "epoch:1/10, batch:78/508, loss:0.3548707962036133\n",
      "epoch:1/10, batch:79/508, loss:0.40922972559928894\n",
      "epoch:1/10, batch:80/508, loss:0.4903244376182556\n",
      "epoch:1/10, batch:81/508, loss:0.355657160282135\n",
      "epoch:1/10, batch:82/508, loss:0.3624071478843689\n",
      "epoch:1/10, batch:83/508, loss:0.4453965127468109\n",
      "epoch:1/10, batch:84/508, loss:0.3521118462085724\n",
      "epoch:1/10, batch:85/508, loss:0.36151057481765747\n",
      "epoch:1/10, batch:86/508, loss:0.3683021068572998\n",
      "epoch:1/10, batch:87/508, loss:0.3925538659095764\n",
      "epoch:1/10, batch:88/508, loss:0.4339004158973694\n",
      "epoch:1/10, batch:89/508, loss:0.4081844389438629\n",
      "epoch:1/10, batch:90/508, loss:0.3740503787994385\n",
      "epoch:1/10, batch:91/508, loss:0.3589133620262146\n",
      "epoch:1/10, batch:92/508, loss:0.45538896322250366\n",
      "epoch:1/10, batch:93/508, loss:0.3720324635505676\n",
      "epoch:1/10, batch:94/508, loss:0.40304815769195557\n",
      "epoch:1/10, batch:95/508, loss:0.36769574880599976\n",
      "epoch:1/10, batch:96/508, loss:0.3952934145927429\n",
      "epoch:1/10, batch:97/508, loss:0.3721541464328766\n",
      "epoch:1/10, batch:98/508, loss:0.3482245206832886\n",
      "epoch:1/10, batch:99/508, loss:0.32233262062072754\n",
      "epoch:1/10, batch:100/508, loss:0.35144370794296265\n",
      "epoch:1/10, batch:101/508, loss:0.39945080876350403\n",
      "epoch:1/10, batch:102/508, loss:0.3372615873813629\n",
      "epoch:1/10, batch:103/508, loss:0.3960113823413849\n",
      "epoch:1/10, batch:104/508, loss:0.5088971257209778\n",
      "epoch:1/10, batch:105/508, loss:0.3868315815925598\n",
      "epoch:1/10, batch:106/508, loss:0.36863285303115845\n",
      "epoch:1/10, batch:107/508, loss:0.31597909331321716\n",
      "epoch:1/10, batch:108/508, loss:0.35296037793159485\n",
      "epoch:1/10, batch:109/508, loss:0.3635942339897156\n",
      "epoch:1/10, batch:110/508, loss:0.35886460542678833\n",
      "epoch:1/10, batch:111/508, loss:0.2807890474796295\n",
      "epoch:1/10, batch:112/508, loss:0.3523169755935669\n",
      "epoch:1/10, batch:113/508, loss:0.33508172631263733\n",
      "epoch:1/10, batch:114/508, loss:0.3920367360115051\n",
      "epoch:1/10, batch:115/508, loss:0.3471870720386505\n",
      "epoch:1/10, batch:116/508, loss:0.36746323108673096\n",
      "epoch:1/10, batch:117/508, loss:0.4132470488548279\n",
      "epoch:1/10, batch:118/508, loss:0.3683620095252991\n",
      "epoch:1/10, batch:119/508, loss:0.31220340728759766\n",
      "epoch:1/10, batch:120/508, loss:0.38619646430015564\n",
      "epoch:1/10, batch:121/508, loss:0.3853146433830261\n",
      "epoch:1/10, batch:122/508, loss:0.36963555216789246\n",
      "epoch:1/10, batch:123/508, loss:0.36337971687316895\n",
      "epoch:1/10, batch:124/508, loss:0.33617275953292847\n",
      "epoch:1/10, batch:125/508, loss:0.2907380759716034\n",
      "epoch:1/10, batch:126/508, loss:0.3841628134250641\n",
      "epoch:1/10, batch:127/508, loss:0.31308650970458984\n",
      "evaluating ....\n",
      "Dev Performance P@5 0.35652173913\n",
      "Dev Performance P@1 0.472049689441\n",
      "Dev Performance MAP 0.5946817935427837\n",
      "Dev Performance MRR 0.6206042394661322\n",
      "------------------\n",
      "Test Performance P@5 0.352201257862\n",
      "Test Performance P@1 0.471698113208\n",
      "Test Performance MAP 0.5967468369501523\n",
      "Test Performance MRR 0.6296879157107559\n",
      "epoch:1/10, batch:128/508, loss:0.38810548186302185\n",
      "epoch:1/10, batch:129/508, loss:0.3257090151309967\n",
      "epoch:1/10, batch:130/508, loss:0.40530434250831604\n",
      "epoch:1/10, batch:131/508, loss:0.4208141565322876\n",
      "epoch:1/10, batch:132/508, loss:0.32536736130714417\n",
      "epoch:1/10, batch:133/508, loss:0.3326844274997711\n",
      "epoch:1/10, batch:134/508, loss:0.40974536538124084\n",
      "epoch:1/10, batch:135/508, loss:0.2978750765323639\n",
      "epoch:1/10, batch:136/508, loss:0.30368930101394653\n",
      "epoch:1/10, batch:137/508, loss:0.3598563075065613\n",
      "epoch:1/10, batch:138/508, loss:0.3019961714744568\n",
      "epoch:1/10, batch:139/508, loss:0.3489164710044861\n",
      "epoch:1/10, batch:140/508, loss:0.4461476504802704\n",
      "epoch:1/10, batch:141/508, loss:0.40063491463661194\n",
      "epoch:1/10, batch:142/508, loss:0.3867228627204895\n",
      "epoch:1/10, batch:143/508, loss:0.41369831562042236\n",
      "epoch:1/10, batch:144/508, loss:0.33025944232940674\n",
      "epoch:1/10, batch:145/508, loss:0.3391762375831604\n",
      "epoch:1/10, batch:146/508, loss:0.33506283164024353\n",
      "epoch:1/10, batch:147/508, loss:0.3359110653400421\n",
      "epoch:1/10, batch:148/508, loss:0.29835018515586853\n",
      "epoch:1/10, batch:149/508, loss:0.33105558156967163\n",
      "epoch:1/10, batch:150/508, loss:0.3849356174468994\n",
      "epoch:1/10, batch:151/508, loss:0.4518043100833893\n",
      "epoch:1/10, batch:152/508, loss:0.3407853841781616\n",
      "epoch:1/10, batch:153/508, loss:0.3551708459854126\n",
      "epoch:1/10, batch:154/508, loss:0.34481045603752136\n",
      "epoch:1/10, batch:155/508, loss:0.3655294179916382\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1/10, batch:156/508, loss:0.3436394929885864\n",
      "epoch:1/10, batch:157/508, loss:0.30853477120399475\n",
      "epoch:1/10, batch:158/508, loss:0.3045615255832672\n",
      "epoch:1/10, batch:159/508, loss:0.3891583979129791\n",
      "epoch:1/10, batch:160/508, loss:0.40409210324287415\n",
      "epoch:1/10, batch:161/508, loss:0.34816503524780273\n",
      "epoch:1/10, batch:162/508, loss:0.3790401518344879\n",
      "epoch:1/10, batch:163/508, loss:0.40466371178627014\n",
      "epoch:1/10, batch:164/508, loss:0.3888358771800995\n",
      "epoch:1/10, batch:165/508, loss:0.3659917116165161\n",
      "epoch:1/10, batch:166/508, loss:0.38307780027389526\n",
      "epoch:1/10, batch:167/508, loss:0.3464403450489044\n",
      "epoch:1/10, batch:168/508, loss:0.3292739689350128\n",
      "epoch:1/10, batch:169/508, loss:0.27529025077819824\n",
      "epoch:1/10, batch:170/508, loss:0.3382743000984192\n",
      "epoch:1/10, batch:171/508, loss:0.36284366250038147\n",
      "epoch:1/10, batch:172/508, loss:0.3087102770805359\n",
      "epoch:1/10, batch:173/508, loss:0.334555983543396\n",
      "epoch:1/10, batch:174/508, loss:0.29747191071510315\n",
      "epoch:1/10, batch:175/508, loss:0.39918431639671326\n",
      "epoch:1/10, batch:176/508, loss:0.3465634882450104\n",
      "epoch:1/10, batch:177/508, loss:0.3104202151298523\n",
      "epoch:1/10, batch:178/508, loss:0.35004565119743347\n",
      "epoch:1/10, batch:179/508, loss:0.37228822708129883\n",
      "epoch:1/10, batch:180/508, loss:0.3402508795261383\n",
      "epoch:1/10, batch:181/508, loss:0.35160836577415466\n",
      "epoch:1/10, batch:182/508, loss:0.3255452811717987\n",
      "epoch:1/10, batch:183/508, loss:0.4736206829547882\n",
      "epoch:1/10, batch:184/508, loss:0.3279823660850525\n",
      "epoch:1/10, batch:185/508, loss:0.427774041891098\n",
      "epoch:1/10, batch:186/508, loss:0.32676348090171814\n",
      "epoch:1/10, batch:187/508, loss:0.41931474208831787\n",
      "epoch:1/10, batch:188/508, loss:0.3824184536933899\n",
      "epoch:1/10, batch:189/508, loss:0.3057585656642914\n",
      "epoch:1/10, batch:190/508, loss:0.3347896635532379\n",
      "epoch:1/10, batch:191/508, loss:0.37750867009162903\n",
      "epoch:1/10, batch:192/508, loss:0.3994973301887512\n",
      "epoch:1/10, batch:193/508, loss:0.2972756028175354\n",
      "epoch:1/10, batch:194/508, loss:0.288767009973526\n",
      "epoch:1/10, batch:195/508, loss:0.3498649001121521\n",
      "epoch:1/10, batch:196/508, loss:0.29956188797950745\n",
      "epoch:1/10, batch:197/508, loss:0.3703598380088806\n",
      "epoch:1/10, batch:198/508, loss:0.3002379834651947\n",
      "epoch:1/10, batch:199/508, loss:0.34294554591178894\n",
      "epoch:1/10, batch:200/508, loss:0.363275945186615\n",
      "epoch:1/10, batch:201/508, loss:0.38007086515426636\n",
      "epoch:1/10, batch:202/508, loss:0.32369759678840637\n",
      "epoch:1/10, batch:203/508, loss:0.31442543864250183\n",
      "epoch:1/10, batch:204/508, loss:0.33573129773139954\n",
      "epoch:1/10, batch:205/508, loss:0.34021028876304626\n",
      "epoch:1/10, batch:206/508, loss:0.40520453453063965\n",
      "epoch:1/10, batch:207/508, loss:0.33348584175109863\n",
      "epoch:1/10, batch:208/508, loss:0.3344821333885193\n",
      "epoch:1/10, batch:209/508, loss:0.31736162304878235\n",
      "epoch:1/10, batch:210/508, loss:0.3113079369068146\n",
      "epoch:1/10, batch:211/508, loss:0.38617533445358276\n",
      "epoch:1/10, batch:212/508, loss:0.42555540800094604\n",
      "epoch:1/10, batch:213/508, loss:0.2832162380218506\n",
      "epoch:1/10, batch:214/508, loss:0.3576175570487976\n",
      "epoch:1/10, batch:215/508, loss:0.39258888363838196\n",
      "epoch:1/10, batch:216/508, loss:0.32446011900901794\n",
      "epoch:1/10, batch:217/508, loss:0.3847164511680603\n",
      "epoch:1/10, batch:218/508, loss:0.3623419404029846\n",
      "epoch:1/10, batch:219/508, loss:0.3396720290184021\n",
      "epoch:1/10, batch:220/508, loss:0.30624476075172424\n",
      "epoch:1/10, batch:221/508, loss:0.36221975088119507\n",
      "epoch:1/10, batch:222/508, loss:0.3162113130092621\n",
      "epoch:1/10, batch:223/508, loss:0.3910025954246521\n",
      "epoch:1/10, batch:224/508, loss:0.3002050817012787\n",
      "epoch:1/10, batch:225/508, loss:0.36185139417648315\n",
      "epoch:1/10, batch:226/508, loss:0.325903058052063\n",
      "epoch:1/10, batch:227/508, loss:0.3203343152999878\n",
      "epoch:1/10, batch:228/508, loss:0.4170382022857666\n",
      "epoch:1/10, batch:229/508, loss:0.4019034504890442\n",
      "epoch:1/10, batch:230/508, loss:0.2893036901950836\n",
      "epoch:1/10, batch:231/508, loss:0.334600031375885\n",
      "epoch:1/10, batch:232/508, loss:0.32409268617630005\n",
      "epoch:1/10, batch:233/508, loss:0.3446537256240845\n",
      "epoch:1/10, batch:234/508, loss:0.2309361845254898\n",
      "epoch:1/10, batch:235/508, loss:0.36800187826156616\n",
      "epoch:1/10, batch:236/508, loss:0.34219926595687866\n",
      "epoch:1/10, batch:237/508, loss:0.3622852563858032\n",
      "epoch:1/10, batch:238/508, loss:0.385710209608078\n",
      "epoch:1/10, batch:239/508, loss:0.3146858513355255\n",
      "epoch:1/10, batch:240/508, loss:0.323464959859848\n",
      "epoch:1/10, batch:241/508, loss:0.29721468687057495\n",
      "epoch:1/10, batch:242/508, loss:0.2754787504673004\n",
      "epoch:1/10, batch:243/508, loss:0.3303103446960449\n",
      "epoch:1/10, batch:244/508, loss:0.3342770040035248\n",
      "epoch:1/10, batch:245/508, loss:0.22826537489891052\n",
      "epoch:1/10, batch:246/508, loss:0.31084248423576355\n",
      "epoch:1/10, batch:247/508, loss:0.3439583480358124\n",
      "epoch:1/10, batch:248/508, loss:0.31128883361816406\n",
      "epoch:1/10, batch:249/508, loss:0.32853949069976807\n",
      "epoch:1/10, batch:250/508, loss:0.36713385581970215\n",
      "epoch:1/10, batch:251/508, loss:0.3204995095729828\n",
      "epoch:1/10, batch:252/508, loss:0.3110661208629608\n",
      "epoch:1/10, batch:253/508, loss:0.332133024930954\n",
      "epoch:1/10, batch:254/508, loss:0.3060818314552307\n",
      "evaluating ....\n",
      "Dev Performance P@5 0.367701863354\n",
      "Dev Performance P@1 0.527950310559\n",
      "Dev Performance MAP 0.6194429193147776\n",
      "Dev Performance MRR 0.6564636900350705\n",
      "------------------\n",
      "Test Performance P@5 0.363522012579\n",
      "Test Performance P@1 0.509433962264\n",
      "Test Performance MAP 0.6159374756201557\n",
      "Test Performance MRR 0.6561243755508899\n",
      "epoch:1/10, batch:255/508, loss:0.2608392536640167\n",
      "epoch:1/10, batch:256/508, loss:0.30224037170410156\n",
      "epoch:1/10, batch:257/508, loss:0.3639804720878601\n",
      "epoch:1/10, batch:258/508, loss:0.31591299176216125\n",
      "epoch:1/10, batch:259/508, loss:0.3891417384147644\n",
      "epoch:1/10, batch:260/508, loss:0.3610425591468811\n",
      "epoch:1/10, batch:261/508, loss:0.3076590895652771\n",
      "epoch:1/10, batch:262/508, loss:0.29864126443862915\n",
      "epoch:1/10, batch:263/508, loss:0.32784372568130493\n",
      "epoch:1/10, batch:264/508, loss:0.38616642355918884\n",
      "epoch:1/10, batch:265/508, loss:0.2912818491458893\n",
      "epoch:1/10, batch:266/508, loss:0.3901342451572418\n",
      "epoch:1/10, batch:267/508, loss:0.36968177556991577\n",
      "epoch:1/10, batch:268/508, loss:0.398227721452713\n",
      "epoch:1/10, batch:269/508, loss:0.334734708070755\n",
      "epoch:1/10, batch:270/508, loss:0.3517017066478729\n",
      "epoch:1/10, batch:271/508, loss:0.3353155255317688\n",
      "epoch:1/10, batch:272/508, loss:0.3051561117172241\n",
      "epoch:1/10, batch:273/508, loss:0.30223435163497925\n",
      "epoch:1/10, batch:274/508, loss:0.297322541475296\n",
      "epoch:1/10, batch:275/508, loss:0.2508668899536133\n",
      "epoch:1/10, batch:276/508, loss:0.3500271439552307\n",
      "epoch:1/10, batch:277/508, loss:0.3217678666114807\n",
      "epoch:1/10, batch:278/508, loss:0.3378939628601074\n",
      "epoch:1/10, batch:279/508, loss:0.31817716360092163\n",
      "epoch:1/10, batch:280/508, loss:0.3216732442378998\n",
      "epoch:1/10, batch:281/508, loss:0.2200928032398224\n",
      "epoch:1/10, batch:282/508, loss:0.30090299248695374\n",
      "epoch:1/10, batch:283/508, loss:0.3578414022922516\n",
      "epoch:1/10, batch:284/508, loss:0.2969646453857422\n",
      "epoch:1/10, batch:285/508, loss:0.34320732951164246\n",
      "epoch:1/10, batch:286/508, loss:0.34262943267822266\n",
      "epoch:1/10, batch:287/508, loss:0.3124220371246338\n",
      "epoch:1/10, batch:288/508, loss:0.3010372221469879\n",
      "epoch:1/10, batch:289/508, loss:0.36652421951293945\n",
      "epoch:1/10, batch:290/508, loss:0.40655627846717834\n",
      "epoch:1/10, batch:291/508, loss:0.2993946969509125\n",
      "epoch:1/10, batch:292/508, loss:0.35830068588256836\n",
      "epoch:1/10, batch:293/508, loss:0.3479119539260864\n",
      "epoch:1/10, batch:294/508, loss:0.3702389895915985\n",
      "epoch:1/10, batch:295/508, loss:0.33720406889915466\n",
      "epoch:1/10, batch:296/508, loss:0.33233606815338135\n",
      "epoch:1/10, batch:297/508, loss:0.378482848405838\n",
      "epoch:1/10, batch:298/508, loss:0.3730577230453491\n",
      "epoch:1/10, batch:299/508, loss:0.2886946499347687\n",
      "epoch:1/10, batch:300/508, loss:0.26741963624954224\n",
      "epoch:1/10, batch:301/508, loss:0.36512646079063416\n",
      "epoch:1/10, batch:302/508, loss:0.3042125701904297\n",
      "epoch:1/10, batch:303/508, loss:0.3387588560581207\n",
      "epoch:1/10, batch:304/508, loss:0.29641222953796387\n",
      "epoch:1/10, batch:305/508, loss:0.39793625473976135\n",
      "epoch:1/10, batch:306/508, loss:0.31750524044036865\n",
      "epoch:1/10, batch:307/508, loss:0.30596011877059937\n",
      "epoch:1/10, batch:308/508, loss:0.3034321963787079\n",
      "epoch:1/10, batch:309/508, loss:0.3417948782444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1/10, batch:310/508, loss:0.3460395932197571\n",
      "epoch:1/10, batch:311/508, loss:0.31791192293167114\n",
      "epoch:1/10, batch:312/508, loss:0.3219287097454071\n",
      "epoch:1/10, batch:313/508, loss:0.299938827753067\n",
      "epoch:1/10, batch:314/508, loss:0.2909744381904602\n",
      "epoch:1/10, batch:315/508, loss:0.2915215492248535\n",
      "epoch:1/10, batch:316/508, loss:0.3896145522594452\n",
      "epoch:1/10, batch:317/508, loss:0.34968647360801697\n",
      "epoch:1/10, batch:318/508, loss:0.3023347854614258\n",
      "epoch:1/10, batch:319/508, loss:0.345836341381073\n",
      "epoch:1/10, batch:320/508, loss:0.31608057022094727\n",
      "epoch:1/10, batch:321/508, loss:0.3416295349597931\n",
      "epoch:1/10, batch:322/508, loss:0.2625827193260193\n",
      "epoch:1/10, batch:323/508, loss:0.27997732162475586\n",
      "epoch:1/10, batch:324/508, loss:0.33261600136756897\n",
      "epoch:1/10, batch:325/508, loss:0.33321765065193176\n",
      "epoch:1/10, batch:326/508, loss:0.33245015144348145\n",
      "epoch:1/10, batch:327/508, loss:0.29648256301879883\n",
      "epoch:1/10, batch:328/508, loss:0.3275316655635834\n",
      "epoch:1/10, batch:329/508, loss:0.3008832037448883\n",
      "epoch:1/10, batch:330/508, loss:0.4713938236236572\n",
      "epoch:1/10, batch:331/508, loss:0.3286687135696411\n",
      "epoch:1/10, batch:332/508, loss:0.30820026993751526\n",
      "epoch:1/10, batch:333/508, loss:0.2822243273258209\n",
      "epoch:1/10, batch:334/508, loss:0.379629522562027\n",
      "epoch:1/10, batch:335/508, loss:0.3222973048686981\n",
      "epoch:1/10, batch:336/508, loss:0.3344116508960724\n",
      "epoch:1/10, batch:337/508, loss:0.3229367136955261\n",
      "epoch:1/10, batch:338/508, loss:0.2914317846298218\n",
      "epoch:1/10, batch:339/508, loss:0.26509392261505127\n",
      "epoch:1/10, batch:340/508, loss:0.3045642077922821\n",
      "epoch:1/10, batch:341/508, loss:0.34143710136413574\n",
      "epoch:1/10, batch:342/508, loss:0.2714722156524658\n",
      "epoch:1/10, batch:343/508, loss:0.39507126808166504\n",
      "epoch:1/10, batch:344/508, loss:0.2975492775440216\n",
      "epoch:1/10, batch:345/508, loss:0.38527795672416687\n",
      "epoch:1/10, batch:346/508, loss:0.3088231682777405\n",
      "epoch:1/10, batch:347/508, loss:0.35355520248413086\n",
      "epoch:1/10, batch:348/508, loss:0.35100188851356506\n",
      "epoch:1/10, batch:349/508, loss:0.30527791380882263\n",
      "epoch:1/10, batch:350/508, loss:0.35195374488830566\n",
      "epoch:1/10, batch:351/508, loss:0.3235457241535187\n",
      "epoch:1/10, batch:352/508, loss:0.3442334830760956\n",
      "epoch:1/10, batch:353/508, loss:0.31454282999038696\n",
      "epoch:1/10, batch:354/508, loss:0.385469913482666\n",
      "epoch:1/10, batch:355/508, loss:0.39786770939826965\n",
      "epoch:1/10, batch:356/508, loss:0.3237980306148529\n",
      "epoch:1/10, batch:357/508, loss:0.34617865085601807\n",
      "epoch:1/10, batch:358/508, loss:0.3377103805541992\n",
      "epoch:1/10, batch:359/508, loss:0.3530544340610504\n",
      "epoch:1/10, batch:360/508, loss:0.27164313197135925\n",
      "epoch:1/10, batch:361/508, loss:0.33010849356651306\n",
      "epoch:1/10, batch:362/508, loss:0.3053273856639862\n",
      "epoch:1/10, batch:363/508, loss:0.33726978302001953\n",
      "epoch:1/10, batch:364/508, loss:0.3190803527832031\n",
      "epoch:1/10, batch:365/508, loss:0.24583622813224792\n",
      "epoch:1/10, batch:366/508, loss:0.27490362524986267\n",
      "epoch:1/10, batch:367/508, loss:0.33332863450050354\n",
      "epoch:1/10, batch:368/508, loss:0.29205039143562317\n",
      "epoch:1/10, batch:369/508, loss:0.41018712520599365\n",
      "epoch:1/10, batch:370/508, loss:0.2602628469467163\n",
      "epoch:1/10, batch:371/508, loss:0.3466434180736542\n",
      "epoch:1/10, batch:372/508, loss:0.31885236501693726\n",
      "epoch:1/10, batch:373/508, loss:0.4175412654876709\n",
      "epoch:1/10, batch:374/508, loss:0.32409414649009705\n",
      "epoch:1/10, batch:375/508, loss:0.3102055788040161\n",
      "epoch:1/10, batch:376/508, loss:0.42975935339927673\n",
      "epoch:1/10, batch:377/508, loss:0.30927661061286926\n",
      "epoch:1/10, batch:378/508, loss:0.28502732515335083\n",
      "epoch:1/10, batch:379/508, loss:0.28769758343696594\n",
      "epoch:1/10, batch:380/508, loss:0.3320920467376709\n",
      "epoch:1/10, batch:381/508, loss:0.2601432502269745\n",
      "evaluating ....\n",
      "Dev Performance P@5 0.376397515528\n",
      "Dev Performance P@1 0.496894409938\n",
      "Dev Performance MAP 0.6141460861202763\n",
      "Dev Performance MRR 0.6404311724689875\n",
      "------------------\n",
      "Test Performance P@5 0.362264150943\n",
      "Test Performance P@1 0.496855345912\n",
      "Test Performance MAP 0.6221023031667083\n",
      "Test Performance MRR 0.6675407369672511\n",
      "epoch:1/10, batch:382/508, loss:0.36712804436683655\n",
      "epoch:1/10, batch:383/508, loss:0.30311062932014465\n",
      "epoch:1/10, batch:384/508, loss:0.31047454476356506\n",
      "epoch:1/10, batch:385/508, loss:0.2755730450153351\n",
      "epoch:1/10, batch:386/508, loss:0.31328022480010986\n",
      "epoch:1/10, batch:387/508, loss:0.2561747431755066\n",
      "epoch:1/10, batch:388/508, loss:0.3290732800960541\n",
      "epoch:1/10, batch:389/508, loss:0.3527558147907257\n",
      "epoch:1/10, batch:390/508, loss:0.2831970453262329\n",
      "epoch:1/10, batch:391/508, loss:0.30545279383659363\n",
      "epoch:1/10, batch:392/508, loss:0.3119604289531708\n",
      "epoch:1/10, batch:393/508, loss:0.27827098965644836\n",
      "epoch:1/10, batch:394/508, loss:0.3039429485797882\n",
      "epoch:1/10, batch:395/508, loss:0.4142846167087555\n",
      "epoch:1/10, batch:396/508, loss:0.38884010910987854\n",
      "epoch:1/10, batch:397/508, loss:0.2800969183444977\n",
      "epoch:1/10, batch:398/508, loss:0.3050888478755951\n",
      "epoch:1/10, batch:399/508, loss:0.3264820873737335\n",
      "epoch:1/10, batch:400/508, loss:0.27037161588668823\n",
      "epoch:1/10, batch:401/508, loss:0.33237048983573914\n",
      "epoch:1/10, batch:402/508, loss:0.29125767946243286\n",
      "epoch:1/10, batch:403/508, loss:0.32655057311058044\n",
      "epoch:1/10, batch:404/508, loss:0.421199232339859\n",
      "epoch:1/10, batch:405/508, loss:0.28580063581466675\n",
      "epoch:1/10, batch:406/508, loss:0.29351019859313965\n",
      "epoch:1/10, batch:407/508, loss:0.2952899932861328\n",
      "epoch:1/10, batch:408/508, loss:0.3573826849460602\n",
      "epoch:1/10, batch:409/508, loss:0.27847883105278015\n",
      "epoch:1/10, batch:410/508, loss:0.3140629231929779\n",
      "epoch:1/10, batch:411/508, loss:0.3133389353752136\n",
      "epoch:1/10, batch:412/508, loss:0.26840561628341675\n",
      "epoch:1/10, batch:413/508, loss:0.3486201763153076\n",
      "epoch:1/10, batch:414/508, loss:0.3861556649208069\n",
      "epoch:1/10, batch:415/508, loss:0.3119332790374756\n",
      "epoch:1/10, batch:416/508, loss:0.3433660864830017\n",
      "epoch:1/10, batch:417/508, loss:0.3297204375267029\n",
      "epoch:1/10, batch:418/508, loss:0.27863550186157227\n",
      "epoch:1/10, batch:419/508, loss:0.2879733741283417\n",
      "epoch:1/10, batch:420/508, loss:0.33303892612457275\n",
      "epoch:1/10, batch:421/508, loss:0.29086869955062866\n",
      "epoch:1/10, batch:422/508, loss:0.2751750946044922\n",
      "epoch:1/10, batch:423/508, loss:0.3573583662509918\n",
      "epoch:1/10, batch:424/508, loss:0.3182448744773865\n",
      "epoch:1/10, batch:425/508, loss:0.2980552315711975\n",
      "epoch:1/10, batch:426/508, loss:0.28138643503189087\n",
      "epoch:1/10, batch:427/508, loss:0.326770544052124\n",
      "epoch:1/10, batch:428/508, loss:0.3127019703388214\n",
      "epoch:1/10, batch:429/508, loss:0.326677531003952\n",
      "epoch:1/10, batch:430/508, loss:0.285067081451416\n",
      "epoch:1/10, batch:431/508, loss:0.3196769058704376\n",
      "epoch:1/10, batch:432/508, loss:0.2836335003376007\n",
      "epoch:1/10, batch:433/508, loss:0.3709764778614044\n",
      "epoch:1/10, batch:434/508, loss:0.32804247736930847\n",
      "epoch:1/10, batch:435/508, loss:0.3309248089790344\n",
      "epoch:1/10, batch:436/508, loss:0.3622628152370453\n",
      "epoch:1/10, batch:437/508, loss:0.4263516962528229\n",
      "epoch:1/10, batch:438/508, loss:0.32111403346061707\n",
      "epoch:1/10, batch:439/508, loss:0.25707775354385376\n",
      "epoch:1/10, batch:440/508, loss:0.3217061161994934\n",
      "epoch:1/10, batch:441/508, loss:0.2920156419277191\n",
      "epoch:1/10, batch:442/508, loss:0.3045499324798584\n",
      "epoch:1/10, batch:443/508, loss:0.31689178943634033\n",
      "epoch:1/10, batch:444/508, loss:0.3557022213935852\n",
      "epoch:1/10, batch:445/508, loss:0.34473544359207153\n",
      "epoch:1/10, batch:446/508, loss:0.28265127539634705\n",
      "epoch:1/10, batch:447/508, loss:0.35516223311424255\n",
      "epoch:1/10, batch:448/508, loss:0.30596885085105896\n",
      "epoch:1/10, batch:449/508, loss:0.3788890242576599\n",
      "epoch:1/10, batch:450/508, loss:0.3126157522201538\n",
      "epoch:1/10, batch:451/508, loss:0.3204452395439148\n",
      "epoch:1/10, batch:452/508, loss:0.29282549023628235\n",
      "epoch:1/10, batch:453/508, loss:0.360293447971344\n",
      "epoch:1/10, batch:454/508, loss:0.3245633542537689\n",
      "epoch:1/10, batch:455/508, loss:0.31800881028175354\n",
      "epoch:1/10, batch:456/508, loss:0.3065177798271179\n",
      "epoch:1/10, batch:457/508, loss:0.33985447883605957\n",
      "epoch:1/10, batch:458/508, loss:0.32597270607948303\n",
      "epoch:1/10, batch:459/508, loss:0.29391366243362427\n",
      "epoch:1/10, batch:460/508, loss:0.3758825659751892\n",
      "epoch:1/10, batch:461/508, loss:0.2544974088668823\n",
      "epoch:1/10, batch:462/508, loss:0.2991154193878174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1/10, batch:463/508, loss:0.28970664739608765\n",
      "epoch:1/10, batch:464/508, loss:0.26896750926971436\n",
      "epoch:1/10, batch:465/508, loss:0.3158222436904907\n",
      "epoch:1/10, batch:466/508, loss:0.3548130989074707\n",
      "epoch:1/10, batch:467/508, loss:0.354170024394989\n",
      "epoch:1/10, batch:468/508, loss:0.30167487263679504\n",
      "epoch:1/10, batch:469/508, loss:0.3403187394142151\n",
      "epoch:1/10, batch:470/508, loss:0.2833174765110016\n",
      "epoch:1/10, batch:471/508, loss:0.3017997443675995\n",
      "epoch:1/10, batch:472/508, loss:0.31458860635757446\n",
      "epoch:1/10, batch:473/508, loss:0.2985970675945282\n",
      "epoch:1/10, batch:474/508, loss:0.24410149455070496\n",
      "epoch:1/10, batch:475/508, loss:0.2699381411075592\n",
      "epoch:1/10, batch:476/508, loss:0.2831593453884125\n",
      "epoch:1/10, batch:477/508, loss:0.33156293630599976\n",
      "epoch:1/10, batch:478/508, loss:0.3193717300891876\n",
      "epoch:1/10, batch:479/508, loss:0.2889942526817322\n",
      "epoch:1/10, batch:480/508, loss:0.27773547172546387\n",
      "epoch:1/10, batch:481/508, loss:0.2296927571296692\n",
      "epoch:1/10, batch:482/508, loss:0.3808438777923584\n",
      "epoch:1/10, batch:483/508, loss:0.33490023016929626\n",
      "epoch:1/10, batch:484/508, loss:0.27899718284606934\n",
      "epoch:1/10, batch:485/508, loss:0.23469282686710358\n",
      "epoch:1/10, batch:486/508, loss:0.28257638216018677\n",
      "epoch:1/10, batch:487/508, loss:0.3278298079967499\n",
      "epoch:1/10, batch:488/508, loss:0.3742355704307556\n",
      "epoch:1/10, batch:489/508, loss:0.32234424352645874\n",
      "epoch:1/10, batch:490/508, loss:0.3560074269771576\n",
      "epoch:1/10, batch:491/508, loss:0.2731932997703552\n",
      "epoch:1/10, batch:492/508, loss:0.31048864126205444\n",
      "epoch:1/10, batch:493/508, loss:0.31027719378471375\n",
      "epoch:1/10, batch:494/508, loss:0.29694968461990356\n",
      "epoch:1/10, batch:495/508, loss:0.26522746682167053\n",
      "epoch:1/10, batch:496/508, loss:0.3525162935256958\n",
      "epoch:1/10, batch:497/508, loss:0.3398659825325012\n",
      "epoch:1/10, batch:498/508, loss:0.269381046295166\n",
      "epoch:1/10, batch:499/508, loss:0.29353615641593933\n",
      "epoch:1/10, batch:500/508, loss:0.3261168897151947\n",
      "epoch:1/10, batch:501/508, loss:0.1898031383752823\n",
      "epoch:1/10, batch:502/508, loss:0.33782708644866943\n",
      "epoch:1/10, batch:503/508, loss:0.3008926510810852\n",
      "epoch:1/10, batch:504/508, loss:0.33396509289741516\n",
      "epoch:1/10, batch:505/508, loss:0.4094238579273224\n",
      "epoch:1/10, batch:506/508, loss:0.2913355231285095\n",
      "epoch:1/10, batch:507/508, loss:0.30629000067710876\n",
      "epoch:1/10, batch:508/508, loss:0.2913808226585388\n",
      "evaluating ....\n",
      "Dev Performance P@5 0.387577639752\n",
      "Dev Performance P@1 0.527950310559\n",
      "Dev Performance MAP 0.625208690700313\n",
      "Dev Performance MRR 0.6592387710126111\n",
      "------------------\n",
      "Test Performance P@5 0.367295597484\n",
      "Test Performance P@1 0.540880503145\n",
      "Test Performance MAP 0.6299722330856734\n",
      "Test Performance MRR 0.6824961781019377\n",
      "epoch:2/10, batch:1/508, loss:0.29298704862594604\n",
      "epoch:2/10, batch:2/508, loss:0.3200368583202362\n",
      "epoch:2/10, batch:3/508, loss:0.34005075693130493\n",
      "epoch:2/10, batch:4/508, loss:0.2769061326980591\n",
      "epoch:2/10, batch:5/508, loss:0.36085617542266846\n",
      "epoch:2/10, batch:6/508, loss:0.342717707157135\n",
      "epoch:2/10, batch:7/508, loss:0.2736421227455139\n",
      "epoch:2/10, batch:8/508, loss:0.19747783243656158\n",
      "epoch:2/10, batch:9/508, loss:0.2685203552246094\n",
      "epoch:2/10, batch:10/508, loss:0.2200237214565277\n",
      "epoch:2/10, batch:11/508, loss:0.28226935863494873\n",
      "epoch:2/10, batch:12/508, loss:0.3429892063140869\n",
      "epoch:2/10, batch:13/508, loss:0.17860236763954163\n",
      "epoch:2/10, batch:14/508, loss:0.25015655159950256\n",
      "epoch:2/10, batch:15/508, loss:0.2936808168888092\n",
      "epoch:2/10, batch:16/508, loss:0.22320038080215454\n",
      "epoch:2/10, batch:17/508, loss:0.3077835440635681\n",
      "epoch:2/10, batch:18/508, loss:0.3421383202075958\n",
      "epoch:2/10, batch:19/508, loss:0.24655942618846893\n",
      "epoch:2/10, batch:20/508, loss:0.23535774648189545\n",
      "epoch:2/10, batch:21/508, loss:0.3006041944026947\n",
      "epoch:2/10, batch:22/508, loss:0.263904333114624\n",
      "epoch:2/10, batch:23/508, loss:0.28797757625579834\n",
      "epoch:2/10, batch:24/508, loss:0.2780143618583679\n",
      "epoch:2/10, batch:25/508, loss:0.29007387161254883\n",
      "epoch:2/10, batch:26/508, loss:0.27136725187301636\n",
      "epoch:2/10, batch:27/508, loss:0.2787478268146515\n",
      "epoch:2/10, batch:28/508, loss:0.3112916350364685\n",
      "epoch:2/10, batch:29/508, loss:0.32542553544044495\n",
      "epoch:2/10, batch:30/508, loss:0.29909053444862366\n",
      "epoch:2/10, batch:31/508, loss:0.30844464898109436\n",
      "epoch:2/10, batch:32/508, loss:0.3113669455051422\n",
      "epoch:2/10, batch:33/508, loss:0.3109768331050873\n",
      "epoch:2/10, batch:34/508, loss:0.3038068115711212\n",
      "epoch:2/10, batch:35/508, loss:0.29123392701148987\n",
      "epoch:2/10, batch:36/508, loss:0.32675012946128845\n",
      "epoch:2/10, batch:37/508, loss:0.26290231943130493\n",
      "epoch:2/10, batch:38/508, loss:0.33842745423316956\n",
      "epoch:2/10, batch:39/508, loss:0.2439367026090622\n",
      "epoch:2/10, batch:40/508, loss:0.27348801493644714\n",
      "epoch:2/10, batch:41/508, loss:0.26056593656539917\n",
      "epoch:2/10, batch:42/508, loss:0.30303308367729187\n",
      "epoch:2/10, batch:43/508, loss:0.34523043036460876\n",
      "epoch:2/10, batch:44/508, loss:0.25841376185417175\n",
      "epoch:2/10, batch:45/508, loss:0.26467442512512207\n",
      "epoch:2/10, batch:46/508, loss:0.3095981478691101\n",
      "epoch:2/10, batch:47/508, loss:0.3225884735584259\n",
      "epoch:2/10, batch:48/508, loss:0.31088706851005554\n",
      "epoch:2/10, batch:49/508, loss:0.30871015787124634\n",
      "epoch:2/10, batch:50/508, loss:0.26368558406829834\n",
      "epoch:2/10, batch:51/508, loss:0.2445732057094574\n",
      "epoch:2/10, batch:52/508, loss:0.20605917274951935\n",
      "epoch:2/10, batch:53/508, loss:0.2645481824874878\n",
      "epoch:2/10, batch:54/508, loss:0.26829737424850464\n",
      "epoch:2/10, batch:55/508, loss:0.28181663155555725\n",
      "epoch:2/10, batch:56/508, loss:0.2306518703699112\n",
      "epoch:2/10, batch:57/508, loss:0.2893754839897156\n",
      "epoch:2/10, batch:58/508, loss:0.29114702343940735\n",
      "epoch:2/10, batch:59/508, loss:0.35235974192619324\n",
      "epoch:2/10, batch:60/508, loss:0.3462306261062622\n",
      "epoch:2/10, batch:61/508, loss:0.2901500165462494\n",
      "epoch:2/10, batch:62/508, loss:0.27366918325424194\n",
      "epoch:2/10, batch:63/508, loss:0.3461639881134033\n",
      "epoch:2/10, batch:64/508, loss:0.35843366384506226\n",
      "epoch:2/10, batch:65/508, loss:0.3227759003639221\n",
      "epoch:2/10, batch:66/508, loss:0.25944042205810547\n",
      "epoch:2/10, batch:67/508, loss:0.24644535779953003\n",
      "epoch:2/10, batch:68/508, loss:0.2499942034482956\n",
      "epoch:2/10, batch:69/508, loss:0.23279717564582825\n",
      "epoch:2/10, batch:70/508, loss:0.2717078924179077\n",
      "epoch:2/10, batch:71/508, loss:0.2957000434398651\n",
      "epoch:2/10, batch:72/508, loss:0.303681880235672\n",
      "epoch:2/10, batch:73/508, loss:0.2593349516391754\n",
      "epoch:2/10, batch:74/508, loss:0.34683164954185486\n",
      "epoch:2/10, batch:75/508, loss:0.3218909800052643\n",
      "epoch:2/10, batch:76/508, loss:0.3113366365432739\n",
      "epoch:2/10, batch:77/508, loss:0.33217620849609375\n",
      "epoch:2/10, batch:78/508, loss:0.2606937289237976\n",
      "epoch:2/10, batch:79/508, loss:0.36115753650665283\n",
      "epoch:2/10, batch:80/508, loss:0.35091036558151245\n",
      "epoch:2/10, batch:81/508, loss:0.23371918499469757\n",
      "epoch:2/10, batch:82/508, loss:0.2789402902126312\n",
      "epoch:2/10, batch:83/508, loss:0.33892524242401123\n",
      "epoch:2/10, batch:84/508, loss:0.2494376003742218\n",
      "epoch:2/10, batch:85/508, loss:0.24109894037246704\n",
      "epoch:2/10, batch:86/508, loss:0.3030250072479248\n",
      "epoch:2/10, batch:87/508, loss:0.22810257971286774\n",
      "epoch:2/10, batch:88/508, loss:0.29328498244285583\n",
      "epoch:2/10, batch:89/508, loss:0.30862128734588623\n",
      "epoch:2/10, batch:90/508, loss:0.2537067234516144\n",
      "epoch:2/10, batch:91/508, loss:0.2236868143081665\n",
      "epoch:2/10, batch:92/508, loss:0.35230207443237305\n",
      "epoch:2/10, batch:93/508, loss:0.30089128017425537\n",
      "epoch:2/10, batch:94/508, loss:0.2797107398509979\n",
      "epoch:2/10, batch:95/508, loss:0.2619412839412689\n",
      "epoch:2/10, batch:96/508, loss:0.3335288465023041\n",
      "epoch:2/10, batch:97/508, loss:0.2803444564342499\n",
      "epoch:2/10, batch:98/508, loss:0.2760019600391388\n",
      "epoch:2/10, batch:99/508, loss:0.23056098818778992\n",
      "epoch:2/10, batch:100/508, loss:0.3046669363975525\n",
      "epoch:2/10, batch:101/508, loss:0.3172507584095001\n",
      "epoch:2/10, batch:102/508, loss:0.2527100741863251\n",
      "epoch:2/10, batch:103/508, loss:0.321697860956192\n",
      "epoch:2/10, batch:104/508, loss:0.4050089716911316\n",
      "epoch:2/10, batch:105/508, loss:0.2875853180885315\n",
      "epoch:2/10, batch:106/508, loss:0.22440683841705322\n",
      "epoch:2/10, batch:107/508, loss:0.30400943756103516\n",
      "epoch:2/10, batch:108/508, loss:0.33049502968788147\n",
      "epoch:2/10, batch:109/508, loss:0.2668876647949219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2/10, batch:110/508, loss:0.2238190621137619\n",
      "epoch:2/10, batch:111/508, loss:0.22265294194221497\n",
      "epoch:2/10, batch:112/508, loss:0.2873879373073578\n",
      "epoch:2/10, batch:113/508, loss:0.2538389563560486\n",
      "epoch:2/10, batch:114/508, loss:0.35955604910850525\n",
      "epoch:2/10, batch:115/508, loss:0.33673274517059326\n",
      "epoch:2/10, batch:116/508, loss:0.2686859965324402\n",
      "epoch:2/10, batch:117/508, loss:0.3220653533935547\n",
      "epoch:2/10, batch:118/508, loss:0.30356574058532715\n",
      "epoch:2/10, batch:119/508, loss:0.27062103152275085\n",
      "epoch:2/10, batch:120/508, loss:0.32498806715011597\n",
      "epoch:2/10, batch:121/508, loss:0.29050949215888977\n",
      "epoch:2/10, batch:122/508, loss:0.2985553443431854\n",
      "epoch:2/10, batch:123/508, loss:0.2955058515071869\n",
      "epoch:2/10, batch:124/508, loss:0.25236427783966064\n",
      "epoch:2/10, batch:125/508, loss:0.2133544534444809\n",
      "epoch:2/10, batch:126/508, loss:0.26590195298194885\n",
      "epoch:2/10, batch:127/508, loss:0.27163830399513245\n",
      "evaluating ....\n",
      "Dev Performance P@5 0.377639751553\n",
      "Dev Performance P@1 0.534161490683\n",
      "Dev Performance MAP 0.6301937752057211\n",
      "Dev Performance MRR 0.668364062832411\n",
      "------------------\n",
      "Test Performance P@5 0.354716981132\n",
      "Test Performance P@1 0.522012578616\n",
      "Test Performance MAP 0.6210081302662905\n",
      "Test Performance MRR 0.6687589195533584\n",
      "epoch:2/10, batch:128/508, loss:0.3020941913127899\n",
      "epoch:2/10, batch:129/508, loss:0.22948338091373444\n",
      "epoch:2/10, batch:130/508, loss:0.32797229290008545\n",
      "epoch:2/10, batch:131/508, loss:0.3768388330936432\n",
      "epoch:2/10, batch:132/508, loss:0.22271840274333954\n",
      "epoch:2/10, batch:133/508, loss:0.2295835018157959\n",
      "epoch:2/10, batch:134/508, loss:0.3029915690422058\n",
      "epoch:2/10, batch:135/508, loss:0.21591044962406158\n",
      "epoch:2/10, batch:136/508, loss:0.2239757925271988\n",
      "epoch:2/10, batch:137/508, loss:0.2946179509162903\n",
      "epoch:2/10, batch:138/508, loss:0.2494371086359024\n",
      "epoch:2/10, batch:139/508, loss:0.24525682628154755\n",
      "epoch:2/10, batch:140/508, loss:0.3746512532234192\n",
      "epoch:2/10, batch:141/508, loss:0.3142380118370056\n",
      "epoch:2/10, batch:142/508, loss:0.26774975657463074\n",
      "epoch:2/10, batch:143/508, loss:0.3084804117679596\n",
      "epoch:2/10, batch:144/508, loss:0.27537503838539124\n",
      "epoch:2/10, batch:145/508, loss:0.27835771441459656\n",
      "epoch:2/10, batch:146/508, loss:0.3149706721305847\n",
      "epoch:2/10, batch:147/508, loss:0.30605772137641907\n",
      "epoch:2/10, batch:148/508, loss:0.2389908879995346\n",
      "epoch:2/10, batch:149/508, loss:0.26854658126831055\n",
      "epoch:2/10, batch:150/508, loss:0.30389222502708435\n",
      "epoch:2/10, batch:151/508, loss:0.3598613142967224\n",
      "epoch:2/10, batch:152/508, loss:0.29744628071784973\n",
      "epoch:2/10, batch:153/508, loss:0.28292667865753174\n",
      "epoch:2/10, batch:154/508, loss:0.30866074562072754\n",
      "epoch:2/10, batch:155/508, loss:0.2828715741634369\n",
      "epoch:2/10, batch:156/508, loss:0.26516810059547424\n",
      "epoch:2/10, batch:157/508, loss:0.21489763259887695\n",
      "epoch:2/10, batch:158/508, loss:0.2309088557958603\n",
      "epoch:2/10, batch:159/508, loss:0.2824414074420929\n",
      "epoch:2/10, batch:160/508, loss:0.3013545572757721\n",
      "epoch:2/10, batch:161/508, loss:0.27973809838294983\n",
      "epoch:2/10, batch:162/508, loss:0.2878343462944031\n",
      "epoch:2/10, batch:163/508, loss:0.3362158238887787\n",
      "epoch:2/10, batch:164/508, loss:0.2816547155380249\n",
      "epoch:2/10, batch:165/508, loss:0.31464001536369324\n",
      "epoch:2/10, batch:166/508, loss:0.3269578516483307\n",
      "epoch:2/10, batch:167/508, loss:0.2675410211086273\n",
      "epoch:2/10, batch:168/508, loss:0.28317880630493164\n",
      "epoch:2/10, batch:169/508, loss:0.2249293029308319\n",
      "epoch:2/10, batch:170/508, loss:0.2768164277076721\n",
      "epoch:2/10, batch:171/508, loss:0.2748905420303345\n",
      "epoch:2/10, batch:172/508, loss:0.2851296365261078\n",
      "epoch:2/10, batch:173/508, loss:0.2686813771724701\n",
      "epoch:2/10, batch:174/508, loss:0.2664766311645508\n",
      "epoch:2/10, batch:175/508, loss:0.3102269768714905\n",
      "epoch:2/10, batch:176/508, loss:0.2644508183002472\n",
      "epoch:2/10, batch:177/508, loss:0.2559327185153961\n",
      "epoch:2/10, batch:178/508, loss:0.2587932050228119\n",
      "epoch:2/10, batch:179/508, loss:0.27902844548225403\n",
      "epoch:2/10, batch:180/508, loss:0.306021511554718\n",
      "epoch:2/10, batch:181/508, loss:0.2805541753768921\n",
      "epoch:2/10, batch:182/508, loss:0.25288575887680054\n",
      "epoch:2/10, batch:183/508, loss:0.3596048057079315\n",
      "epoch:2/10, batch:184/508, loss:0.2774157226085663\n",
      "epoch:2/10, batch:185/508, loss:0.30361536145210266\n",
      "epoch:2/10, batch:186/508, loss:0.23243853449821472\n",
      "epoch:2/10, batch:187/508, loss:0.37635868787765503\n",
      "epoch:2/10, batch:188/508, loss:0.32625919580459595\n",
      "epoch:2/10, batch:189/508, loss:0.22508978843688965\n",
      "epoch:2/10, batch:190/508, loss:0.3067237138748169\n",
      "epoch:2/10, batch:191/508, loss:0.3191901445388794\n",
      "epoch:2/10, batch:192/508, loss:0.3818496763706207\n",
      "epoch:2/10, batch:193/508, loss:0.24344123899936676\n",
      "epoch:2/10, batch:194/508, loss:0.2692488729953766\n",
      "epoch:2/10, batch:195/508, loss:0.2647642195224762\n",
      "epoch:2/10, batch:196/508, loss:0.24373890459537506\n",
      "epoch:2/10, batch:197/508, loss:0.32922986149787903\n",
      "epoch:2/10, batch:198/508, loss:0.28607767820358276\n",
      "epoch:2/10, batch:199/508, loss:0.2934277355670929\n",
      "epoch:2/10, batch:200/508, loss:0.31103911995887756\n",
      "epoch:2/10, batch:201/508, loss:0.3064163327217102\n",
      "epoch:2/10, batch:202/508, loss:0.2991410791873932\n",
      "epoch:2/10, batch:203/508, loss:0.2665117681026459\n",
      "epoch:2/10, batch:204/508, loss:0.25663939118385315\n",
      "epoch:2/10, batch:205/508, loss:0.274007111787796\n",
      "epoch:2/10, batch:206/508, loss:0.36306682229042053\n",
      "epoch:2/10, batch:207/508, loss:0.30378666520118713\n",
      "epoch:2/10, batch:208/508, loss:0.3056342899799347\n",
      "epoch:2/10, batch:209/508, loss:0.2694688141345978\n",
      "epoch:2/10, batch:210/508, loss:0.2656049430370331\n",
      "epoch:2/10, batch:211/508, loss:0.30616188049316406\n",
      "epoch:2/10, batch:212/508, loss:0.3478546142578125\n",
      "epoch:2/10, batch:213/508, loss:0.24829944968223572\n",
      "epoch:2/10, batch:214/508, loss:0.3177827000617981\n",
      "epoch:2/10, batch:215/508, loss:0.28973597288131714\n",
      "epoch:2/10, batch:216/508, loss:0.2625696063041687\n",
      "epoch:2/10, batch:217/508, loss:0.33474352955818176\n",
      "epoch:2/10, batch:218/508, loss:0.28703996539115906\n",
      "epoch:2/10, batch:219/508, loss:0.22512443363666534\n",
      "epoch:2/10, batch:220/508, loss:0.24601130187511444\n",
      "epoch:2/10, batch:221/508, loss:0.28290215134620667\n",
      "epoch:2/10, batch:222/508, loss:0.30448469519615173\n",
      "epoch:2/10, batch:223/508, loss:0.34941017627716064\n",
      "epoch:2/10, batch:224/508, loss:0.25982096791267395\n",
      "epoch:2/10, batch:225/508, loss:0.29844483733177185\n",
      "epoch:2/10, batch:226/508, loss:0.2546675205230713\n",
      "epoch:2/10, batch:227/508, loss:0.26228323578834534\n",
      "epoch:2/10, batch:228/508, loss:0.3319244384765625\n",
      "epoch:2/10, batch:229/508, loss:0.314145028591156\n",
      "epoch:2/10, batch:230/508, loss:0.25327053666114807\n",
      "epoch:2/10, batch:231/508, loss:0.299772709608078\n",
      "epoch:2/10, batch:232/508, loss:0.30641481280326843\n",
      "epoch:2/10, batch:233/508, loss:0.28766995668411255\n",
      "epoch:2/10, batch:234/508, loss:0.19280648231506348\n",
      "epoch:2/10, batch:235/508, loss:0.3360026180744171\n",
      "epoch:2/10, batch:236/508, loss:0.2664717435836792\n",
      "epoch:2/10, batch:237/508, loss:0.3185710906982422\n",
      "epoch:2/10, batch:238/508, loss:0.321763277053833\n",
      "epoch:2/10, batch:239/508, loss:0.27739790081977844\n",
      "epoch:2/10, batch:240/508, loss:0.28544580936431885\n",
      "epoch:2/10, batch:241/508, loss:0.2844614088535309\n",
      "epoch:2/10, batch:242/508, loss:0.2350320965051651\n",
      "epoch:2/10, batch:243/508, loss:0.2888897955417633\n",
      "epoch:2/10, batch:244/508, loss:0.28083860874176025\n",
      "epoch:2/10, batch:245/508, loss:0.17393159866333008\n",
      "epoch:2/10, batch:246/508, loss:0.2623719871044159\n",
      "epoch:2/10, batch:247/508, loss:0.2997470498085022\n",
      "epoch:2/10, batch:248/508, loss:0.25673192739486694\n",
      "epoch:2/10, batch:249/508, loss:0.25583553314208984\n",
      "epoch:2/10, batch:250/508, loss:0.3328215777873993\n",
      "epoch:2/10, batch:251/508, loss:0.268458753824234\n",
      "epoch:2/10, batch:252/508, loss:0.2527211904525757\n",
      "epoch:2/10, batch:253/508, loss:0.31664636731147766\n",
      "epoch:2/10, batch:254/508, loss:0.291113018989563\n",
      "evaluating ....\n",
      "Dev Performance P@5 0.395031055901\n",
      "Dev Performance P@1 0.509316770186\n",
      "Dev Performance MAP 0.6251785135183147\n",
      "Dev Performance MRR 0.6584735966951899\n",
      "------------------\n",
      "Test Performance P@5 0.374842767296\n",
      "Test Performance P@1 0.534591194969\n",
      "Test Performance MAP 0.6307340168135267\n",
      "Test Performance MRR 0.6783091352356495\n",
      "epoch:2/10, batch:255/508, loss:0.20858381688594818\n",
      "epoch:2/10, batch:256/508, loss:0.25075390934944153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2/10, batch:257/508, loss:0.2860791087150574\n",
      "epoch:2/10, batch:258/508, loss:0.32776716351509094\n",
      "epoch:2/10, batch:259/508, loss:0.31448066234588623\n",
      "epoch:2/10, batch:260/508, loss:0.2873518466949463\n",
      "epoch:2/10, batch:261/508, loss:0.2835582494735718\n",
      "epoch:2/10, batch:262/508, loss:0.21262376010417938\n",
      "epoch:2/10, batch:263/508, loss:0.23099285364151\n",
      "epoch:2/10, batch:264/508, loss:0.3300468921661377\n",
      "epoch:2/10, batch:265/508, loss:0.24554404616355896\n",
      "epoch:2/10, batch:266/508, loss:0.33389902114868164\n",
      "epoch:2/10, batch:267/508, loss:0.3125488758087158\n",
      "epoch:2/10, batch:268/508, loss:0.3255211412906647\n",
      "epoch:2/10, batch:269/508, loss:0.30353912711143494\n",
      "epoch:2/10, batch:270/508, loss:0.29936525225639343\n",
      "epoch:2/10, batch:271/508, loss:0.3105025589466095\n",
      "epoch:2/10, batch:272/508, loss:0.2713741660118103\n",
      "epoch:2/10, batch:273/508, loss:0.2454729527235031\n",
      "epoch:2/10, batch:274/508, loss:0.26846587657928467\n",
      "epoch:2/10, batch:275/508, loss:0.2546684145927429\n",
      "epoch:2/10, batch:276/508, loss:0.31605836749076843\n",
      "epoch:2/10, batch:277/508, loss:0.3371228575706482\n",
      "epoch:2/10, batch:278/508, loss:0.2609727084636688\n",
      "epoch:2/10, batch:279/508, loss:0.2903025150299072\n",
      "epoch:2/10, batch:280/508, loss:0.2464022934436798\n",
      "epoch:2/10, batch:281/508, loss:0.2369368076324463\n",
      "epoch:2/10, batch:282/508, loss:0.31786230206489563\n",
      "epoch:2/10, batch:283/508, loss:0.3069600760936737\n",
      "epoch:2/10, batch:284/508, loss:0.27798697352409363\n",
      "epoch:2/10, batch:285/508, loss:0.2911645174026489\n",
      "epoch:2/10, batch:286/508, loss:0.33473411202430725\n",
      "epoch:2/10, batch:287/508, loss:0.3022741675376892\n",
      "epoch:2/10, batch:288/508, loss:0.24681176245212555\n",
      "epoch:2/10, batch:289/508, loss:0.2856154143810272\n",
      "epoch:2/10, batch:290/508, loss:0.2929941713809967\n",
      "epoch:2/10, batch:291/508, loss:0.2598055601119995\n",
      "epoch:2/10, batch:292/508, loss:0.31825143098831177\n",
      "epoch:2/10, batch:293/508, loss:0.3073139786720276\n",
      "epoch:2/10, batch:294/508, loss:0.31765300035476685\n",
      "epoch:2/10, batch:295/508, loss:0.2516104578971863\n",
      "epoch:2/10, batch:296/508, loss:0.291737824678421\n",
      "epoch:2/10, batch:297/508, loss:0.26541468501091003\n",
      "epoch:2/10, batch:298/508, loss:0.26905691623687744\n",
      "epoch:2/10, batch:299/508, loss:0.2726133167743683\n",
      "epoch:2/10, batch:300/508, loss:0.2509792447090149\n",
      "epoch:2/10, batch:301/508, loss:0.3559732735157013\n",
      "epoch:2/10, batch:302/508, loss:0.26111406087875366\n",
      "epoch:2/10, batch:303/508, loss:0.27418118715286255\n",
      "epoch:2/10, batch:304/508, loss:0.25609269738197327\n",
      "epoch:2/10, batch:305/508, loss:0.3499826192855835\n",
      "epoch:2/10, batch:306/508, loss:0.2431693971157074\n",
      "epoch:2/10, batch:307/508, loss:0.24984903633594513\n",
      "epoch:2/10, batch:308/508, loss:0.3037714958190918\n",
      "epoch:2/10, batch:309/508, loss:0.30268216133117676\n",
      "epoch:2/10, batch:310/508, loss:0.3062468469142914\n",
      "epoch:2/10, batch:311/508, loss:0.24845555424690247\n",
      "epoch:2/10, batch:312/508, loss:0.2706694006919861\n",
      "epoch:2/10, batch:313/508, loss:0.21689718961715698\n",
      "epoch:2/10, batch:314/508, loss:0.23304013907909393\n",
      "epoch:2/10, batch:315/508, loss:0.2911016345024109\n",
      "epoch:2/10, batch:316/508, loss:0.3307326138019562\n",
      "epoch:2/10, batch:317/508, loss:0.32262423634529114\n",
      "epoch:2/10, batch:318/508, loss:0.23533320426940918\n",
      "epoch:2/10, batch:319/508, loss:0.28496262431144714\n",
      "epoch:2/10, batch:320/508, loss:0.2857992947101593\n",
      "epoch:2/10, batch:321/508, loss:0.26262006163597107\n",
      "epoch:2/10, batch:322/508, loss:0.2823537290096283\n",
      "epoch:2/10, batch:323/508, loss:0.24378056824207306\n",
      "epoch:2/10, batch:324/508, loss:0.2922471761703491\n",
      "epoch:2/10, batch:325/508, loss:0.288044810295105\n",
      "epoch:2/10, batch:326/508, loss:0.3392651081085205\n",
      "epoch:2/10, batch:327/508, loss:0.2736552357673645\n",
      "epoch:2/10, batch:328/508, loss:0.26664257049560547\n",
      "epoch:2/10, batch:329/508, loss:0.28678858280181885\n",
      "epoch:2/10, batch:330/508, loss:0.37296196818351746\n",
      "epoch:2/10, batch:331/508, loss:0.33564189076423645\n",
      "epoch:2/10, batch:332/508, loss:0.28654786944389343\n",
      "epoch:2/10, batch:333/508, loss:0.27629315853118896\n",
      "epoch:2/10, batch:334/508, loss:0.2884795367717743\n",
      "epoch:2/10, batch:335/508, loss:0.27392134070396423\n",
      "epoch:2/10, batch:336/508, loss:0.2627147138118744\n",
      "epoch:2/10, batch:337/508, loss:0.23333296179771423\n",
      "epoch:2/10, batch:338/508, loss:0.29558053612709045\n",
      "epoch:2/10, batch:339/508, loss:0.2471473515033722\n",
      "epoch:2/10, batch:340/508, loss:0.23042984306812286\n",
      "epoch:2/10, batch:341/508, loss:0.2943592369556427\n",
      "epoch:2/10, batch:342/508, loss:0.20265959203243256\n",
      "epoch:2/10, batch:343/508, loss:0.3343381881713867\n",
      "epoch:2/10, batch:344/508, loss:0.2589758634567261\n",
      "epoch:2/10, batch:345/508, loss:0.3493489921092987\n",
      "epoch:2/10, batch:346/508, loss:0.2318449169397354\n",
      "epoch:2/10, batch:347/508, loss:0.3078044652938843\n",
      "epoch:2/10, batch:348/508, loss:0.30368849635124207\n",
      "epoch:2/10, batch:349/508, loss:0.30504387617111206\n",
      "epoch:2/10, batch:350/508, loss:0.3299597501754761\n",
      "epoch:2/10, batch:351/508, loss:0.26318755745887756\n",
      "epoch:2/10, batch:352/508, loss:0.30616146326065063\n",
      "epoch:2/10, batch:353/508, loss:0.2697938084602356\n",
      "epoch:2/10, batch:354/508, loss:0.3000875413417816\n",
      "epoch:2/10, batch:355/508, loss:0.3388292193412781\n",
      "epoch:2/10, batch:356/508, loss:0.32553187012672424\n",
      "epoch:2/10, batch:357/508, loss:0.2722792625427246\n",
      "epoch:2/10, batch:358/508, loss:0.31599628925323486\n",
      "epoch:2/10, batch:359/508, loss:0.24407640099525452\n",
      "epoch:2/10, batch:360/508, loss:0.2450336068868637\n",
      "epoch:2/10, batch:361/508, loss:0.30899614095687866\n",
      "epoch:2/10, batch:362/508, loss:0.29123568534851074\n",
      "epoch:2/10, batch:363/508, loss:0.33426788449287415\n",
      "epoch:2/10, batch:364/508, loss:0.28557053208351135\n",
      "epoch:2/10, batch:365/508, loss:0.19016429781913757\n",
      "epoch:2/10, batch:366/508, loss:0.23313160240650177\n",
      "epoch:2/10, batch:367/508, loss:0.2749357521533966\n",
      "epoch:2/10, batch:368/508, loss:0.24320970475673676\n",
      "epoch:2/10, batch:369/508, loss:0.3524196445941925\n",
      "epoch:2/10, batch:370/508, loss:0.2574556767940521\n",
      "epoch:2/10, batch:371/508, loss:0.312705934047699\n",
      "epoch:2/10, batch:372/508, loss:0.2439342588186264\n",
      "epoch:2/10, batch:373/508, loss:0.36250078678131104\n",
      "epoch:2/10, batch:374/508, loss:0.2825973927974701\n",
      "epoch:2/10, batch:375/508, loss:0.2736125886440277\n",
      "epoch:2/10, batch:376/508, loss:0.34800270199775696\n",
      "epoch:2/10, batch:377/508, loss:0.30059579014778137\n",
      "epoch:2/10, batch:378/508, loss:0.2736915946006775\n",
      "epoch:2/10, batch:379/508, loss:0.2558133006095886\n",
      "epoch:2/10, batch:380/508, loss:0.292501300573349\n",
      "epoch:2/10, batch:381/508, loss:0.23853333294391632\n",
      "evaluating ....\n",
      "Dev Performance P@5 0.386335403727\n",
      "Dev Performance P@1 0.521739130435\n",
      "Dev Performance MAP 0.6218392734920285\n",
      "Dev Performance MRR 0.658550076562499\n",
      "------------------\n",
      "Test Performance P@5 0.357232704403\n",
      "Test Performance P@1 0.566037735849\n",
      "Test Performance MAP 0.6428336345892997\n",
      "Test Performance MRR 0.6991516138611471\n",
      "epoch:2/10, batch:382/508, loss:0.285795122385025\n",
      "epoch:2/10, batch:383/508, loss:0.24005478620529175\n",
      "epoch:2/10, batch:384/508, loss:0.27173829078674316\n",
      "epoch:2/10, batch:385/508, loss:0.2570331394672394\n",
      "epoch:2/10, batch:386/508, loss:0.25705334544181824\n",
      "epoch:2/10, batch:387/508, loss:0.22885698080062866\n",
      "epoch:2/10, batch:388/508, loss:0.2919207513332367\n",
      "epoch:2/10, batch:389/508, loss:0.30971843004226685\n",
      "epoch:2/10, batch:390/508, loss:0.2500399053096771\n",
      "epoch:2/10, batch:391/508, loss:0.2343270480632782\n",
      "epoch:2/10, batch:392/508, loss:0.26286202669143677\n",
      "epoch:2/10, batch:393/508, loss:0.2610328197479248\n",
      "epoch:2/10, batch:394/508, loss:0.28800952434539795\n",
      "epoch:2/10, batch:395/508, loss:0.3663491904735565\n",
      "epoch:2/10, batch:396/508, loss:0.34218838810920715\n",
      "epoch:2/10, batch:397/508, loss:0.2978973388671875\n",
      "epoch:2/10, batch:398/508, loss:0.25927087664604187\n",
      "epoch:2/10, batch:399/508, loss:0.2806132137775421\n",
      "epoch:2/10, batch:400/508, loss:0.3035811185836792\n",
      "epoch:2/10, batch:401/508, loss:0.3009841740131378\n",
      "epoch:2/10, batch:402/508, loss:0.24110832810401917\n",
      "epoch:2/10, batch:403/508, loss:0.2770608961582184\n",
      "epoch:2/10, batch:404/508, loss:0.35674864053726196\n",
      "epoch:2/10, batch:405/508, loss:0.22047068178653717\n",
      "epoch:2/10, batch:406/508, loss:0.2640460133552551\n",
      "epoch:2/10, batch:407/508, loss:0.17204302549362183\n",
      "epoch:2/10, batch:408/508, loss:0.3395335376262665\n",
      "epoch:2/10, batch:409/508, loss:0.24329353868961334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2/10, batch:410/508, loss:0.27778229117393494\n",
      "epoch:2/10, batch:411/508, loss:0.23350785672664642\n",
      "epoch:2/10, batch:412/508, loss:0.2629665732383728\n",
      "epoch:2/10, batch:413/508, loss:0.30703479051589966\n",
      "epoch:2/10, batch:414/508, loss:0.28826048970222473\n",
      "epoch:2/10, batch:415/508, loss:0.26076552271842957\n",
      "epoch:2/10, batch:416/508, loss:0.29807403683662415\n",
      "epoch:2/10, batch:417/508, loss:0.3000863492488861\n",
      "epoch:2/10, batch:418/508, loss:0.26149219274520874\n",
      "epoch:2/10, batch:419/508, loss:0.2685050368309021\n",
      "epoch:2/10, batch:420/508, loss:0.19896741211414337\n",
      "epoch:2/10, batch:421/508, loss:0.27996423840522766\n",
      "epoch:2/10, batch:422/508, loss:0.2815401256084442\n",
      "epoch:2/10, batch:423/508, loss:0.2817631959915161\n",
      "epoch:2/10, batch:424/508, loss:0.2883480489253998\n",
      "epoch:2/10, batch:425/508, loss:0.2439553141593933\n",
      "epoch:2/10, batch:426/508, loss:0.26374220848083496\n",
      "epoch:2/10, batch:427/508, loss:0.29972630739212036\n",
      "epoch:2/10, batch:428/508, loss:0.2568334937095642\n",
      "epoch:2/10, batch:429/508, loss:0.26001453399658203\n",
      "epoch:2/10, batch:430/508, loss:0.2553519010543823\n",
      "epoch:2/10, batch:431/508, loss:0.260250061750412\n",
      "epoch:2/10, batch:432/508, loss:0.292507141828537\n",
      "epoch:2/10, batch:433/508, loss:0.34217289090156555\n",
      "epoch:2/10, batch:434/508, loss:0.3279263377189636\n",
      "epoch:2/10, batch:435/508, loss:0.19599731266498566\n",
      "epoch:2/10, batch:436/508, loss:0.3226943016052246\n",
      "epoch:2/10, batch:437/508, loss:0.35529106855392456\n",
      "epoch:2/10, batch:438/508, loss:0.28385159373283386\n",
      "epoch:2/10, batch:439/508, loss:0.2305637001991272\n",
      "epoch:2/10, batch:440/508, loss:0.26653996109962463\n",
      "epoch:2/10, batch:441/508, loss:0.3328767418861389\n",
      "epoch:2/10, batch:442/508, loss:0.30633002519607544\n",
      "epoch:2/10, batch:443/508, loss:0.2657277584075928\n",
      "epoch:2/10, batch:444/508, loss:0.32223328948020935\n",
      "epoch:2/10, batch:445/508, loss:0.2810661494731903\n",
      "epoch:2/10, batch:446/508, loss:0.23725730180740356\n",
      "epoch:2/10, batch:447/508, loss:0.36226269602775574\n",
      "epoch:2/10, batch:448/508, loss:0.28106144070625305\n",
      "epoch:2/10, batch:449/508, loss:0.2990300953388214\n",
      "epoch:2/10, batch:450/508, loss:0.25637152791023254\n",
      "epoch:2/10, batch:451/508, loss:0.30121415853500366\n",
      "epoch:2/10, batch:452/508, loss:0.2683190405368805\n",
      "epoch:2/10, batch:453/508, loss:0.29891055822372437\n",
      "epoch:2/10, batch:454/508, loss:0.27992483973503113\n",
      "epoch:2/10, batch:455/508, loss:0.2619301974773407\n",
      "epoch:2/10, batch:456/508, loss:0.2523415982723236\n",
      "epoch:2/10, batch:457/508, loss:0.28739434480667114\n",
      "epoch:2/10, batch:458/508, loss:0.32942965626716614\n",
      "epoch:2/10, batch:459/508, loss:0.29393312335014343\n",
      "epoch:2/10, batch:460/508, loss:0.35283249616622925\n",
      "epoch:2/10, batch:461/508, loss:0.1988304853439331\n",
      "epoch:2/10, batch:462/508, loss:0.29407885670661926\n",
      "epoch:2/10, batch:463/508, loss:0.24541179835796356\n",
      "epoch:2/10, batch:464/508, loss:0.23669153451919556\n",
      "epoch:2/10, batch:465/508, loss:0.31073489785194397\n",
      "epoch:2/10, batch:466/508, loss:0.28481975197792053\n",
      "epoch:2/10, batch:467/508, loss:0.30990535020828247\n",
      "epoch:2/10, batch:468/508, loss:0.2879113554954529\n",
      "epoch:2/10, batch:469/508, loss:0.26987746357917786\n",
      "epoch:2/10, batch:470/508, loss:0.3296479880809784\n",
      "epoch:2/10, batch:471/508, loss:0.2537691295146942\n",
      "epoch:2/10, batch:472/508, loss:0.3443068861961365\n",
      "epoch:2/10, batch:473/508, loss:0.25275418162345886\n",
      "epoch:2/10, batch:474/508, loss:0.17018504440784454\n",
      "epoch:2/10, batch:475/508, loss:0.25257253646850586\n",
      "epoch:2/10, batch:476/508, loss:0.2205026000738144\n",
      "epoch:2/10, batch:477/508, loss:0.33367857336997986\n",
      "epoch:2/10, batch:478/508, loss:0.2644886076450348\n",
      "epoch:2/10, batch:479/508, loss:0.25374361872673035\n",
      "epoch:2/10, batch:480/508, loss:0.22997838258743286\n",
      "epoch:2/10, batch:481/508, loss:0.24520932137966156\n",
      "epoch:2/10, batch:482/508, loss:0.3580017387866974\n",
      "epoch:2/10, batch:483/508, loss:0.2807852327823639\n",
      "epoch:2/10, batch:484/508, loss:0.20806623995304108\n",
      "epoch:2/10, batch:485/508, loss:0.15869876742362976\n",
      "epoch:2/10, batch:486/508, loss:0.2363629788160324\n",
      "epoch:2/10, batch:487/508, loss:0.2590100169181824\n",
      "epoch:2/10, batch:488/508, loss:0.33770138025283813\n",
      "epoch:2/10, batch:489/508, loss:0.27946600317955017\n",
      "epoch:2/10, batch:490/508, loss:0.29444339871406555\n",
      "epoch:2/10, batch:491/508, loss:0.32840028405189514\n",
      "epoch:2/10, batch:492/508, loss:0.2692677676677704\n",
      "epoch:2/10, batch:493/508, loss:0.23310644924640656\n",
      "epoch:2/10, batch:494/508, loss:0.21216337382793427\n",
      "epoch:2/10, batch:495/508, loss:0.3043361306190491\n",
      "epoch:2/10, batch:496/508, loss:0.31998375058174133\n",
      "epoch:2/10, batch:497/508, loss:0.27505791187286377\n",
      "epoch:2/10, batch:498/508, loss:0.21386228501796722\n",
      "epoch:2/10, batch:499/508, loss:0.2792043685913086\n",
      "epoch:2/10, batch:500/508, loss:0.26505157351493835\n",
      "epoch:2/10, batch:501/508, loss:0.22104522585868835\n",
      "epoch:2/10, batch:502/508, loss:0.2701018452644348\n",
      "epoch:2/10, batch:503/508, loss:0.23458364605903625\n",
      "epoch:2/10, batch:504/508, loss:0.28198182582855225\n",
      "epoch:2/10, batch:505/508, loss:0.37670356035232544\n",
      "epoch:2/10, batch:506/508, loss:0.23008622229099274\n",
      "epoch:2/10, batch:507/508, loss:0.2813930809497833\n",
      "epoch:2/10, batch:508/508, loss:0.25222060084342957\n",
      "evaluating ....\n",
      "Dev Performance P@5 0.40248447205\n",
      "Dev Performance P@1 0.515527950311\n",
      "Dev Performance MAP 0.6274113059747856\n",
      "Dev Performance MRR 0.6634238310708895\n",
      "------------------\n",
      "Test Performance P@5 0.366037735849\n",
      "Test Performance P@1 0.547169811321\n",
      "Test Performance MAP 0.6367860042442657\n",
      "Test Performance MRR 0.6885161015746912\n",
      "epoch:3/10, batch:1/508, loss:0.25909996032714844\n",
      "epoch:3/10, batch:2/508, loss:0.28820616006851196\n",
      "epoch:3/10, batch:3/508, loss:0.32705846428871155\n",
      "epoch:3/10, batch:4/508, loss:0.25712019205093384\n",
      "epoch:3/10, batch:5/508, loss:0.3083360493183136\n",
      "epoch:3/10, batch:6/508, loss:0.3279387354850769\n",
      "epoch:3/10, batch:7/508, loss:0.27583596110343933\n",
      "epoch:3/10, batch:8/508, loss:0.1718999296426773\n",
      "epoch:3/10, batch:9/508, loss:0.28641775250434875\n",
      "epoch:3/10, batch:10/508, loss:0.18429264426231384\n",
      "epoch:3/10, batch:11/508, loss:0.21325069665908813\n",
      "epoch:3/10, batch:12/508, loss:0.2850063145160675\n",
      "epoch:3/10, batch:13/508, loss:0.17825168371200562\n",
      "epoch:3/10, batch:14/508, loss:0.25417572259902954\n",
      "epoch:3/10, batch:15/508, loss:0.26230067014694214\n",
      "epoch:3/10, batch:16/508, loss:0.21195629239082336\n",
      "epoch:3/10, batch:17/508, loss:0.29731717705726624\n",
      "epoch:3/10, batch:18/508, loss:0.32705965638160706\n",
      "epoch:3/10, batch:19/508, loss:0.2157435119152069\n",
      "epoch:3/10, batch:20/508, loss:0.21678227186203003\n",
      "epoch:3/10, batch:21/508, loss:0.27184030413627625\n",
      "epoch:3/10, batch:22/508, loss:0.2308805137872696\n",
      "epoch:3/10, batch:23/508, loss:0.25349748134613037\n",
      "epoch:3/10, batch:24/508, loss:0.24394702911376953\n",
      "epoch:3/10, batch:25/508, loss:0.2786276936531067\n",
      "epoch:3/10, batch:26/508, loss:0.2974563539028168\n",
      "epoch:3/10, batch:27/508, loss:0.2312297374010086\n",
      "epoch:3/10, batch:28/508, loss:0.2548072636127472\n",
      "epoch:3/10, batch:29/508, loss:0.28313133120536804\n",
      "epoch:3/10, batch:30/508, loss:0.2996412217617035\n",
      "epoch:3/10, batch:31/508, loss:0.2787283957004547\n",
      "epoch:3/10, batch:32/508, loss:0.29401594400405884\n",
      "epoch:3/10, batch:33/508, loss:0.22872982919216156\n",
      "epoch:3/10, batch:34/508, loss:0.2506829798221588\n",
      "epoch:3/10, batch:35/508, loss:0.2430885136127472\n",
      "epoch:3/10, batch:36/508, loss:0.29242435097694397\n",
      "epoch:3/10, batch:37/508, loss:0.21867512166500092\n",
      "epoch:3/10, batch:38/508, loss:0.27984800934791565\n",
      "epoch:3/10, batch:39/508, loss:0.2091512382030487\n",
      "epoch:3/10, batch:40/508, loss:0.27479788661003113\n",
      "epoch:3/10, batch:41/508, loss:0.22078849375247955\n",
      "epoch:3/10, batch:42/508, loss:0.24430765211582184\n",
      "epoch:3/10, batch:43/508, loss:0.292997270822525\n",
      "epoch:3/10, batch:44/508, loss:0.22285832464694977\n",
      "epoch:3/10, batch:45/508, loss:0.18725207448005676\n",
      "epoch:3/10, batch:46/508, loss:0.241750568151474\n",
      "epoch:3/10, batch:47/508, loss:0.3225404918193817\n",
      "epoch:3/10, batch:48/508, loss:0.3041662275791168\n",
      "epoch:3/10, batch:49/508, loss:0.2994582951068878\n",
      "epoch:3/10, batch:50/508, loss:0.2754771113395691\n",
      "epoch:3/10, batch:51/508, loss:0.23133830726146698\n",
      "epoch:3/10, batch:52/508, loss:0.1872466504573822\n",
      "epoch:3/10, batch:53/508, loss:0.2576241195201874\n",
      "epoch:3/10, batch:54/508, loss:0.26527512073516846\n",
      "epoch:3/10, batch:55/508, loss:0.293147474527359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3/10, batch:56/508, loss:0.27423927187919617\n",
      "epoch:3/10, batch:57/508, loss:0.27509769797325134\n",
      "epoch:3/10, batch:58/508, loss:0.18880820274353027\n",
      "epoch:3/10, batch:59/508, loss:0.3019920289516449\n",
      "epoch:3/10, batch:60/508, loss:0.28015244007110596\n",
      "epoch:3/10, batch:61/508, loss:0.2744930684566498\n",
      "epoch:3/10, batch:62/508, loss:0.20420804619789124\n",
      "epoch:3/10, batch:63/508, loss:0.30691054463386536\n",
      "epoch:3/10, batch:64/508, loss:0.32762622833251953\n",
      "epoch:3/10, batch:65/508, loss:0.3215194642543793\n",
      "epoch:3/10, batch:66/508, loss:0.30190256237983704\n",
      "epoch:3/10, batch:67/508, loss:0.22764849662780762\n",
      "epoch:3/10, batch:68/508, loss:0.2205575853586197\n",
      "epoch:3/10, batch:69/508, loss:0.21595411002635956\n",
      "epoch:3/10, batch:70/508, loss:0.23625540733337402\n",
      "epoch:3/10, batch:71/508, loss:0.2658565938472748\n",
      "epoch:3/10, batch:72/508, loss:0.26144859194755554\n",
      "epoch:3/10, batch:73/508, loss:0.19822362065315247\n",
      "epoch:3/10, batch:74/508, loss:0.26010099053382874\n",
      "epoch:3/10, batch:75/508, loss:0.32241055369377136\n",
      "epoch:3/10, batch:76/508, loss:0.2880328893661499\n",
      "epoch:3/10, batch:77/508, loss:0.31654977798461914\n",
      "epoch:3/10, batch:78/508, loss:0.23865382373332977\n",
      "epoch:3/10, batch:79/508, loss:0.30076801776885986\n",
      "epoch:3/10, batch:80/508, loss:0.2959589958190918\n",
      "epoch:3/10, batch:81/508, loss:0.2137177288532257\n",
      "epoch:3/10, batch:82/508, loss:0.22572575509548187\n",
      "epoch:3/10, batch:83/508, loss:0.3124663829803467\n",
      "epoch:3/10, batch:84/508, loss:0.22327496111392975\n",
      "epoch:3/10, batch:85/508, loss:0.2265785187482834\n",
      "epoch:3/10, batch:86/508, loss:0.26935461163520813\n",
      "epoch:3/10, batch:87/508, loss:0.2335132360458374\n",
      "epoch:3/10, batch:88/508, loss:0.24963362514972687\n",
      "epoch:3/10, batch:89/508, loss:0.269980251789093\n",
      "epoch:3/10, batch:90/508, loss:0.20992200076580048\n",
      "epoch:3/10, batch:91/508, loss:0.2551056444644928\n",
      "epoch:3/10, batch:92/508, loss:0.2934685945510864\n",
      "epoch:3/10, batch:93/508, loss:0.24751348793506622\n",
      "epoch:3/10, batch:94/508, loss:0.24980612099170685\n",
      "epoch:3/10, batch:95/508, loss:0.20658929646015167\n",
      "epoch:3/10, batch:96/508, loss:0.3076230585575104\n",
      "epoch:3/10, batch:97/508, loss:0.2998879849910736\n",
      "epoch:3/10, batch:98/508, loss:0.21514298021793365\n",
      "epoch:3/10, batch:99/508, loss:0.19060193002223969\n",
      "epoch:3/10, batch:100/508, loss:0.2912787199020386\n",
      "epoch:3/10, batch:101/508, loss:0.3142105042934418\n",
      "epoch:3/10, batch:102/508, loss:0.2175346165895462\n",
      "epoch:3/10, batch:103/508, loss:0.30313944816589355\n",
      "epoch:3/10, batch:104/508, loss:0.39094552397727966\n",
      "epoch:3/10, batch:105/508, loss:0.26150038838386536\n",
      "epoch:3/10, batch:106/508, loss:0.24882301688194275\n",
      "epoch:3/10, batch:107/508, loss:0.2497018277645111\n",
      "epoch:3/10, batch:108/508, loss:0.2749217450618744\n",
      "epoch:3/10, batch:109/508, loss:0.24202445149421692\n",
      "epoch:3/10, batch:110/508, loss:0.2740902304649353\n",
      "epoch:3/10, batch:111/508, loss:0.22831253707408905\n",
      "epoch:3/10, batch:112/508, loss:0.2550213932991028\n",
      "epoch:3/10, batch:113/508, loss:0.22343909740447998\n",
      "epoch:3/10, batch:114/508, loss:0.33396923542022705\n",
      "epoch:3/10, batch:115/508, loss:0.2819824516773224\n",
      "epoch:3/10, batch:116/508, loss:0.22329939901828766\n",
      "epoch:3/10, batch:117/508, loss:0.2693512737751007\n",
      "epoch:3/10, batch:118/508, loss:0.22220513224601746\n",
      "epoch:3/10, batch:119/508, loss:0.22496692836284637\n",
      "epoch:3/10, batch:120/508, loss:0.2635834813117981\n",
      "epoch:3/10, batch:121/508, loss:0.2814787030220032\n",
      "epoch:3/10, batch:122/508, loss:0.2797229588031769\n",
      "epoch:3/10, batch:123/508, loss:0.26703399419784546\n",
      "epoch:3/10, batch:124/508, loss:0.2519010007381439\n",
      "epoch:3/10, batch:125/508, loss:0.28250324726104736\n",
      "epoch:3/10, batch:126/508, loss:0.26370203495025635\n",
      "epoch:3/10, batch:127/508, loss:0.25296834111213684\n",
      "evaluating ....\n",
      "Dev Performance P@5 0.40248447205\n",
      "Dev Performance P@1 0.546583850932\n",
      "Dev Performance MAP 0.6378182182512931\n",
      "Dev Performance MRR 0.6810205865056066\n",
      "------------------\n",
      "Test Performance P@5 0.374842767296\n",
      "Test Performance P@1 0.547169811321\n",
      "Test Performance MAP 0.6402121312667964\n",
      "Test Performance MRR 0.6937753190703129\n",
      "epoch:3/10, batch:128/508, loss:0.2434810847043991\n",
      "epoch:3/10, batch:129/508, loss:0.21265695989131927\n",
      "epoch:3/10, batch:130/508, loss:0.3001351058483124\n",
      "epoch:3/10, batch:131/508, loss:0.3301771581172943\n",
      "epoch:3/10, batch:132/508, loss:0.2112472653388977\n",
      "epoch:3/10, batch:133/508, loss:0.22678810358047485\n",
      "epoch:3/10, batch:134/508, loss:0.3322170376777649\n",
      "epoch:3/10, batch:135/508, loss:0.2337544560432434\n",
      "epoch:3/10, batch:136/508, loss:0.18277521431446075\n",
      "epoch:3/10, batch:137/508, loss:0.22681504487991333\n",
      "epoch:3/10, batch:138/508, loss:0.26267990469932556\n",
      "epoch:3/10, batch:139/508, loss:0.22994199395179749\n",
      "epoch:3/10, batch:140/508, loss:0.3405754864215851\n",
      "epoch:3/10, batch:141/508, loss:0.3295297920703888\n",
      "epoch:3/10, batch:142/508, loss:0.23815491795539856\n",
      "epoch:3/10, batch:143/508, loss:0.2746180295944214\n",
      "epoch:3/10, batch:144/508, loss:0.2737407982349396\n",
      "epoch:3/10, batch:145/508, loss:0.2169494926929474\n",
      "epoch:3/10, batch:146/508, loss:0.22188276052474976\n",
      "epoch:3/10, batch:147/508, loss:0.2572207748889923\n",
      "epoch:3/10, batch:148/508, loss:0.23397770524024963\n",
      "epoch:3/10, batch:149/508, loss:0.22332477569580078\n",
      "epoch:3/10, batch:150/508, loss:0.2531866729259491\n",
      "epoch:3/10, batch:151/508, loss:0.34789425134658813\n",
      "epoch:3/10, batch:152/508, loss:0.2796722650527954\n",
      "epoch:3/10, batch:153/508, loss:0.27045196294784546\n",
      "epoch:3/10, batch:154/508, loss:0.29065024852752686\n",
      "epoch:3/10, batch:155/508, loss:0.26879358291625977\n",
      "epoch:3/10, batch:156/508, loss:0.22762519121170044\n",
      "epoch:3/10, batch:157/508, loss:0.20367929339408875\n",
      "epoch:3/10, batch:158/508, loss:0.20551763474941254\n",
      "epoch:3/10, batch:159/508, loss:0.28316530585289\n",
      "epoch:3/10, batch:160/508, loss:0.29960936307907104\n",
      "epoch:3/10, batch:161/508, loss:0.2746637165546417\n",
      "epoch:3/10, batch:162/508, loss:0.2701825499534607\n",
      "epoch:3/10, batch:163/508, loss:0.3270082473754883\n",
      "epoch:3/10, batch:164/508, loss:0.3196984529495239\n",
      "epoch:3/10, batch:165/508, loss:0.30037638545036316\n",
      "epoch:3/10, batch:166/508, loss:0.28059741854667664\n",
      "epoch:3/10, batch:167/508, loss:0.21520306169986725\n",
      "epoch:3/10, batch:168/508, loss:0.27688291668891907\n",
      "epoch:3/10, batch:169/508, loss:0.18718738853931427\n",
      "epoch:3/10, batch:170/508, loss:0.25719255208969116\n",
      "epoch:3/10, batch:171/508, loss:0.30646881461143494\n",
      "epoch:3/10, batch:172/508, loss:0.24161459505558014\n",
      "epoch:3/10, batch:173/508, loss:0.25632965564727783\n",
      "epoch:3/10, batch:174/508, loss:0.21778756380081177\n",
      "epoch:3/10, batch:175/508, loss:0.29723620414733887\n",
      "epoch:3/10, batch:176/508, loss:0.26990875601768494\n",
      "epoch:3/10, batch:177/508, loss:0.2543359100818634\n",
      "epoch:3/10, batch:178/508, loss:0.24303077161312103\n",
      "epoch:3/10, batch:179/508, loss:0.24021832644939423\n",
      "epoch:3/10, batch:180/508, loss:0.24956126511096954\n",
      "epoch:3/10, batch:181/508, loss:0.25386595726013184\n",
      "epoch:3/10, batch:182/508, loss:0.22897298634052277\n",
      "epoch:3/10, batch:183/508, loss:0.3250064253807068\n",
      "epoch:3/10, batch:184/508, loss:0.24858301877975464\n",
      "epoch:3/10, batch:185/508, loss:0.31430646777153015\n",
      "epoch:3/10, batch:186/508, loss:0.2724706828594208\n",
      "epoch:3/10, batch:187/508, loss:0.3687128722667694\n",
      "epoch:3/10, batch:188/508, loss:0.28135743737220764\n",
      "epoch:3/10, batch:189/508, loss:0.22868987917900085\n",
      "epoch:3/10, batch:190/508, loss:0.2839767336845398\n",
      "epoch:3/10, batch:191/508, loss:0.3054310381412506\n",
      "epoch:3/10, batch:192/508, loss:0.3369307219982147\n",
      "epoch:3/10, batch:193/508, loss:0.23125383257865906\n",
      "epoch:3/10, batch:194/508, loss:0.2651709020137787\n",
      "epoch:3/10, batch:195/508, loss:0.2858661711215973\n",
      "epoch:3/10, batch:196/508, loss:0.22386103868484497\n",
      "epoch:3/10, batch:197/508, loss:0.2998960614204407\n",
      "epoch:3/10, batch:198/508, loss:0.25551825761795044\n",
      "epoch:3/10, batch:199/508, loss:0.21388515830039978\n",
      "epoch:3/10, batch:200/508, loss:0.29742616415023804\n",
      "epoch:3/10, batch:201/508, loss:0.3535662591457367\n",
      "epoch:3/10, batch:202/508, loss:0.27291858196258545\n",
      "epoch:3/10, batch:203/508, loss:0.23238101601600647\n",
      "epoch:3/10, batch:204/508, loss:0.2489049732685089\n",
      "epoch:3/10, batch:205/508, loss:0.26321277022361755\n",
      "epoch:3/10, batch:206/508, loss:0.3039233088493347\n",
      "epoch:3/10, batch:207/508, loss:0.2417284995317459\n",
      "epoch:3/10, batch:208/508, loss:0.25928354263305664\n",
      "epoch:3/10, batch:209/508, loss:0.22680486738681793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3/10, batch:210/508, loss:0.2434147447347641\n",
      "epoch:3/10, batch:211/508, loss:0.2558598518371582\n",
      "epoch:3/10, batch:212/508, loss:0.37800005078315735\n",
      "epoch:3/10, batch:213/508, loss:0.20981402695178986\n",
      "epoch:3/10, batch:214/508, loss:0.30235588550567627\n",
      "epoch:3/10, batch:215/508, loss:0.2826959788799286\n",
      "epoch:3/10, batch:216/508, loss:0.24250176548957825\n",
      "epoch:3/10, batch:217/508, loss:0.2992197871208191\n",
      "epoch:3/10, batch:218/508, loss:0.2629643976688385\n",
      "epoch:3/10, batch:219/508, loss:0.19408287107944489\n",
      "epoch:3/10, batch:220/508, loss:0.19412493705749512\n",
      "epoch:3/10, batch:221/508, loss:0.2577503025531769\n",
      "epoch:3/10, batch:222/508, loss:0.29893097281455994\n",
      "epoch:3/10, batch:223/508, loss:0.3149651288986206\n",
      "epoch:3/10, batch:224/508, loss:0.26383447647094727\n",
      "epoch:3/10, batch:225/508, loss:0.29929471015930176\n",
      "epoch:3/10, batch:226/508, loss:0.21213072538375854\n",
      "epoch:3/10, batch:227/508, loss:0.23090480268001556\n",
      "epoch:3/10, batch:228/508, loss:0.31558331847190857\n",
      "epoch:3/10, batch:229/508, loss:0.29070189595222473\n",
      "epoch:3/10, batch:230/508, loss:0.20169547200202942\n",
      "epoch:3/10, batch:231/508, loss:0.2623600363731384\n",
      "epoch:3/10, batch:232/508, loss:0.31396543979644775\n",
      "epoch:3/10, batch:233/508, loss:0.2886921167373657\n",
      "epoch:3/10, batch:234/508, loss:0.18175311386585236\n",
      "epoch:3/10, batch:235/508, loss:0.2719371020793915\n",
      "epoch:3/10, batch:236/508, loss:0.2498316764831543\n",
      "epoch:3/10, batch:237/508, loss:0.2853042781352997\n",
      "epoch:3/10, batch:238/508, loss:0.31719154119491577\n",
      "epoch:3/10, batch:239/508, loss:0.27964529395103455\n",
      "epoch:3/10, batch:240/508, loss:0.25448060035705566\n",
      "epoch:3/10, batch:241/508, loss:0.23221702873706818\n",
      "epoch:3/10, batch:242/508, loss:0.24335280060768127\n",
      "epoch:3/10, batch:243/508, loss:0.284335732460022\n",
      "epoch:3/10, batch:244/508, loss:0.28502774238586426\n",
      "epoch:3/10, batch:245/508, loss:0.16225099563598633\n",
      "epoch:3/10, batch:246/508, loss:0.21557463705539703\n",
      "epoch:3/10, batch:247/508, loss:0.287769079208374\n",
      "epoch:3/10, batch:248/508, loss:0.25367793440818787\n",
      "epoch:3/10, batch:249/508, loss:0.2176554948091507\n",
      "epoch:3/10, batch:250/508, loss:0.282670795917511\n",
      "epoch:3/10, batch:251/508, loss:0.21010638773441315\n",
      "epoch:3/10, batch:252/508, loss:0.21082735061645508\n",
      "epoch:3/10, batch:253/508, loss:0.26209819316864014\n",
      "epoch:3/10, batch:254/508, loss:0.2525523006916046\n",
      "evaluating ....\n",
      "Dev Performance P@5 0.398757763975\n",
      "Dev Performance P@1 0.496894409938\n",
      "Dev Performance MAP 0.6149121255410984\n",
      "Dev Performance MRR 0.6460687629668266\n",
      "------------------\n",
      "Test Performance P@5 0.374842767296\n",
      "Test Performance P@1 0.540880503145\n",
      "Test Performance MAP 0.6325231643279698\n",
      "Test Performance MRR 0.6871058416738655\n",
      "epoch:3/10, batch:255/508, loss:0.17262309789657593\n",
      "epoch:3/10, batch:256/508, loss:0.2042054533958435\n",
      "epoch:3/10, batch:257/508, loss:0.26621878147125244\n",
      "epoch:3/10, batch:258/508, loss:0.28194481134414673\n",
      "epoch:3/10, batch:259/508, loss:0.2914021611213684\n",
      "epoch:3/10, batch:260/508, loss:0.31679797172546387\n",
      "epoch:3/10, batch:261/508, loss:0.26288336515426636\n",
      "epoch:3/10, batch:262/508, loss:0.18369320034980774\n",
      "epoch:3/10, batch:263/508, loss:0.24185481667518616\n",
      "epoch:3/10, batch:264/508, loss:0.29078084230422974\n",
      "epoch:3/10, batch:265/508, loss:0.22617216408252716\n",
      "epoch:3/10, batch:266/508, loss:0.34259527921676636\n",
      "epoch:3/10, batch:267/508, loss:0.2857806980609894\n",
      "epoch:3/10, batch:268/508, loss:0.3022972643375397\n",
      "epoch:3/10, batch:269/508, loss:0.26342421770095825\n",
      "epoch:3/10, batch:270/508, loss:0.23546189069747925\n",
      "epoch:3/10, batch:271/508, loss:0.22623582184314728\n",
      "epoch:3/10, batch:272/508, loss:0.25429120659828186\n",
      "epoch:3/10, batch:273/508, loss:0.2156096249818802\n",
      "epoch:3/10, batch:274/508, loss:0.24301457405090332\n",
      "epoch:3/10, batch:275/508, loss:0.23746787011623383\n",
      "epoch:3/10, batch:276/508, loss:0.3018814027309418\n",
      "epoch:3/10, batch:277/508, loss:0.2906322777271271\n",
      "epoch:3/10, batch:278/508, loss:0.3066013753414154\n",
      "epoch:3/10, batch:279/508, loss:0.23236137628555298\n",
      "epoch:3/10, batch:280/508, loss:0.2522163689136505\n",
      "epoch:3/10, batch:281/508, loss:0.25253015756607056\n",
      "epoch:3/10, batch:282/508, loss:0.25688672065734863\n",
      "epoch:3/10, batch:283/508, loss:0.2775518298149109\n",
      "epoch:3/10, batch:284/508, loss:0.3051169216632843\n",
      "epoch:3/10, batch:285/508, loss:0.2832583487033844\n",
      "epoch:3/10, batch:286/508, loss:0.28179311752319336\n",
      "epoch:3/10, batch:287/508, loss:0.25783947110176086\n",
      "epoch:3/10, batch:288/508, loss:0.19111451506614685\n",
      "epoch:3/10, batch:289/508, loss:0.2296494096517563\n",
      "epoch:3/10, batch:290/508, loss:0.2958798110485077\n",
      "epoch:3/10, batch:291/508, loss:0.23995272815227509\n",
      "epoch:3/10, batch:292/508, loss:0.30982452630996704\n",
      "epoch:3/10, batch:293/508, loss:0.302216500043869\n",
      "epoch:3/10, batch:294/508, loss:0.26843973994255066\n",
      "epoch:3/10, batch:295/508, loss:0.23552007973194122\n",
      "epoch:3/10, batch:296/508, loss:0.241873100399971\n",
      "epoch:3/10, batch:297/508, loss:0.2525077760219574\n",
      "epoch:3/10, batch:298/508, loss:0.26359862089157104\n",
      "epoch:3/10, batch:299/508, loss:0.22560888528823853\n",
      "epoch:3/10, batch:300/508, loss:0.217834010720253\n",
      "epoch:3/10, batch:301/508, loss:0.32886451482772827\n",
      "epoch:3/10, batch:302/508, loss:0.26444077491760254\n",
      "epoch:3/10, batch:303/508, loss:0.25785031914711\n",
      "epoch:3/10, batch:304/508, loss:0.2509947121143341\n",
      "epoch:3/10, batch:305/508, loss:0.33235251903533936\n",
      "epoch:3/10, batch:306/508, loss:0.2375417798757553\n",
      "epoch:3/10, batch:307/508, loss:0.2400464564561844\n",
      "epoch:3/10, batch:308/508, loss:0.23739056289196014\n",
      "epoch:3/10, batch:309/508, loss:0.2760002315044403\n",
      "epoch:3/10, batch:310/508, loss:0.2486240416765213\n",
      "epoch:3/10, batch:311/508, loss:0.22524455189704895\n",
      "epoch:3/10, batch:312/508, loss:0.2628341317176819\n",
      "epoch:3/10, batch:313/508, loss:0.2513003647327423\n",
      "epoch:3/10, batch:314/508, loss:0.24886250495910645\n",
      "epoch:3/10, batch:315/508, loss:0.27023330330848694\n",
      "epoch:3/10, batch:316/508, loss:0.2924618721008301\n",
      "epoch:3/10, batch:317/508, loss:0.32527273893356323\n",
      "epoch:3/10, batch:318/508, loss:0.23982813954353333\n",
      "epoch:3/10, batch:319/508, loss:0.25693896412849426\n",
      "epoch:3/10, batch:320/508, loss:0.2345675826072693\n",
      "epoch:3/10, batch:321/508, loss:0.24826450645923615\n",
      "epoch:3/10, batch:322/508, loss:0.24149976670742035\n",
      "epoch:3/10, batch:323/508, loss:0.23951339721679688\n",
      "epoch:3/10, batch:324/508, loss:0.32361888885498047\n",
      "epoch:3/10, batch:325/508, loss:0.27895256876945496\n",
      "epoch:3/10, batch:326/508, loss:0.30252644419670105\n",
      "epoch:3/10, batch:327/508, loss:0.20619451999664307\n",
      "epoch:3/10, batch:328/508, loss:0.28062692284584045\n",
      "epoch:3/10, batch:329/508, loss:0.2559942305088043\n",
      "epoch:3/10, batch:330/508, loss:0.3455812931060791\n",
      "epoch:3/10, batch:331/508, loss:0.2885585427284241\n",
      "epoch:3/10, batch:332/508, loss:0.24448126554489136\n",
      "epoch:3/10, batch:333/508, loss:0.2300652116537094\n",
      "epoch:3/10, batch:334/508, loss:0.2725035548210144\n",
      "epoch:3/10, batch:335/508, loss:0.2546166181564331\n",
      "epoch:3/10, batch:336/508, loss:0.2125863879919052\n",
      "epoch:3/10, batch:337/508, loss:0.2598402798175812\n",
      "epoch:3/10, batch:338/508, loss:0.2614668011665344\n",
      "epoch:3/10, batch:339/508, loss:0.19194790720939636\n",
      "epoch:3/10, batch:340/508, loss:0.20318111777305603\n",
      "epoch:3/10, batch:341/508, loss:0.3091946542263031\n",
      "epoch:3/10, batch:342/508, loss:0.23473893105983734\n",
      "epoch:3/10, batch:343/508, loss:0.36198437213897705\n",
      "epoch:3/10, batch:344/508, loss:0.2291952669620514\n",
      "epoch:3/10, batch:345/508, loss:0.3546963930130005\n",
      "epoch:3/10, batch:346/508, loss:0.22223727405071259\n",
      "epoch:3/10, batch:347/508, loss:0.259870707988739\n",
      "epoch:3/10, batch:348/508, loss:0.31679806113243103\n",
      "epoch:3/10, batch:349/508, loss:0.24607406556606293\n",
      "epoch:3/10, batch:350/508, loss:0.3061943054199219\n",
      "epoch:3/10, batch:351/508, loss:0.2519288957118988\n",
      "epoch:3/10, batch:352/508, loss:0.2773299813270569\n",
      "epoch:3/10, batch:353/508, loss:0.3026635944843292\n",
      "epoch:3/10, batch:354/508, loss:0.25525808334350586\n",
      "epoch:3/10, batch:355/508, loss:0.33824965357780457\n",
      "epoch:3/10, batch:356/508, loss:0.2805761694908142\n",
      "epoch:3/10, batch:357/508, loss:0.2358381301164627\n",
      "epoch:3/10, batch:358/508, loss:0.28790634870529175\n",
      "epoch:3/10, batch:359/508, loss:0.23501478135585785\n",
      "epoch:3/10, batch:360/508, loss:0.19696980714797974\n",
      "epoch:3/10, batch:361/508, loss:0.22312170267105103\n",
      "epoch:3/10, batch:362/508, loss:0.25199049711227417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3/10, batch:363/508, loss:0.289591908454895\n",
      "epoch:3/10, batch:364/508, loss:0.3599427342414856\n",
      "epoch:3/10, batch:365/508, loss:0.1588636189699173\n",
      "epoch:3/10, batch:366/508, loss:0.2288312464952469\n",
      "epoch:3/10, batch:367/508, loss:0.2701561152935028\n",
      "epoch:3/10, batch:368/508, loss:0.20979104936122894\n",
      "epoch:3/10, batch:369/508, loss:0.2986891567707062\n",
      "epoch:3/10, batch:370/508, loss:0.1722392737865448\n",
      "epoch:3/10, batch:371/508, loss:0.25000759959220886\n",
      "epoch:3/10, batch:372/508, loss:0.2352309226989746\n",
      "epoch:3/10, batch:373/508, loss:0.3217352628707886\n",
      "epoch:3/10, batch:374/508, loss:0.2817058563232422\n",
      "epoch:3/10, batch:375/508, loss:0.23171792924404144\n",
      "epoch:3/10, batch:376/508, loss:0.332952618598938\n",
      "epoch:3/10, batch:377/508, loss:0.2376551628112793\n",
      "epoch:3/10, batch:378/508, loss:0.19681565463542938\n",
      "epoch:3/10, batch:379/508, loss:0.2531086802482605\n",
      "epoch:3/10, batch:380/508, loss:0.2672154903411865\n",
      "epoch:3/10, batch:381/508, loss:0.18012070655822754\n",
      "evaluating ....\n",
      "Dev Performance P@5 0.390062111801\n",
      "Dev Performance P@1 0.496894409938\n",
      "Dev Performance MAP 0.6196249776115481\n",
      "Dev Performance MRR 0.6439644921513756\n",
      "------------------\n",
      "Test Performance P@5 0.362264150943\n",
      "Test Performance P@1 0.547169811321\n",
      "Test Performance MAP 0.6329417133539961\n",
      "Test Performance MRR 0.6859702721420694\n",
      "epoch:3/10, batch:382/508, loss:0.3070608675479889\n",
      "epoch:3/10, batch:383/508, loss:0.21849596500396729\n",
      "epoch:3/10, batch:384/508, loss:0.1946505606174469\n",
      "epoch:3/10, batch:385/508, loss:0.2586842179298401\n",
      "epoch:3/10, batch:386/508, loss:0.20627844333648682\n",
      "epoch:3/10, batch:387/508, loss:0.20835000276565552\n",
      "epoch:3/10, batch:388/508, loss:0.24140645563602448\n",
      "epoch:3/10, batch:389/508, loss:0.2806166708469391\n",
      "epoch:3/10, batch:390/508, loss:0.22831560671329498\n",
      "epoch:3/10, batch:391/508, loss:0.2190047651529312\n",
      "epoch:3/10, batch:392/508, loss:0.25438642501831055\n",
      "epoch:3/10, batch:393/508, loss:0.26253852248191833\n",
      "epoch:3/10, batch:394/508, loss:0.25838702917099\n",
      "epoch:3/10, batch:395/508, loss:0.3308868408203125\n",
      "epoch:3/10, batch:396/508, loss:0.3209855854511261\n",
      "epoch:3/10, batch:397/508, loss:0.2528753876686096\n",
      "epoch:3/10, batch:398/508, loss:0.2426215410232544\n",
      "epoch:3/10, batch:399/508, loss:0.2714882493019104\n",
      "epoch:3/10, batch:400/508, loss:0.2627015709877014\n",
      "epoch:3/10, batch:401/508, loss:0.2851630747318268\n",
      "epoch:3/10, batch:402/508, loss:0.20948243141174316\n",
      "epoch:3/10, batch:403/508, loss:0.2693268656730652\n",
      "epoch:3/10, batch:404/508, loss:0.3710292875766754\n",
      "epoch:3/10, batch:405/508, loss:0.20755594968795776\n",
      "epoch:3/10, batch:406/508, loss:0.24552308022975922\n",
      "epoch:3/10, batch:407/508, loss:0.16931766271591187\n",
      "epoch:3/10, batch:408/508, loss:0.3071396052837372\n",
      "epoch:3/10, batch:409/508, loss:0.21787984669208527\n",
      "epoch:3/10, batch:410/508, loss:0.27196526527404785\n",
      "epoch:3/10, batch:411/508, loss:0.22814123332500458\n",
      "epoch:3/10, batch:412/508, loss:0.25337034463882446\n",
      "epoch:3/10, batch:413/508, loss:0.28108730912208557\n",
      "epoch:3/10, batch:414/508, loss:0.2929133474826813\n",
      "epoch:3/10, batch:415/508, loss:0.2483789622783661\n",
      "epoch:3/10, batch:416/508, loss:0.2832651436328888\n",
      "epoch:3/10, batch:417/508, loss:0.2726871371269226\n",
      "epoch:3/10, batch:418/508, loss:0.22909489274024963\n",
      "epoch:3/10, batch:419/508, loss:0.2498573213815689\n",
      "epoch:3/10, batch:420/508, loss:0.20966099202632904\n",
      "epoch:3/10, batch:421/508, loss:0.23068401217460632\n",
      "epoch:3/10, batch:422/508, loss:0.26080912351608276\n",
      "epoch:3/10, batch:423/508, loss:0.31086307764053345\n",
      "epoch:3/10, batch:424/508, loss:0.2824978530406952\n",
      "epoch:3/10, batch:425/508, loss:0.21800357103347778\n",
      "epoch:3/10, batch:426/508, loss:0.2565805912017822\n",
      "epoch:3/10, batch:427/508, loss:0.2711590528488159\n",
      "epoch:3/10, batch:428/508, loss:0.20939132571220398\n",
      "epoch:3/10, batch:429/508, loss:0.2252233624458313\n",
      "epoch:3/10, batch:430/508, loss:0.21966247260570526\n",
      "epoch:3/10, batch:431/508, loss:0.2357647866010666\n",
      "epoch:3/10, batch:432/508, loss:0.21306930482387543\n",
      "epoch:3/10, batch:433/508, loss:0.29839688539505005\n",
      "epoch:3/10, batch:434/508, loss:0.3141917288303375\n",
      "epoch:3/10, batch:435/508, loss:0.20025406777858734\n",
      "epoch:3/10, batch:436/508, loss:0.3204871714115143\n",
      "epoch:3/10, batch:437/508, loss:0.370351105928421\n",
      "epoch:3/10, batch:438/508, loss:0.2720215916633606\n",
      "epoch:3/10, batch:439/508, loss:0.21692541241645813\n",
      "epoch:3/10, batch:440/508, loss:0.2763799726963043\n",
      "epoch:3/10, batch:441/508, loss:0.26603689789772034\n",
      "epoch:3/10, batch:442/508, loss:0.29568901658058167\n",
      "epoch:3/10, batch:443/508, loss:0.24870961904525757\n",
      "epoch:3/10, batch:444/508, loss:0.308460533618927\n",
      "epoch:3/10, batch:445/508, loss:0.28873950242996216\n",
      "epoch:3/10, batch:446/508, loss:0.20389488339424133\n",
      "epoch:3/10, batch:447/508, loss:0.2778490483760834\n",
      "epoch:3/10, batch:448/508, loss:0.21910910308361053\n",
      "epoch:3/10, batch:449/508, loss:0.24212585389614105\n",
      "epoch:3/10, batch:450/508, loss:0.22238658368587494\n",
      "epoch:3/10, batch:451/508, loss:0.2679811120033264\n",
      "epoch:3/10, batch:452/508, loss:0.20975984632968903\n",
      "epoch:3/10, batch:453/508, loss:0.3253155052661896\n",
      "epoch:3/10, batch:454/508, loss:0.28889355063438416\n",
      "epoch:3/10, batch:455/508, loss:0.26069310307502747\n",
      "epoch:3/10, batch:456/508, loss:0.23939335346221924\n",
      "epoch:3/10, batch:457/508, loss:0.2530681788921356\n",
      "epoch:3/10, batch:458/508, loss:0.26773086190223694\n",
      "epoch:3/10, batch:459/508, loss:0.2817727029323578\n",
      "epoch:3/10, batch:460/508, loss:0.32669907808303833\n",
      "epoch:3/10, batch:461/508, loss:0.18847377598285675\n",
      "epoch:3/10, batch:462/508, loss:0.3068661093711853\n",
      "epoch:3/10, batch:463/508, loss:0.27071723341941833\n",
      "epoch:3/10, batch:464/508, loss:0.23787763714790344\n",
      "epoch:3/10, batch:465/508, loss:0.2888384461402893\n",
      "epoch:3/10, batch:466/508, loss:0.25349098443984985\n",
      "epoch:3/10, batch:467/508, loss:0.28256842494010925\n",
      "epoch:3/10, batch:468/508, loss:0.27493005990982056\n",
      "epoch:3/10, batch:469/508, loss:0.2596285343170166\n",
      "epoch:3/10, batch:470/508, loss:0.2991009056568146\n",
      "epoch:3/10, batch:471/508, loss:0.26378849148750305\n",
      "epoch:3/10, batch:472/508, loss:0.3156037926673889\n",
      "epoch:3/10, batch:473/508, loss:0.3036903738975525\n",
      "epoch:3/10, batch:474/508, loss:0.16457991302013397\n",
      "epoch:3/10, batch:475/508, loss:0.1988382339477539\n",
      "epoch:3/10, batch:476/508, loss:0.22346197068691254\n",
      "epoch:3/10, batch:477/508, loss:0.2826419472694397\n",
      "epoch:3/10, batch:478/508, loss:0.24846279621124268\n",
      "epoch:3/10, batch:479/508, loss:0.262838751077652\n",
      "epoch:3/10, batch:480/508, loss:0.2072855830192566\n",
      "epoch:3/10, batch:481/508, loss:0.19839318096637726\n",
      "epoch:3/10, batch:482/508, loss:0.2911665439605713\n",
      "epoch:3/10, batch:483/508, loss:0.28023219108581543\n",
      "epoch:3/10, batch:484/508, loss:0.18485136330127716\n",
      "epoch:3/10, batch:485/508, loss:0.17518025636672974\n",
      "epoch:3/10, batch:486/508, loss:0.23313762247562408\n",
      "epoch:3/10, batch:487/508, loss:0.2482936680316925\n",
      "epoch:3/10, batch:488/508, loss:0.3495088219642639\n",
      "epoch:3/10, batch:489/508, loss:0.287679523229599\n",
      "epoch:3/10, batch:490/508, loss:0.2827189266681671\n",
      "epoch:3/10, batch:491/508, loss:0.23993593454360962\n",
      "epoch:3/10, batch:492/508, loss:0.2347477823495865\n",
      "epoch:3/10, batch:493/508, loss:0.26567453145980835\n",
      "epoch:3/10, batch:494/508, loss:0.2124539464712143\n",
      "epoch:3/10, batch:495/508, loss:0.2387417107820511\n",
      "epoch:3/10, batch:496/508, loss:0.282975971698761\n",
      "epoch:3/10, batch:497/508, loss:0.24516160786151886\n",
      "epoch:3/10, batch:498/508, loss:0.20195795595645905\n",
      "epoch:3/10, batch:499/508, loss:0.2521320879459381\n",
      "epoch:3/10, batch:500/508, loss:0.25678279995918274\n",
      "epoch:3/10, batch:501/508, loss:0.19751295447349548\n",
      "epoch:3/10, batch:502/508, loss:0.28982678055763245\n",
      "epoch:3/10, batch:503/508, loss:0.25445395708084106\n",
      "epoch:3/10, batch:504/508, loss:0.23647277057170868\n",
      "epoch:3/10, batch:505/508, loss:0.3316948413848877\n",
      "epoch:3/10, batch:506/508, loss:0.2625270485877991\n",
      "epoch:3/10, batch:507/508, loss:0.28147104382514954\n",
      "epoch:3/10, batch:508/508, loss:0.23744836449623108\n",
      "evaluating ....\n",
      "Dev Performance P@5 0.395031055901\n",
      "Dev Performance P@1 0.509316770186\n",
      "Dev Performance MAP 0.6221813511168734\n",
      "Dev Performance MRR 0.6570060597462861\n",
      "------------------\n",
      "Test Performance P@5 0.367295597484\n",
      "Test Performance P@1 0.534591194969\n",
      "Test Performance MAP 0.6286606284830862\n",
      "Test Performance MRR 0.6778374371224422\n",
      "epoch:4/10, batch:1/508, loss:0.23737889528274536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4/10, batch:2/508, loss:0.24041998386383057\n",
      "epoch:4/10, batch:3/508, loss:0.3200435936450958\n",
      "epoch:4/10, batch:4/508, loss:0.2114938497543335\n",
      "epoch:4/10, batch:5/508, loss:0.30499574542045593\n",
      "epoch:4/10, batch:6/508, loss:0.31289756298065186\n",
      "epoch:4/10, batch:7/508, loss:0.24708640575408936\n",
      "epoch:4/10, batch:8/508, loss:0.1642632633447647\n",
      "epoch:4/10, batch:9/508, loss:0.297350138425827\n",
      "epoch:4/10, batch:10/508, loss:0.1795019954442978\n",
      "epoch:4/10, batch:11/508, loss:0.21369075775146484\n",
      "epoch:4/10, batch:12/508, loss:0.2803500294685364\n",
      "epoch:4/10, batch:13/508, loss:0.1691911369562149\n",
      "epoch:4/10, batch:14/508, loss:0.22765208780765533\n",
      "epoch:4/10, batch:15/508, loss:0.23459947109222412\n",
      "epoch:4/10, batch:16/508, loss:0.19273756444454193\n",
      "epoch:4/10, batch:17/508, loss:0.2851704955101013\n",
      "epoch:4/10, batch:18/508, loss:0.29575687646865845\n",
      "epoch:4/10, batch:19/508, loss:0.17685487866401672\n",
      "epoch:4/10, batch:20/508, loss:0.17783381044864655\n",
      "epoch:4/10, batch:21/508, loss:0.23925445973873138\n",
      "epoch:4/10, batch:22/508, loss:0.18014918267726898\n",
      "epoch:4/10, batch:23/508, loss:0.20418991148471832\n",
      "epoch:4/10, batch:24/508, loss:0.25257712602615356\n",
      "epoch:4/10, batch:25/508, loss:0.22048795223236084\n",
      "epoch:4/10, batch:26/508, loss:0.2797831594944\n",
      "epoch:4/10, batch:27/508, loss:0.2543831765651703\n",
      "epoch:4/10, batch:28/508, loss:0.2200668752193451\n",
      "epoch:4/10, batch:29/508, loss:0.2544589638710022\n",
      "epoch:4/10, batch:30/508, loss:0.25499463081359863\n",
      "epoch:4/10, batch:31/508, loss:0.2509121596813202\n",
      "epoch:4/10, batch:32/508, loss:0.28067201375961304\n",
      "epoch:4/10, batch:33/508, loss:0.25510454177856445\n",
      "epoch:4/10, batch:34/508, loss:0.2066819816827774\n",
      "epoch:4/10, batch:35/508, loss:0.2036636471748352\n",
      "epoch:4/10, batch:36/508, loss:0.2684643566608429\n",
      "epoch:4/10, batch:37/508, loss:0.19366908073425293\n",
      "epoch:4/10, batch:38/508, loss:0.23076534271240234\n",
      "epoch:4/10, batch:39/508, loss:0.1903543770313263\n",
      "epoch:4/10, batch:40/508, loss:0.24609971046447754\n",
      "epoch:4/10, batch:41/508, loss:0.19368073344230652\n",
      "epoch:4/10, batch:42/508, loss:0.22767770290374756\n",
      "epoch:4/10, batch:43/508, loss:0.25967270135879517\n",
      "epoch:4/10, batch:44/508, loss:0.2258239984512329\n",
      "epoch:4/10, batch:45/508, loss:0.23063765466213226\n",
      "epoch:4/10, batch:46/508, loss:0.20589227974414825\n",
      "epoch:4/10, batch:47/508, loss:0.2740364670753479\n",
      "epoch:4/10, batch:48/508, loss:0.2731172740459442\n",
      "epoch:4/10, batch:49/508, loss:0.2471999078989029\n",
      "epoch:4/10, batch:50/508, loss:0.23492197692394257\n",
      "epoch:4/10, batch:51/508, loss:0.21387964487075806\n",
      "epoch:4/10, batch:52/508, loss:0.20901067554950714\n",
      "epoch:4/10, batch:53/508, loss:0.2498815804719925\n",
      "epoch:4/10, batch:54/508, loss:0.21183820068836212\n",
      "epoch:4/10, batch:55/508, loss:0.2325802594423294\n",
      "epoch:4/10, batch:56/508, loss:0.24033735692501068\n",
      "epoch:4/10, batch:57/508, loss:0.26225656270980835\n",
      "epoch:4/10, batch:58/508, loss:0.20755857229232788\n",
      "epoch:4/10, batch:59/508, loss:0.28618910908699036\n",
      "epoch:4/10, batch:60/508, loss:0.2937219738960266\n",
      "epoch:4/10, batch:61/508, loss:0.2766597867012024\n",
      "epoch:4/10, batch:62/508, loss:0.1854759305715561\n",
      "epoch:4/10, batch:63/508, loss:0.2894197404384613\n",
      "epoch:4/10, batch:64/508, loss:0.32957470417022705\n",
      "epoch:4/10, batch:65/508, loss:0.26712343096733093\n",
      "epoch:4/10, batch:66/508, loss:0.256326287984848\n",
      "epoch:4/10, batch:67/508, loss:0.19252194464206696\n",
      "epoch:4/10, batch:68/508, loss:0.20704279839992523\n",
      "epoch:4/10, batch:69/508, loss:0.18205010890960693\n",
      "epoch:4/10, batch:70/508, loss:0.24450014531612396\n",
      "epoch:4/10, batch:71/508, loss:0.2413601130247116\n",
      "epoch:4/10, batch:72/508, loss:0.23538441956043243\n",
      "epoch:4/10, batch:73/508, loss:0.16653208434581757\n",
      "epoch:4/10, batch:74/508, loss:0.28515827655792236\n",
      "epoch:4/10, batch:75/508, loss:0.29995763301849365\n",
      "epoch:4/10, batch:76/508, loss:0.30901527404785156\n",
      "epoch:4/10, batch:77/508, loss:0.27479809522628784\n",
      "epoch:4/10, batch:78/508, loss:0.22836121916770935\n",
      "epoch:4/10, batch:79/508, loss:0.2679210603237152\n",
      "epoch:4/10, batch:80/508, loss:0.31304478645324707\n",
      "epoch:4/10, batch:81/508, loss:0.16786421835422516\n",
      "epoch:4/10, batch:82/508, loss:0.22639083862304688\n",
      "epoch:4/10, batch:83/508, loss:0.3047710955142975\n",
      "epoch:4/10, batch:84/508, loss:0.22334729135036469\n",
      "epoch:4/10, batch:85/508, loss:0.16638700664043427\n",
      "epoch:4/10, batch:86/508, loss:0.28672662377357483\n",
      "epoch:4/10, batch:87/508, loss:0.23874805867671967\n",
      "epoch:4/10, batch:88/508, loss:0.23357287049293518\n",
      "epoch:4/10, batch:89/508, loss:0.25851061940193176\n",
      "epoch:4/10, batch:90/508, loss:0.20355509221553802\n",
      "epoch:4/10, batch:91/508, loss:0.1932215392589569\n",
      "epoch:4/10, batch:92/508, loss:0.30622631311416626\n",
      "epoch:4/10, batch:93/508, loss:0.22839125990867615\n",
      "epoch:4/10, batch:94/508, loss:0.21214784681797028\n",
      "epoch:4/10, batch:95/508, loss:0.1849314421415329\n",
      "epoch:4/10, batch:96/508, loss:0.2621177136898041\n",
      "epoch:4/10, batch:97/508, loss:0.2581881880760193\n",
      "epoch:4/10, batch:98/508, loss:0.2185966670513153\n",
      "epoch:4/10, batch:99/508, loss:0.1678469032049179\n",
      "epoch:4/10, batch:100/508, loss:0.26443108916282654\n",
      "epoch:4/10, batch:101/508, loss:0.2668467164039612\n",
      "epoch:4/10, batch:102/508, loss:0.1907208114862442\n",
      "epoch:4/10, batch:103/508, loss:0.2783207893371582\n",
      "epoch:4/10, batch:104/508, loss:0.3876500129699707\n",
      "epoch:4/10, batch:105/508, loss:0.2720107436180115\n",
      "epoch:4/10, batch:106/508, loss:0.16871251165866852\n",
      "epoch:4/10, batch:107/508, loss:0.27323609590530396\n",
      "epoch:4/10, batch:108/508, loss:0.19653543829917908\n",
      "epoch:4/10, batch:109/508, loss:0.2416914999485016\n",
      "epoch:4/10, batch:110/508, loss:0.2458471655845642\n",
      "epoch:4/10, batch:111/508, loss:0.21143926680088043\n",
      "epoch:4/10, batch:112/508, loss:0.250601202249527\n",
      "epoch:4/10, batch:113/508, loss:0.20617562532424927\n",
      "epoch:4/10, batch:114/508, loss:0.2983385920524597\n",
      "epoch:4/10, batch:115/508, loss:0.25406408309936523\n",
      "epoch:4/10, batch:116/508, loss:0.22841335833072662\n",
      "epoch:4/10, batch:117/508, loss:0.24735258519649506\n",
      "epoch:4/10, batch:118/508, loss:0.28756651282310486\n",
      "epoch:4/10, batch:119/508, loss:0.18804340064525604\n",
      "epoch:4/10, batch:120/508, loss:0.33447858691215515\n",
      "epoch:4/10, batch:121/508, loss:0.2302752584218979\n",
      "epoch:4/10, batch:122/508, loss:0.26231148838996887\n",
      "epoch:4/10, batch:123/508, loss:0.26269829273223877\n",
      "epoch:4/10, batch:124/508, loss:0.20310333371162415\n",
      "epoch:4/10, batch:125/508, loss:0.19883307814598083\n",
      "epoch:4/10, batch:126/508, loss:0.23810599744319916\n",
      "epoch:4/10, batch:127/508, loss:0.2320244014263153\n",
      "evaluating ....\n",
      "Dev Performance P@5 0.396273291925\n",
      "Dev Performance P@1 0.496894409938\n",
      "Dev Performance MAP 0.6150261366248441\n",
      "Dev Performance MRR 0.6471665376396833\n",
      "------------------\n",
      "Test Performance P@5 0.381132075472\n",
      "Test Performance P@1 0.522012578616\n",
      "Test Performance MAP 0.634589562595109\n",
      "Test Performance MRR 0.6741300912850068\n",
      "epoch:4/10, batch:128/508, loss:0.24459049105644226\n",
      "epoch:4/10, batch:129/508, loss:0.21202488243579865\n",
      "epoch:4/10, batch:130/508, loss:0.26609253883361816\n",
      "epoch:4/10, batch:131/508, loss:0.3243093192577362\n",
      "epoch:4/10, batch:132/508, loss:0.19136062264442444\n",
      "epoch:4/10, batch:133/508, loss:0.19416871666908264\n",
      "epoch:4/10, batch:134/508, loss:0.33035919070243835\n",
      "epoch:4/10, batch:135/508, loss:0.21473512053489685\n",
      "epoch:4/10, batch:136/508, loss:0.19474291801452637\n",
      "epoch:4/10, batch:137/508, loss:0.24706342816352844\n",
      "epoch:4/10, batch:138/508, loss:0.22533249855041504\n",
      "epoch:4/10, batch:139/508, loss:0.22529907524585724\n",
      "epoch:4/10, batch:140/508, loss:0.3050077557563782\n",
      "epoch:4/10, batch:141/508, loss:0.3001241683959961\n",
      "epoch:4/10, batch:142/508, loss:0.24591150879859924\n",
      "epoch:4/10, batch:143/508, loss:0.2468218356370926\n",
      "epoch:4/10, batch:144/508, loss:0.24470990896224976\n",
      "epoch:4/10, batch:145/508, loss:0.20062735676765442\n",
      "epoch:4/10, batch:146/508, loss:0.2644418478012085\n",
      "epoch:4/10, batch:147/508, loss:0.23909708857536316\n",
      "epoch:4/10, batch:148/508, loss:0.2319154292345047\n",
      "epoch:4/10, batch:149/508, loss:0.20282381772994995\n",
      "epoch:4/10, batch:150/508, loss:0.23552337288856506\n",
      "epoch:4/10, batch:151/508, loss:0.36145928502082825\n",
      "epoch:4/10, batch:152/508, loss:0.20320414006710052\n",
      "epoch:4/10, batch:153/508, loss:0.28180649876594543\n",
      "epoch:4/10, batch:154/508, loss:0.2496458739042282\n",
      "epoch:4/10, batch:155/508, loss:0.2626042068004608\n",
      "epoch:4/10, batch:156/508, loss:0.22094598412513733\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4/10, batch:157/508, loss:0.23594997823238373\n",
      "epoch:4/10, batch:158/508, loss:0.19763651490211487\n",
      "epoch:4/10, batch:159/508, loss:0.25291723012924194\n",
      "epoch:4/10, batch:160/508, loss:0.2525399625301361\n",
      "epoch:4/10, batch:161/508, loss:0.24783939123153687\n",
      "epoch:4/10, batch:162/508, loss:0.278709352016449\n",
      "epoch:4/10, batch:163/508, loss:0.3223874270915985\n",
      "epoch:4/10, batch:164/508, loss:0.28209877014160156\n",
      "epoch:4/10, batch:165/508, loss:0.2693496346473694\n",
      "epoch:4/10, batch:166/508, loss:0.3017217218875885\n",
      "epoch:4/10, batch:167/508, loss:0.23064807057380676\n",
      "epoch:4/10, batch:168/508, loss:0.2436118721961975\n",
      "epoch:4/10, batch:169/508, loss:0.17443174123764038\n",
      "epoch:4/10, batch:170/508, loss:0.21813975274562836\n",
      "epoch:4/10, batch:171/508, loss:0.30737099051475525\n",
      "epoch:4/10, batch:172/508, loss:0.23890022933483124\n",
      "epoch:4/10, batch:173/508, loss:0.23747028410434723\n",
      "epoch:4/10, batch:174/508, loss:0.20366856455802917\n",
      "epoch:4/10, batch:175/508, loss:0.26482436060905457\n",
      "epoch:4/10, batch:176/508, loss:0.23683154582977295\n",
      "epoch:4/10, batch:177/508, loss:0.2084694504737854\n",
      "epoch:4/10, batch:178/508, loss:0.23904931545257568\n",
      "epoch:4/10, batch:179/508, loss:0.2335936278104782\n",
      "epoch:4/10, batch:180/508, loss:0.22894853353500366\n",
      "epoch:4/10, batch:181/508, loss:0.23188139498233795\n",
      "epoch:4/10, batch:182/508, loss:0.19024315476417542\n",
      "epoch:4/10, batch:183/508, loss:0.32520928978919983\n",
      "epoch:4/10, batch:184/508, loss:0.24631059169769287\n",
      "epoch:4/10, batch:185/508, loss:0.2880527079105377\n",
      "epoch:4/10, batch:186/508, loss:0.1907457858324051\n",
      "epoch:4/10, batch:187/508, loss:0.3425396978855133\n",
      "epoch:4/10, batch:188/508, loss:0.22292830049991608\n",
      "epoch:4/10, batch:189/508, loss:0.2690752148628235\n",
      "epoch:4/10, batch:190/508, loss:0.27584147453308105\n",
      "epoch:4/10, batch:191/508, loss:0.24473737180233002\n",
      "epoch:4/10, batch:192/508, loss:0.3028273284435272\n",
      "epoch:4/10, batch:193/508, loss:0.20380175113677979\n",
      "epoch:4/10, batch:194/508, loss:0.2291901558637619\n",
      "epoch:4/10, batch:195/508, loss:0.2737880051136017\n",
      "epoch:4/10, batch:196/508, loss:0.20366084575653076\n",
      "epoch:4/10, batch:197/508, loss:0.3123342990875244\n",
      "epoch:4/10, batch:198/508, loss:0.2548275887966156\n",
      "epoch:4/10, batch:199/508, loss:0.19252794981002808\n",
      "epoch:4/10, batch:200/508, loss:0.26654455065727234\n",
      "epoch:4/10, batch:201/508, loss:0.3172871768474579\n",
      "epoch:4/10, batch:202/508, loss:0.26863932609558105\n",
      "epoch:4/10, batch:203/508, loss:0.20187348127365112\n",
      "epoch:4/10, batch:204/508, loss:0.24972006678581238\n",
      "epoch:4/10, batch:205/508, loss:0.2139967828989029\n",
      "epoch:4/10, batch:206/508, loss:0.2756240963935852\n",
      "epoch:4/10, batch:207/508, loss:0.26138731837272644\n",
      "epoch:4/10, batch:208/508, loss:0.2124301940202713\n",
      "epoch:4/10, batch:209/508, loss:0.1734498143196106\n",
      "epoch:4/10, batch:210/508, loss:0.21736957132816315\n",
      "epoch:4/10, batch:211/508, loss:0.2421952337026596\n",
      "epoch:4/10, batch:212/508, loss:0.32945212721824646\n",
      "epoch:4/10, batch:213/508, loss:0.20839369297027588\n",
      "epoch:4/10, batch:214/508, loss:0.3063557744026184\n",
      "epoch:4/10, batch:215/508, loss:0.20780473947525024\n",
      "epoch:4/10, batch:216/508, loss:0.20232288539409637\n",
      "epoch:4/10, batch:217/508, loss:0.2763662040233612\n",
      "epoch:4/10, batch:218/508, loss:0.24323619902133942\n",
      "epoch:4/10, batch:219/508, loss:0.19786643981933594\n",
      "epoch:4/10, batch:220/508, loss:0.1905958354473114\n",
      "epoch:4/10, batch:221/508, loss:0.20126603543758392\n",
      "epoch:4/10, batch:222/508, loss:0.24342860281467438\n",
      "epoch:4/10, batch:223/508, loss:0.30834725499153137\n",
      "epoch:4/10, batch:224/508, loss:0.26821526885032654\n",
      "epoch:4/10, batch:225/508, loss:0.24535192549228668\n",
      "epoch:4/10, batch:226/508, loss:0.18897326290607452\n",
      "epoch:4/10, batch:227/508, loss:0.19443538784980774\n",
      "epoch:4/10, batch:228/508, loss:0.29386892914772034\n",
      "epoch:4/10, batch:229/508, loss:0.30311715602874756\n",
      "epoch:4/10, batch:230/508, loss:0.20813260972499847\n",
      "epoch:4/10, batch:231/508, loss:0.21628935635089874\n",
      "epoch:4/10, batch:232/508, loss:0.29389339685440063\n",
      "epoch:4/10, batch:233/508, loss:0.24623367190361023\n",
      "epoch:4/10, batch:234/508, loss:0.15450648963451385\n",
      "epoch:4/10, batch:235/508, loss:0.28916576504707336\n",
      "epoch:4/10, batch:236/508, loss:0.20861324667930603\n",
      "epoch:4/10, batch:237/508, loss:0.26877686381340027\n",
      "epoch:4/10, batch:238/508, loss:0.27535754442214966\n",
      "epoch:4/10, batch:239/508, loss:0.24002774059772491\n",
      "epoch:4/10, batch:240/508, loss:0.31865930557250977\n",
      "epoch:4/10, batch:241/508, loss:0.22553393244743347\n",
      "epoch:4/10, batch:242/508, loss:0.1985723078250885\n",
      "epoch:4/10, batch:243/508, loss:0.20144234597682953\n",
      "epoch:4/10, batch:244/508, loss:0.2686544954776764\n",
      "epoch:4/10, batch:245/508, loss:0.11375165730714798\n",
      "epoch:4/10, batch:246/508, loss:0.24806413054466248\n",
      "epoch:4/10, batch:247/508, loss:0.28205692768096924\n",
      "epoch:4/10, batch:248/508, loss:0.2558784484863281\n",
      "epoch:4/10, batch:249/508, loss:0.19356782734394073\n",
      "epoch:4/10, batch:250/508, loss:0.2808322012424469\n",
      "epoch:4/10, batch:251/508, loss:0.22075217962265015\n",
      "epoch:4/10, batch:252/508, loss:0.16810210049152374\n",
      "epoch:4/10, batch:253/508, loss:0.2891775369644165\n",
      "epoch:4/10, batch:254/508, loss:0.23584438860416412\n",
      "evaluating ....\n",
      "Dev Performance P@5 0.401242236025\n",
      "Dev Performance P@1 0.527950310559\n",
      "Dev Performance MAP 0.6307236269037968\n",
      "Dev Performance MRR 0.6680927276150148\n",
      "------------------\n",
      "Test Performance P@5 0.378616352201\n",
      "Test Performance P@1 0.559748427673\n",
      "Test Performance MAP 0.6443605689775694\n",
      "Test Performance MRR 0.6966294941220462\n",
      "epoch:4/10, batch:255/508, loss:0.19423533976078033\n",
      "epoch:4/10, batch:256/508, loss:0.22752775251865387\n",
      "epoch:4/10, batch:257/508, loss:0.2615579664707184\n",
      "epoch:4/10, batch:258/508, loss:0.23733197152614594\n",
      "epoch:4/10, batch:259/508, loss:0.2835087776184082\n",
      "epoch:4/10, batch:260/508, loss:0.27912697196006775\n",
      "epoch:4/10, batch:261/508, loss:0.2289184033870697\n",
      "epoch:4/10, batch:262/508, loss:0.16594524681568146\n",
      "epoch:4/10, batch:263/508, loss:0.2222878783941269\n",
      "epoch:4/10, batch:264/508, loss:0.2604089677333832\n",
      "epoch:4/10, batch:265/508, loss:0.23173488676548004\n",
      "epoch:4/10, batch:266/508, loss:0.2672382891178131\n",
      "epoch:4/10, batch:267/508, loss:0.2572802007198334\n",
      "epoch:4/10, batch:268/508, loss:0.2783007025718689\n",
      "epoch:4/10, batch:269/508, loss:0.22735846042633057\n",
      "epoch:4/10, batch:270/508, loss:0.2740263044834137\n",
      "epoch:4/10, batch:271/508, loss:0.23571501672267914\n",
      "epoch:4/10, batch:272/508, loss:0.26130837202072144\n",
      "epoch:4/10, batch:273/508, loss:0.21425099670886993\n",
      "epoch:4/10, batch:274/508, loss:0.20870719850063324\n",
      "epoch:4/10, batch:275/508, loss:0.2058876007795334\n",
      "epoch:4/10, batch:276/508, loss:0.24197113513946533\n",
      "epoch:4/10, batch:277/508, loss:0.2676275372505188\n",
      "epoch:4/10, batch:278/508, loss:0.2578149735927582\n",
      "epoch:4/10, batch:279/508, loss:0.2486874759197235\n",
      "epoch:4/10, batch:280/508, loss:0.264068067073822\n",
      "epoch:4/10, batch:281/508, loss:0.19342707097530365\n",
      "epoch:4/10, batch:282/508, loss:0.21073219180107117\n",
      "epoch:4/10, batch:283/508, loss:0.2443293333053589\n",
      "epoch:4/10, batch:284/508, loss:0.2726159691810608\n",
      "epoch:4/10, batch:285/508, loss:0.25572481751441956\n",
      "epoch:4/10, batch:286/508, loss:0.2623879909515381\n",
      "epoch:4/10, batch:287/508, loss:0.21855655312538147\n",
      "epoch:4/10, batch:288/508, loss:0.23172861337661743\n",
      "epoch:4/10, batch:289/508, loss:0.2714071273803711\n",
      "epoch:4/10, batch:290/508, loss:0.26616716384887695\n",
      "epoch:4/10, batch:291/508, loss:0.21911822259426117\n",
      "epoch:4/10, batch:292/508, loss:0.28670039772987366\n",
      "epoch:4/10, batch:293/508, loss:0.2611764669418335\n",
      "epoch:4/10, batch:294/508, loss:0.2680175006389618\n",
      "epoch:4/10, batch:295/508, loss:0.26972076296806335\n",
      "epoch:4/10, batch:296/508, loss:0.23308885097503662\n",
      "epoch:4/10, batch:297/508, loss:0.2875031530857086\n",
      "epoch:4/10, batch:298/508, loss:0.24525204300880432\n",
      "epoch:4/10, batch:299/508, loss:0.22657586634159088\n",
      "epoch:4/10, batch:300/508, loss:0.19041450321674347\n",
      "epoch:4/10, batch:301/508, loss:0.3177822530269623\n",
      "epoch:4/10, batch:302/508, loss:0.24802373349666595\n",
      "epoch:4/10, batch:303/508, loss:0.2427617609500885\n",
      "epoch:4/10, batch:304/508, loss:0.22641314566135406\n",
      "epoch:4/10, batch:305/508, loss:0.3010368049144745\n",
      "epoch:4/10, batch:306/508, loss:0.22841370105743408\n",
      "epoch:4/10, batch:307/508, loss:0.24772946536540985\n",
      "epoch:4/10, batch:308/508, loss:0.2484019249677658\n",
      "epoch:4/10, batch:309/508, loss:0.2534427344799042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4/10, batch:310/508, loss:0.21495231986045837\n",
      "epoch:4/10, batch:311/508, loss:0.20262759923934937\n",
      "epoch:4/10, batch:312/508, loss:0.23852457106113434\n",
      "epoch:4/10, batch:313/508, loss:0.19837042689323425\n",
      "epoch:4/10, batch:314/508, loss:0.19560427963733673\n",
      "epoch:4/10, batch:315/508, loss:0.21853548288345337\n",
      "epoch:4/10, batch:316/508, loss:0.28820592164993286\n",
      "epoch:4/10, batch:317/508, loss:0.32830867171287537\n",
      "epoch:4/10, batch:318/508, loss:0.22656570374965668\n",
      "epoch:4/10, batch:319/508, loss:0.25254860520362854\n",
      "epoch:4/10, batch:320/508, loss:0.23788179457187653\n",
      "epoch:4/10, batch:321/508, loss:0.24188975989818573\n",
      "epoch:4/10, batch:322/508, loss:0.2470225840806961\n",
      "epoch:4/10, batch:323/508, loss:0.2317601889371872\n",
      "epoch:4/10, batch:324/508, loss:0.23982089757919312\n",
      "epoch:4/10, batch:325/508, loss:0.27079546451568604\n",
      "epoch:4/10, batch:326/508, loss:0.2692287266254425\n",
      "epoch:4/10, batch:327/508, loss:0.20037007331848145\n",
      "epoch:4/10, batch:328/508, loss:0.23771953582763672\n",
      "epoch:4/10, batch:329/508, loss:0.20513424277305603\n",
      "epoch:4/10, batch:330/508, loss:0.33784112334251404\n",
      "epoch:4/10, batch:331/508, loss:0.2987557053565979\n",
      "epoch:4/10, batch:332/508, loss:0.2248421311378479\n",
      "epoch:4/10, batch:333/508, loss:0.20076151192188263\n",
      "epoch:4/10, batch:334/508, loss:0.2844395339488983\n",
      "epoch:4/10, batch:335/508, loss:0.22762718796730042\n",
      "epoch:4/10, batch:336/508, loss:0.21651388704776764\n",
      "epoch:4/10, batch:337/508, loss:0.21345050632953644\n",
      "epoch:4/10, batch:338/508, loss:0.25950396060943604\n",
      "epoch:4/10, batch:339/508, loss:0.18552352488040924\n",
      "epoch:4/10, batch:340/508, loss:0.22937312722206116\n",
      "epoch:4/10, batch:341/508, loss:0.2803608179092407\n",
      "epoch:4/10, batch:342/508, loss:0.23913730680942535\n",
      "epoch:4/10, batch:343/508, loss:0.3338983952999115\n",
      "epoch:4/10, batch:344/508, loss:0.20275172591209412\n",
      "epoch:4/10, batch:345/508, loss:0.3186524212360382\n",
      "epoch:4/10, batch:346/508, loss:0.18494686484336853\n",
      "epoch:4/10, batch:347/508, loss:0.2813459634780884\n",
      "epoch:4/10, batch:348/508, loss:0.2721448540687561\n",
      "epoch:4/10, batch:349/508, loss:0.2510572075843811\n",
      "epoch:4/10, batch:350/508, loss:0.2943311035633087\n",
      "epoch:4/10, batch:351/508, loss:0.2421312779188156\n",
      "epoch:4/10, batch:352/508, loss:0.2650032639503479\n",
      "epoch:4/10, batch:353/508, loss:0.2528035342693329\n",
      "epoch:4/10, batch:354/508, loss:0.2586808204650879\n",
      "epoch:4/10, batch:355/508, loss:0.29397156834602356\n",
      "epoch:4/10, batch:356/508, loss:0.2389102280139923\n",
      "epoch:4/10, batch:357/508, loss:0.1822327822446823\n",
      "epoch:4/10, batch:358/508, loss:0.2732626497745514\n",
      "epoch:4/10, batch:359/508, loss:0.21873588860034943\n",
      "epoch:4/10, batch:360/508, loss:0.2085411101579666\n",
      "epoch:4/10, batch:361/508, loss:0.23543502390384674\n",
      "epoch:4/10, batch:362/508, loss:0.24520641565322876\n",
      "epoch:4/10, batch:363/508, loss:0.2622325122356415\n",
      "epoch:4/10, batch:364/508, loss:0.24518944323062897\n",
      "epoch:4/10, batch:365/508, loss:0.16810618340969086\n",
      "epoch:4/10, batch:366/508, loss:0.2007480412721634\n",
      "epoch:4/10, batch:367/508, loss:0.23356102406978607\n",
      "epoch:4/10, batch:368/508, loss:0.21648451685905457\n",
      "epoch:4/10, batch:369/508, loss:0.27390220761299133\n",
      "epoch:4/10, batch:370/508, loss:0.20343203842639923\n",
      "epoch:4/10, batch:371/508, loss:0.2746422588825226\n",
      "epoch:4/10, batch:372/508, loss:0.2379920780658722\n",
      "epoch:4/10, batch:373/508, loss:0.26919928193092346\n",
      "epoch:4/10, batch:374/508, loss:0.23089829087257385\n",
      "epoch:4/10, batch:375/508, loss:0.24991752207279205\n",
      "epoch:4/10, batch:376/508, loss:0.37861478328704834\n",
      "epoch:4/10, batch:377/508, loss:0.20825938880443573\n",
      "epoch:4/10, batch:378/508, loss:0.19871802628040314\n",
      "epoch:4/10, batch:379/508, loss:0.17063239216804504\n",
      "epoch:4/10, batch:380/508, loss:0.24997004866600037\n",
      "epoch:4/10, batch:381/508, loss:0.21117617189884186\n",
      "evaluating ....\n",
      "Dev Performance P@5 0.401242236025\n",
      "Dev Performance P@1 0.552795031056\n",
      "Dev Performance MAP 0.6294211870237735\n",
      "Dev Performance MRR 0.6718655309493821\n",
      "------------------\n",
      "Test Performance P@5 0.37106918239\n",
      "Test Performance P@1 0.522012578616\n",
      "Test Performance MAP 0.6293137332878355\n",
      "Test Performance MRR 0.6778381756534686\n",
      "epoch:4/10, batch:382/508, loss:0.2782228887081146\n",
      "epoch:4/10, batch:383/508, loss:0.1867786943912506\n",
      "epoch:4/10, batch:384/508, loss:0.21632637083530426\n",
      "epoch:4/10, batch:385/508, loss:0.2458510547876358\n",
      "epoch:4/10, batch:386/508, loss:0.2106170654296875\n",
      "epoch:4/10, batch:387/508, loss:0.20663030445575714\n",
      "epoch:4/10, batch:388/508, loss:0.26147395372390747\n",
      "epoch:4/10, batch:389/508, loss:0.27251794934272766\n",
      "epoch:4/10, batch:390/508, loss:0.23112386465072632\n",
      "epoch:4/10, batch:391/508, loss:0.21954123675823212\n",
      "epoch:4/10, batch:392/508, loss:0.22958356142044067\n",
      "epoch:4/10, batch:393/508, loss:0.2509388327598572\n",
      "epoch:4/10, batch:394/508, loss:0.21258153021335602\n",
      "epoch:4/10, batch:395/508, loss:0.33649519085884094\n",
      "epoch:4/10, batch:396/508, loss:0.28787997364997864\n",
      "epoch:4/10, batch:397/508, loss:0.2526851296424866\n",
      "epoch:4/10, batch:398/508, loss:0.23852328956127167\n",
      "epoch:4/10, batch:399/508, loss:0.29745346307754517\n",
      "epoch:4/10, batch:400/508, loss:0.23578257858753204\n",
      "epoch:4/10, batch:401/508, loss:0.2559138834476471\n",
      "epoch:4/10, batch:402/508, loss:0.20797504484653473\n",
      "epoch:4/10, batch:403/508, loss:0.24451158940792084\n",
      "epoch:4/10, batch:404/508, loss:0.34378373622894287\n",
      "epoch:4/10, batch:405/508, loss:0.1889938861131668\n",
      "epoch:4/10, batch:406/508, loss:0.23390810191631317\n",
      "epoch:4/10, batch:407/508, loss:0.18867333233356476\n",
      "epoch:4/10, batch:408/508, loss:0.3107032775878906\n",
      "epoch:4/10, batch:409/508, loss:0.1739712506532669\n",
      "epoch:4/10, batch:410/508, loss:0.25118735432624817\n",
      "epoch:4/10, batch:411/508, loss:0.22840647399425507\n",
      "epoch:4/10, batch:412/508, loss:0.23572251200675964\n",
      "epoch:4/10, batch:413/508, loss:0.26847216486930847\n",
      "epoch:4/10, batch:414/508, loss:0.25311315059661865\n",
      "epoch:4/10, batch:415/508, loss:0.23564860224723816\n",
      "epoch:4/10, batch:416/508, loss:0.26697900891304016\n",
      "epoch:4/10, batch:417/508, loss:0.2610039710998535\n",
      "epoch:4/10, batch:418/508, loss:0.21219737827777863\n",
      "epoch:4/10, batch:419/508, loss:0.25286397337913513\n",
      "epoch:4/10, batch:420/508, loss:0.2332279086112976\n",
      "epoch:4/10, batch:421/508, loss:0.18636102974414825\n",
      "epoch:4/10, batch:422/508, loss:0.2666143774986267\n",
      "epoch:4/10, batch:423/508, loss:0.2634579539299011\n",
      "epoch:4/10, batch:424/508, loss:0.23671811819076538\n",
      "epoch:4/10, batch:425/508, loss:0.2053278535604477\n",
      "epoch:4/10, batch:426/508, loss:0.2290910929441452\n",
      "epoch:4/10, batch:427/508, loss:0.2637331187725067\n",
      "epoch:4/10, batch:428/508, loss:0.19701911509037018\n",
      "epoch:4/10, batch:429/508, loss:0.23980261385440826\n",
      "epoch:4/10, batch:430/508, loss:0.1795918196439743\n",
      "epoch:4/10, batch:431/508, loss:0.22963373363018036\n",
      "epoch:4/10, batch:432/508, loss:0.18230898678302765\n",
      "epoch:4/10, batch:433/508, loss:0.2799201011657715\n",
      "epoch:4/10, batch:434/508, loss:0.2689919173717499\n",
      "epoch:4/10, batch:435/508, loss:0.19494163990020752\n",
      "epoch:4/10, batch:436/508, loss:0.30935272574424744\n",
      "epoch:4/10, batch:437/508, loss:0.34532034397125244\n",
      "epoch:4/10, batch:438/508, loss:0.2513204514980316\n",
      "epoch:4/10, batch:439/508, loss:0.23576560616493225\n",
      "epoch:4/10, batch:440/508, loss:0.22115230560302734\n",
      "epoch:4/10, batch:441/508, loss:0.23900170624256134\n",
      "epoch:4/10, batch:442/508, loss:0.28073757886886597\n",
      "epoch:4/10, batch:443/508, loss:0.2204262614250183\n",
      "epoch:4/10, batch:444/508, loss:0.25254905223846436\n",
      "epoch:4/10, batch:445/508, loss:0.22628378868103027\n",
      "epoch:4/10, batch:446/508, loss:0.1741441935300827\n",
      "epoch:4/10, batch:447/508, loss:0.28283044695854187\n",
      "epoch:4/10, batch:448/508, loss:0.2035394012928009\n",
      "epoch:4/10, batch:449/508, loss:0.22799114882946014\n",
      "epoch:4/10, batch:450/508, loss:0.20473608374595642\n",
      "epoch:4/10, batch:451/508, loss:0.2630363702774048\n",
      "epoch:4/10, batch:452/508, loss:0.2186826765537262\n",
      "epoch:4/10, batch:453/508, loss:0.23848120868206024\n",
      "epoch:4/10, batch:454/508, loss:0.2900417149066925\n",
      "epoch:4/10, batch:455/508, loss:0.22606024146080017\n",
      "epoch:4/10, batch:456/508, loss:0.2308516949415207\n",
      "epoch:4/10, batch:457/508, loss:0.24337124824523926\n",
      "epoch:4/10, batch:458/508, loss:0.24816995859146118\n",
      "epoch:4/10, batch:459/508, loss:0.26562026143074036\n",
      "epoch:4/10, batch:460/508, loss:0.3279474377632141\n",
      "epoch:4/10, batch:461/508, loss:0.1633073389530182\n",
      "epoch:4/10, batch:462/508, loss:0.25172051787376404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4/10, batch:463/508, loss:0.2724207937717438\n",
      "epoch:4/10, batch:464/508, loss:0.1995088905096054\n",
      "epoch:4/10, batch:465/508, loss:0.2377437949180603\n",
      "epoch:4/10, batch:466/508, loss:0.25949904322624207\n",
      "epoch:4/10, batch:467/508, loss:0.27278339862823486\n",
      "epoch:4/10, batch:468/508, loss:0.2623048722743988\n",
      "epoch:4/10, batch:469/508, loss:0.2593543529510498\n",
      "epoch:4/10, batch:470/508, loss:0.28782618045806885\n",
      "epoch:4/10, batch:471/508, loss:0.2801878750324249\n",
      "epoch:4/10, batch:472/508, loss:0.2774294912815094\n",
      "epoch:4/10, batch:473/508, loss:0.22916428744792938\n",
      "epoch:4/10, batch:474/508, loss:0.21200969815254211\n",
      "epoch:4/10, batch:475/508, loss:0.2268708050251007\n",
      "epoch:4/10, batch:476/508, loss:0.21956926584243774\n",
      "epoch:4/10, batch:477/508, loss:0.3093561828136444\n",
      "epoch:4/10, batch:478/508, loss:0.25246912240982056\n",
      "epoch:4/10, batch:479/508, loss:0.22743161022663116\n",
      "epoch:4/10, batch:480/508, loss:0.19321569800376892\n",
      "epoch:4/10, batch:481/508, loss:0.18387193977832794\n",
      "epoch:4/10, batch:482/508, loss:0.30046796798706055\n",
      "epoch:4/10, batch:483/508, loss:0.2775914669036865\n",
      "epoch:4/10, batch:484/508, loss:0.1934158205986023\n",
      "epoch:4/10, batch:485/508, loss:0.13344743847846985\n",
      "epoch:4/10, batch:486/508, loss:0.23532213270664215\n",
      "epoch:4/10, batch:487/508, loss:0.2556236684322357\n",
      "epoch:4/10, batch:488/508, loss:0.29665184020996094\n",
      "epoch:4/10, batch:489/508, loss:0.2763824164867401\n",
      "epoch:4/10, batch:490/508, loss:0.2936917841434479\n",
      "epoch:4/10, batch:491/508, loss:0.2632729113101959\n",
      "epoch:4/10, batch:492/508, loss:0.18434184789657593\n",
      "epoch:4/10, batch:493/508, loss:0.21907706558704376\n",
      "epoch:4/10, batch:494/508, loss:0.20214203000068665\n",
      "epoch:4/10, batch:495/508, loss:0.19487576186656952\n",
      "epoch:4/10, batch:496/508, loss:0.2464858442544937\n",
      "epoch:4/10, batch:497/508, loss:0.21539592742919922\n",
      "epoch:4/10, batch:498/508, loss:0.20149578154087067\n",
      "epoch:4/10, batch:499/508, loss:0.29798173904418945\n",
      "epoch:4/10, batch:500/508, loss:0.26206833124160767\n",
      "epoch:4/10, batch:501/508, loss:0.1962551325559616\n",
      "epoch:4/10, batch:502/508, loss:0.2760924994945526\n",
      "epoch:4/10, batch:503/508, loss:0.19896377623081207\n",
      "epoch:4/10, batch:504/508, loss:0.21124917268753052\n",
      "epoch:4/10, batch:505/508, loss:0.3371339440345764\n",
      "epoch:4/10, batch:506/508, loss:0.22632499039173126\n",
      "epoch:4/10, batch:507/508, loss:0.261328786611557\n",
      "epoch:4/10, batch:508/508, loss:0.19899554550647736\n",
      "evaluating ....\n",
      "Dev Performance P@5 0.39751552795\n",
      "Dev Performance P@1 0.503105590062\n",
      "Dev Performance MAP 0.6251570251201529\n",
      "Dev Performance MRR 0.6523387747720957\n",
      "------------------\n",
      "Test Performance P@5 0.377358490566\n",
      "Test Performance P@1 0.534591194969\n",
      "Test Performance MAP 0.6315295082303488\n",
      "Test Performance MRR 0.6810234544594028\n",
      "epoch:5/10, batch:1/508, loss:0.23705501854419708\n",
      "epoch:5/10, batch:2/508, loss:0.2737404406070709\n",
      "epoch:5/10, batch:3/508, loss:0.2971018850803375\n",
      "epoch:5/10, batch:4/508, loss:0.20660008490085602\n",
      "epoch:5/10, batch:5/508, loss:0.25705933570861816\n",
      "epoch:5/10, batch:6/508, loss:0.2704194188117981\n",
      "epoch:5/10, batch:7/508, loss:0.23735329508781433\n",
      "epoch:5/10, batch:8/508, loss:0.16556432843208313\n",
      "epoch:5/10, batch:9/508, loss:0.2597506642341614\n",
      "epoch:5/10, batch:10/508, loss:0.18977104127407074\n",
      "epoch:5/10, batch:11/508, loss:0.2187417447566986\n",
      "epoch:5/10, batch:12/508, loss:0.27734142541885376\n",
      "epoch:5/10, batch:13/508, loss:0.1462104469537735\n",
      "epoch:5/10, batch:14/508, loss:0.24302342534065247\n",
      "epoch:5/10, batch:15/508, loss:0.21447044610977173\n",
      "epoch:5/10, batch:16/508, loss:0.21800759434700012\n",
      "epoch:5/10, batch:17/508, loss:0.2686547338962555\n",
      "epoch:5/10, batch:18/508, loss:0.2844921946525574\n",
      "epoch:5/10, batch:19/508, loss:0.1644172966480255\n",
      "epoch:5/10, batch:20/508, loss:0.1591029316186905\n",
      "epoch:5/10, batch:21/508, loss:0.22119320929050446\n",
      "epoch:5/10, batch:22/508, loss:0.20938141644001007\n",
      "epoch:5/10, batch:23/508, loss:0.20694302022457123\n",
      "epoch:5/10, batch:24/508, loss:0.22113071382045746\n",
      "epoch:5/10, batch:25/508, loss:0.2057115137577057\n",
      "epoch:5/10, batch:26/508, loss:0.24621836841106415\n",
      "epoch:5/10, batch:27/508, loss:0.2533632516860962\n",
      "epoch:5/10, batch:28/508, loss:0.25016507506370544\n",
      "epoch:5/10, batch:29/508, loss:0.2838197946548462\n",
      "epoch:5/10, batch:30/508, loss:0.26120129227638245\n",
      "epoch:5/10, batch:31/508, loss:0.2727721631526947\n",
      "epoch:5/10, batch:32/508, loss:0.2803288698196411\n",
      "epoch:5/10, batch:33/508, loss:0.23376168310642242\n",
      "epoch:5/10, batch:34/508, loss:0.21379727125167847\n",
      "epoch:5/10, batch:35/508, loss:0.2129223644733429\n",
      "epoch:5/10, batch:36/508, loss:0.27200737595558167\n",
      "epoch:5/10, batch:37/508, loss:0.2141427844762802\n",
      "epoch:5/10, batch:38/508, loss:0.2277776449918747\n",
      "epoch:5/10, batch:39/508, loss:0.16998505592346191\n",
      "epoch:5/10, batch:40/508, loss:0.16959455609321594\n",
      "epoch:5/10, batch:41/508, loss:0.19711314141750336\n",
      "epoch:5/10, batch:42/508, loss:0.22458593547344208\n",
      "epoch:5/10, batch:43/508, loss:0.255715548992157\n",
      "epoch:5/10, batch:44/508, loss:0.19025537371635437\n",
      "epoch:5/10, batch:45/508, loss:0.15701106190681458\n",
      "epoch:5/10, batch:46/508, loss:0.2380969077348709\n",
      "epoch:5/10, batch:47/508, loss:0.23292800784111023\n",
      "epoch:5/10, batch:48/508, loss:0.24351760745048523\n",
      "epoch:5/10, batch:49/508, loss:0.2617195248603821\n",
      "epoch:5/10, batch:50/508, loss:0.22018882632255554\n",
      "epoch:5/10, batch:51/508, loss:0.18809163570404053\n",
      "epoch:5/10, batch:52/508, loss:0.18584322929382324\n",
      "epoch:5/10, batch:53/508, loss:0.22069762647151947\n",
      "epoch:5/10, batch:54/508, loss:0.20077140629291534\n",
      "epoch:5/10, batch:55/508, loss:0.2060518115758896\n",
      "epoch:5/10, batch:56/508, loss:0.22404465079307556\n",
      "epoch:5/10, batch:57/508, loss:0.25418561697006226\n",
      "epoch:5/10, batch:58/508, loss:0.189034566283226\n",
      "epoch:5/10, batch:59/508, loss:0.29629847407341003\n",
      "epoch:5/10, batch:60/508, loss:0.2723274230957031\n",
      "epoch:5/10, batch:61/508, loss:0.2405019849538803\n",
      "epoch:5/10, batch:62/508, loss:0.17748123407363892\n",
      "epoch:5/10, batch:63/508, loss:0.2541828155517578\n",
      "epoch:5/10, batch:64/508, loss:0.2877028286457062\n",
      "epoch:5/10, batch:65/508, loss:0.2707943618297577\n",
      "epoch:5/10, batch:66/508, loss:0.24242089688777924\n",
      "epoch:5/10, batch:67/508, loss:0.19052769243717194\n",
      "epoch:5/10, batch:68/508, loss:0.1946723610162735\n",
      "epoch:5/10, batch:69/508, loss:0.17020216584205627\n",
      "epoch:5/10, batch:70/508, loss:0.21173135936260223\n",
      "epoch:5/10, batch:71/508, loss:0.22124238312244415\n",
      "epoch:5/10, batch:72/508, loss:0.2264460325241089\n",
      "epoch:5/10, batch:73/508, loss:0.1803293228149414\n",
      "epoch:5/10, batch:74/508, loss:0.25340238213539124\n",
      "epoch:5/10, batch:75/508, loss:0.2573394179344177\n",
      "epoch:5/10, batch:76/508, loss:0.23410649597644806\n",
      "epoch:5/10, batch:77/508, loss:0.28308433294296265\n",
      "epoch:5/10, batch:78/508, loss:0.21154233813285828\n",
      "epoch:5/10, batch:79/508, loss:0.27367740869522095\n",
      "epoch:5/10, batch:80/508, loss:0.2731267511844635\n",
      "epoch:5/10, batch:81/508, loss:0.208152174949646\n",
      "epoch:5/10, batch:82/508, loss:0.21624349057674408\n",
      "epoch:5/10, batch:83/508, loss:0.24710261821746826\n",
      "epoch:5/10, batch:84/508, loss:0.20215854048728943\n",
      "epoch:5/10, batch:85/508, loss:0.14997148513793945\n",
      "epoch:5/10, batch:86/508, loss:0.23478178679943085\n",
      "epoch:5/10, batch:87/508, loss:0.1817372590303421\n",
      "epoch:5/10, batch:88/508, loss:0.22926369309425354\n",
      "epoch:5/10, batch:89/508, loss:0.2649114727973938\n",
      "epoch:5/10, batch:90/508, loss:0.2470422238111496\n",
      "epoch:5/10, batch:91/508, loss:0.21944138407707214\n",
      "epoch:5/10, batch:92/508, loss:0.30705857276916504\n",
      "epoch:5/10, batch:93/508, loss:0.23040421307086945\n",
      "epoch:5/10, batch:94/508, loss:0.176420196890831\n",
      "epoch:5/10, batch:95/508, loss:0.1765083372592926\n",
      "epoch:5/10, batch:96/508, loss:0.2168361097574234\n",
      "epoch:5/10, batch:97/508, loss:0.24542242288589478\n",
      "epoch:5/10, batch:98/508, loss:0.2009904384613037\n",
      "epoch:5/10, batch:99/508, loss:0.19906246662139893\n",
      "epoch:5/10, batch:100/508, loss:0.23771008849143982\n",
      "epoch:5/10, batch:101/508, loss:0.25576460361480713\n",
      "epoch:5/10, batch:102/508, loss:0.16835111379623413\n",
      "epoch:5/10, batch:103/508, loss:0.29059866070747375\n",
      "epoch:5/10, batch:104/508, loss:0.3445146679878235\n",
      "epoch:5/10, batch:105/508, loss:0.2988288104534149\n",
      "epoch:5/10, batch:106/508, loss:0.1813344657421112\n",
      "epoch:5/10, batch:107/508, loss:0.20540358126163483\n",
      "epoch:5/10, batch:108/508, loss:0.22812968492507935\n",
      "epoch:5/10, batch:109/508, loss:0.20336373150348663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5/10, batch:110/508, loss:0.2792927920818329\n",
      "epoch:5/10, batch:111/508, loss:0.18742507696151733\n",
      "epoch:5/10, batch:112/508, loss:0.22436989843845367\n",
      "epoch:5/10, batch:113/508, loss:0.20207613706588745\n",
      "epoch:5/10, batch:114/508, loss:0.29226458072662354\n",
      "epoch:5/10, batch:115/508, loss:0.23603539168834686\n",
      "epoch:5/10, batch:116/508, loss:0.20349618792533875\n",
      "epoch:5/10, batch:117/508, loss:0.23915785551071167\n",
      "epoch:5/10, batch:118/508, loss:0.25877514481544495\n",
      "epoch:5/10, batch:119/508, loss:0.1663273125886917\n",
      "epoch:5/10, batch:120/508, loss:0.26819634437561035\n",
      "epoch:5/10, batch:121/508, loss:0.22131703794002533\n",
      "epoch:5/10, batch:122/508, loss:0.24279989302158356\n",
      "epoch:5/10, batch:123/508, loss:0.24301575124263763\n",
      "epoch:5/10, batch:124/508, loss:0.20106272399425507\n",
      "epoch:5/10, batch:125/508, loss:0.20618030428886414\n",
      "epoch:5/10, batch:126/508, loss:0.22747381031513214\n",
      "epoch:5/10, batch:127/508, loss:0.18711744248867035\n",
      "evaluating ....\n",
      "Dev Performance P@5 0.398757763975\n",
      "Dev Performance P@1 0.515527950311\n",
      "Dev Performance MAP 0.6231994178853235\n",
      "Dev Performance MRR 0.6559614324091855\n",
      "------------------\n",
      "Test Performance P@5 0.37106918239\n",
      "Test Performance P@1 0.51572327044\n",
      "Test Performance MAP 0.6299088695873558\n",
      "Test Performance MRR 0.6693938366977097\n",
      "epoch:5/10, batch:128/508, loss:0.16936515271663666\n",
      "epoch:5/10, batch:129/508, loss:0.21253393590450287\n",
      "epoch:5/10, batch:130/508, loss:0.2508144676685333\n",
      "epoch:5/10, batch:131/508, loss:0.30085673928260803\n",
      "epoch:5/10, batch:132/508, loss:0.18040114641189575\n",
      "epoch:5/10, batch:133/508, loss:0.1639338731765747\n",
      "epoch:5/10, batch:134/508, loss:0.2982141375541687\n",
      "epoch:5/10, batch:135/508, loss:0.19578176736831665\n",
      "epoch:5/10, batch:136/508, loss:0.18981054425239563\n",
      "epoch:5/10, batch:137/508, loss:0.2106316238641739\n",
      "epoch:5/10, batch:138/508, loss:0.21646682918071747\n",
      "epoch:5/10, batch:139/508, loss:0.21040202677249908\n",
      "epoch:5/10, batch:140/508, loss:0.29111209511756897\n",
      "epoch:5/10, batch:141/508, loss:0.27119433879852295\n",
      "epoch:5/10, batch:142/508, loss:0.24191829562187195\n",
      "epoch:5/10, batch:143/508, loss:0.2182849645614624\n",
      "epoch:5/10, batch:144/508, loss:0.20750847458839417\n",
      "epoch:5/10, batch:145/508, loss:0.23107460141181946\n",
      "epoch:5/10, batch:146/508, loss:0.21868272125720978\n",
      "epoch:5/10, batch:147/508, loss:0.21341201663017273\n",
      "epoch:5/10, batch:148/508, loss:0.22516337037086487\n",
      "epoch:5/10, batch:149/508, loss:0.20590057969093323\n",
      "epoch:5/10, batch:150/508, loss:0.22385023534297943\n",
      "epoch:5/10, batch:151/508, loss:0.31870415806770325\n",
      "epoch:5/10, batch:152/508, loss:0.22009626030921936\n",
      "epoch:5/10, batch:153/508, loss:0.21141697466373444\n",
      "epoch:5/10, batch:154/508, loss:0.2528703212738037\n",
      "epoch:5/10, batch:155/508, loss:0.2203308641910553\n",
      "epoch:5/10, batch:156/508, loss:0.21938283741474152\n",
      "epoch:5/10, batch:157/508, loss:0.18282151222229004\n",
      "epoch:5/10, batch:158/508, loss:0.20808391273021698\n",
      "epoch:5/10, batch:159/508, loss:0.27554094791412354\n",
      "epoch:5/10, batch:160/508, loss:0.22735166549682617\n",
      "epoch:5/10, batch:161/508, loss:0.22372354567050934\n",
      "epoch:5/10, batch:162/508, loss:0.20423062145709991\n",
      "epoch:5/10, batch:163/508, loss:0.2797776162624359\n",
      "epoch:5/10, batch:164/508, loss:0.29519084095954895\n",
      "epoch:5/10, batch:165/508, loss:0.2801966965198517\n",
      "epoch:5/10, batch:166/508, loss:0.26553472876548767\n",
      "epoch:5/10, batch:167/508, loss:0.2614530026912689\n",
      "epoch:5/10, batch:168/508, loss:0.24436496198177338\n",
      "epoch:5/10, batch:169/508, loss:0.12366035580635071\n",
      "epoch:5/10, batch:170/508, loss:0.20081664621829987\n",
      "epoch:5/10, batch:171/508, loss:0.29565683007240295\n",
      "epoch:5/10, batch:172/508, loss:0.22583788633346558\n",
      "epoch:5/10, batch:173/508, loss:0.25339779257774353\n",
      "epoch:5/10, batch:174/508, loss:0.2311324030160904\n",
      "epoch:5/10, batch:175/508, loss:0.2510240077972412\n",
      "epoch:5/10, batch:176/508, loss:0.23202906548976898\n",
      "epoch:5/10, batch:177/508, loss:0.16600638628005981\n",
      "epoch:5/10, batch:178/508, loss:0.218721404671669\n",
      "epoch:5/10, batch:179/508, loss:0.20275259017944336\n",
      "epoch:5/10, batch:180/508, loss:0.25513067841529846\n",
      "epoch:5/10, batch:181/508, loss:0.23100975155830383\n",
      "epoch:5/10, batch:182/508, loss:0.20965273678302765\n",
      "epoch:5/10, batch:183/508, loss:0.3172120153903961\n",
      "epoch:5/10, batch:184/508, loss:0.2094268798828125\n",
      "epoch:5/10, batch:185/508, loss:0.23525755107402802\n",
      "epoch:5/10, batch:186/508, loss:0.2240648716688156\n",
      "epoch:5/10, batch:187/508, loss:0.3302842378616333\n",
      "epoch:5/10, batch:188/508, loss:0.23925024271011353\n",
      "epoch:5/10, batch:189/508, loss:0.1981332153081894\n",
      "epoch:5/10, batch:190/508, loss:0.23539462685585022\n",
      "epoch:5/10, batch:191/508, loss:0.263522207736969\n",
      "epoch:5/10, batch:192/508, loss:0.3464052081108093\n",
      "epoch:5/10, batch:193/508, loss:0.17116965353488922\n",
      "epoch:5/10, batch:194/508, loss:0.20923259854316711\n",
      "epoch:5/10, batch:195/508, loss:0.23226748406887054\n",
      "epoch:5/10, batch:196/508, loss:0.1912747323513031\n",
      "epoch:5/10, batch:197/508, loss:0.30233386158943176\n",
      "epoch:5/10, batch:198/508, loss:0.2501688599586487\n",
      "epoch:5/10, batch:199/508, loss:0.2521112561225891\n",
      "epoch:5/10, batch:200/508, loss:0.25111547112464905\n",
      "epoch:5/10, batch:201/508, loss:0.3079361319541931\n",
      "epoch:5/10, batch:202/508, loss:0.21452270448207855\n",
      "epoch:5/10, batch:203/508, loss:0.1763315349817276\n",
      "epoch:5/10, batch:204/508, loss:0.18846707046031952\n",
      "epoch:5/10, batch:205/508, loss:0.2506266236305237\n",
      "epoch:5/10, batch:206/508, loss:0.24519112706184387\n",
      "epoch:5/10, batch:207/508, loss:0.2072766125202179\n",
      "epoch:5/10, batch:208/508, loss:0.22011905908584595\n",
      "epoch:5/10, batch:209/508, loss:0.20588430762290955\n",
      "epoch:5/10, batch:210/508, loss:0.21650035679340363\n",
      "epoch:5/10, batch:211/508, loss:0.24714083969593048\n",
      "epoch:5/10, batch:212/508, loss:0.34045565128326416\n",
      "epoch:5/10, batch:213/508, loss:0.20040379464626312\n",
      "epoch:5/10, batch:214/508, loss:0.24131594598293304\n",
      "epoch:5/10, batch:215/508, loss:0.22715871036052704\n",
      "epoch:5/10, batch:216/508, loss:0.22595931589603424\n",
      "epoch:5/10, batch:217/508, loss:0.25423315167427063\n",
      "epoch:5/10, batch:218/508, loss:0.22812747955322266\n",
      "epoch:5/10, batch:219/508, loss:0.21669258177280426\n",
      "epoch:5/10, batch:220/508, loss:0.17014241218566895\n",
      "epoch:5/10, batch:221/508, loss:0.21868623793125153\n",
      "epoch:5/10, batch:222/508, loss:0.24014812707901\n",
      "epoch:5/10, batch:223/508, loss:0.2957364320755005\n",
      "epoch:5/10, batch:224/508, loss:0.22442714869976044\n",
      "epoch:5/10, batch:225/508, loss:0.2512717545032501\n",
      "epoch:5/10, batch:226/508, loss:0.1719571202993393\n",
      "epoch:5/10, batch:227/508, loss:0.18411549925804138\n",
      "epoch:5/10, batch:228/508, loss:0.3149780035018921\n",
      "epoch:5/10, batch:229/508, loss:0.30272024869918823\n",
      "epoch:5/10, batch:230/508, loss:0.21296632289886475\n",
      "epoch:5/10, batch:231/508, loss:0.19073815643787384\n",
      "epoch:5/10, batch:232/508, loss:0.24111831188201904\n",
      "epoch:5/10, batch:233/508, loss:0.2730821371078491\n",
      "epoch:5/10, batch:234/508, loss:0.15591761469841003\n",
      "epoch:5/10, batch:235/508, loss:0.2700347900390625\n",
      "epoch:5/10, batch:236/508, loss:0.23036910593509674\n",
      "epoch:5/10, batch:237/508, loss:0.23785319924354553\n",
      "epoch:5/10, batch:238/508, loss:0.2928970456123352\n",
      "epoch:5/10, batch:239/508, loss:0.20249886810779572\n",
      "epoch:5/10, batch:240/508, loss:0.24369338154792786\n",
      "epoch:5/10, batch:241/508, loss:0.1916983276605606\n",
      "epoch:5/10, batch:242/508, loss:0.2134047895669937\n",
      "epoch:5/10, batch:243/508, loss:0.1878109574317932\n",
      "epoch:5/10, batch:244/508, loss:0.23369987308979034\n",
      "epoch:5/10, batch:245/508, loss:0.14449214935302734\n",
      "epoch:5/10, batch:246/508, loss:0.2416573166847229\n",
      "epoch:5/10, batch:247/508, loss:0.24911543726921082\n",
      "epoch:5/10, batch:248/508, loss:0.22533468902111053\n",
      "epoch:5/10, batch:249/508, loss:0.19271257519721985\n",
      "epoch:5/10, batch:250/508, loss:0.265632688999176\n",
      "epoch:5/10, batch:251/508, loss:0.2064952552318573\n",
      "epoch:5/10, batch:252/508, loss:0.18556424975395203\n",
      "epoch:5/10, batch:253/508, loss:0.23229096829891205\n",
      "epoch:5/10, batch:254/508, loss:0.22242267429828644\n",
      "evaluating ....\n",
      "Dev Performance P@5 0.393788819876\n",
      "Dev Performance P@1 0.546583850932\n",
      "Dev Performance MAP 0.6340295455245882\n",
      "Dev Performance MRR 0.6732332453605745\n",
      "------------------\n",
      "Test Performance P@5 0.374842767296\n",
      "Test Performance P@1 0.540880503145\n",
      "Test Performance MAP 0.6378979974710961\n",
      "Test Performance MRR 0.6792668253442831\n",
      "epoch:5/10, batch:255/508, loss:0.15651899576187134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5/10, batch:256/508, loss:0.17769354581832886\n",
      "epoch:5/10, batch:257/508, loss:0.26144930720329285\n",
      "epoch:5/10, batch:258/508, loss:0.2440420687198639\n",
      "epoch:5/10, batch:259/508, loss:0.2430574595928192\n",
      "epoch:5/10, batch:260/508, loss:0.24134106934070587\n",
      "epoch:5/10, batch:261/508, loss:0.24582868814468384\n",
      "epoch:5/10, batch:262/508, loss:0.1772305816411972\n",
      "epoch:5/10, batch:263/508, loss:0.23247958719730377\n",
      "epoch:5/10, batch:264/508, loss:0.27415338158607483\n",
      "epoch:5/10, batch:265/508, loss:0.16723093390464783\n",
      "epoch:5/10, batch:266/508, loss:0.2662244141101837\n",
      "epoch:5/10, batch:267/508, loss:0.2843785285949707\n",
      "epoch:5/10, batch:268/508, loss:0.25946709513664246\n",
      "epoch:5/10, batch:269/508, loss:0.22602175176143646\n",
      "epoch:5/10, batch:270/508, loss:0.2564050853252411\n",
      "epoch:5/10, batch:271/508, loss:0.20254814624786377\n",
      "epoch:5/10, batch:272/508, loss:0.23773017525672913\n",
      "epoch:5/10, batch:273/508, loss:0.21339692175388336\n",
      "epoch:5/10, batch:274/508, loss:0.20723706483840942\n",
      "epoch:5/10, batch:275/508, loss:0.19794458150863647\n",
      "epoch:5/10, batch:276/508, loss:0.24082478880882263\n",
      "epoch:5/10, batch:277/508, loss:0.25726786255836487\n",
      "epoch:5/10, batch:278/508, loss:0.2871307134628296\n",
      "epoch:5/10, batch:279/508, loss:0.19587920606136322\n",
      "epoch:5/10, batch:280/508, loss:0.1988351196050644\n",
      "epoch:5/10, batch:281/508, loss:0.20330820977687836\n",
      "epoch:5/10, batch:282/508, loss:0.21765422821044922\n",
      "epoch:5/10, batch:283/508, loss:0.21380715072155\n",
      "epoch:5/10, batch:284/508, loss:0.21782414615154266\n",
      "epoch:5/10, batch:285/508, loss:0.29553329944610596\n",
      "epoch:5/10, batch:286/508, loss:0.24013446271419525\n",
      "epoch:5/10, batch:287/508, loss:0.1901807188987732\n",
      "epoch:5/10, batch:288/508, loss:0.18611854314804077\n",
      "epoch:5/10, batch:289/508, loss:0.22991594672203064\n",
      "epoch:5/10, batch:290/508, loss:0.22027699649333954\n",
      "epoch:5/10, batch:291/508, loss:0.20059405267238617\n",
      "epoch:5/10, batch:292/508, loss:0.2834501564502716\n",
      "epoch:5/10, batch:293/508, loss:0.2556220591068268\n",
      "epoch:5/10, batch:294/508, loss:0.2726968824863434\n",
      "epoch:5/10, batch:295/508, loss:0.24509969353675842\n",
      "epoch:5/10, batch:296/508, loss:0.20225918292999268\n",
      "epoch:5/10, batch:297/508, loss:0.30443906784057617\n",
      "epoch:5/10, batch:298/508, loss:0.22193707525730133\n",
      "epoch:5/10, batch:299/508, loss:0.18464483320713043\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-144-db90b1391fec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEmbeddingLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHIDDEN_DIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lstm'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'lstm'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-142-a0ec9e6a8120>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(layer_type, embedding_layer, batch_size, num_epoch, id_set, eval)\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'epoch:{}/{}, batch:{}/{}, loss:{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \"\"\"\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 98\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = EmbeddingLayer(200, HIDDEN_DIM, 'lstm') # loss margin = 0.5\n",
    "train('lstm', model, batch_size=25, num_epoch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_model(model, 'models/lstm_bi_epoch=4.5_margin=.5_hidden=120')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1/10, batch:1/508, loss:0.06204039976000786\n",
      "epoch:1/10, batch:2/508, loss:0.06698644161224365\n",
      "epoch:1/10, batch:3/508, loss:0.08204396069049835\n",
      "epoch:1/10, batch:4/508, loss:0.07271836698055267\n",
      "epoch:1/10, batch:5/508, loss:0.11348847299814224\n",
      "epoch:1/10, batch:6/508, loss:0.12347452342510223\n",
      "epoch:1/10, batch:7/508, loss:0.07170307636260986\n",
      "epoch:1/10, batch:8/508, loss:0.05405590310692787\n",
      "epoch:1/10, batch:9/508, loss:0.13060428202152252\n",
      "epoch:1/10, batch:10/508, loss:0.07260049134492874\n",
      "epoch:1/10, batch:11/508, loss:0.08073266595602036\n",
      "epoch:1/10, batch:12/508, loss:0.14527538418769836\n",
      "epoch:1/10, batch:13/508, loss:0.05466698110103607\n",
      "epoch:1/10, batch:14/508, loss:0.08360808342695236\n",
      "epoch:1/10, batch:15/508, loss:0.0836087018251419\n",
      "epoch:1/10, batch:16/508, loss:0.09609977155923843\n",
      "epoch:1/10, batch:17/508, loss:0.10398983210325241\n",
      "epoch:1/10, batch:18/508, loss:0.1288212686777115\n",
      "epoch:1/10, batch:19/508, loss:0.03824116662144661\n",
      "epoch:1/10, batch:20/508, loss:0.034473661333322525\n",
      "epoch:1/10, batch:21/508, loss:0.09659384936094284\n",
      "epoch:1/10, batch:22/508, loss:0.07781846076250076\n",
      "epoch:1/10, batch:23/508, loss:0.08880175650119781\n",
      "epoch:1/10, batch:24/508, loss:0.1002836674451828\n",
      "epoch:1/10, batch:25/508, loss:0.09663750976324081\n",
      "evaluating ....\n",
      "Dev Performance P@5 0.403726708075\n",
      "Dev Performance P@1 0.546583850932\n",
      "Dev Performance MAP 0.6264974227024006\n",
      "Dev Performance MRR 0.6723516674329609\n",
      "------------------\n",
      "Test Performance P@5 0.368553459119\n",
      "Test Performance P@1 0.540880503145\n",
      "Test Performance MAP 0.6399132663804957\n",
      "Test Performance MRR 0.6856381793571468\n",
      "epoch:1/10, batch:26/508, loss:0.13447436690330505\n",
      "epoch:1/10, batch:27/508, loss:0.08450022339820862\n",
      "epoch:1/10, batch:28/508, loss:0.09064411371946335\n",
      "epoch:1/10, batch:29/508, loss:0.10594522953033447\n",
      "epoch:1/10, batch:30/508, loss:0.10714061558246613\n",
      "epoch:1/10, batch:31/508, loss:0.12672273814678192\n",
      "epoch:1/10, batch:32/508, loss:0.10365843772888184\n",
      "epoch:1/10, batch:33/508, loss:0.09424255043268204\n",
      "epoch:1/10, batch:34/508, loss:0.10167635977268219\n",
      "epoch:1/10, batch:35/508, loss:0.09326761215925217\n",
      "epoch:1/10, batch:36/508, loss:0.11956734955310822\n",
      "epoch:1/10, batch:37/508, loss:0.08456160128116608\n",
      "epoch:1/10, batch:38/508, loss:0.09245448559522629\n",
      "epoch:1/10, batch:39/508, loss:0.09253036975860596\n",
      "epoch:1/10, batch:40/508, loss:0.10610334575176239\n",
      "epoch:1/10, batch:41/508, loss:0.07033271342515945\n",
      "epoch:1/10, batch:42/508, loss:0.1148577407002449\n",
      "epoch:1/10, batch:43/508, loss:0.13309550285339355\n",
      "epoch:1/10, batch:44/508, loss:0.0906161218881607\n",
      "epoch:1/10, batch:45/508, loss:0.06436991691589355\n",
      "epoch:1/10, batch:46/508, loss:0.106843002140522\n",
      "epoch:1/10, batch:47/508, loss:0.10574543476104736\n",
      "epoch:1/10, batch:48/508, loss:0.10789930075407028\n",
      "epoch:1/10, batch:49/508, loss:0.0982334092259407\n",
      "epoch:1/10, batch:50/508, loss:0.0789995789527893\n",
      "evaluating ....\n",
      "Dev Performance P@5 0.396273291925\n",
      "Dev Performance P@1 0.546583850932\n",
      "Dev Performance MAP 0.6317550376985139\n",
      "Dev Performance MRR 0.6712358689295116\n",
      "------------------\n",
      "Test Performance P@5 0.37106918239\n",
      "Test Performance P@1 0.553459119497\n",
      "Test Performance MAP 0.6430936512210027\n",
      "Test Performance MRR 0.6920103885595446\n",
      "epoch:1/10, batch:51/508, loss:0.08062314987182617\n",
      "epoch:1/10, batch:52/508, loss:0.06334889680147171\n",
      "epoch:1/10, batch:53/508, loss:0.0863930881023407\n",
      "epoch:1/10, batch:54/508, loss:0.08468839526176453\n",
      "epoch:1/10, batch:55/508, loss:0.09255748987197876\n",
      "epoch:1/10, batch:56/508, loss:0.07438299059867859\n",
      "epoch:1/10, batch:57/508, loss:0.10391288995742798\n",
      "epoch:1/10, batch:58/508, loss:0.10485994815826416\n",
      "epoch:1/10, batch:59/508, loss:0.1350363790988922\n",
      "epoch:1/10, batch:60/508, loss:0.11643565446138382\n",
      "epoch:1/10, batch:61/508, loss:0.08424875140190125\n",
      "epoch:1/10, batch:62/508, loss:0.08558327704668045\n",
      "epoch:1/10, batch:63/508, loss:0.1042114719748497\n",
      "epoch:1/10, batch:64/508, loss:0.1378486305475235\n",
      "epoch:1/10, batch:65/508, loss:0.10690349340438843\n",
      "epoch:1/10, batch:66/508, loss:0.07734782993793488\n",
      "epoch:1/10, batch:67/508, loss:0.08245107531547546\n",
      "epoch:1/10, batch:68/508, loss:0.06217960640788078\n",
      "epoch:1/10, batch:69/508, loss:0.07909134775400162\n",
      "epoch:1/10, batch:70/508, loss:0.06453556567430496\n",
      "epoch:1/10, batch:71/508, loss:0.10103791952133179\n",
      "epoch:1/10, batch:72/508, loss:0.0864960253238678\n",
      "epoch:1/10, batch:73/508, loss:0.07632187008857727\n",
      "epoch:1/10, batch:74/508, loss:0.09861849993467331\n",
      "epoch:1/10, batch:75/508, loss:0.10911431163549423\n",
      "evaluating ....\n",
      "Dev Performance P@5 0.398757763975\n",
      "Dev Performance P@1 0.552795031056\n",
      "Dev Performance MAP 0.6319955412091983\n",
      "Dev Performance MRR 0.6741629057622845\n",
      "------------------\n",
      "Test Performance P@5 0.367295597484\n",
      "Test Performance P@1 0.540880503145\n",
      "Test Performance MAP 0.6350623449130176\n",
      "Test Performance MRR 0.6831656518846192\n",
      "epoch:1/10, batch:76/508, loss:0.13024400174617767\n",
      "epoch:1/10, batch:77/508, loss:0.14584314823150635\n",
      "epoch:1/10, batch:78/508, loss:0.05074673891067505\n",
      "epoch:1/10, batch:79/508, loss:0.0887855663895607\n",
      "epoch:1/10, batch:80/508, loss:0.1306835263967514\n",
      "epoch:1/10, batch:81/508, loss:0.08532267063856125\n",
      "epoch:1/10, batch:82/508, loss:0.05905339494347572\n",
      "epoch:1/10, batch:83/508, loss:0.1268201619386673\n",
      "epoch:1/10, batch:84/508, loss:0.09161476045846939\n",
      "epoch:1/10, batch:85/508, loss:0.049834124743938446\n",
      "epoch:1/10, batch:86/508, loss:0.12088322639465332\n",
      "epoch:1/10, batch:87/508, loss:0.09998011589050293\n",
      "epoch:1/10, batch:88/508, loss:0.0831763744354248\n",
      "epoch:1/10, batch:89/508, loss:0.08975543826818466\n",
      "epoch:1/10, batch:90/508, loss:0.06727343797683716\n",
      "epoch:1/10, batch:91/508, loss:0.07930073142051697\n",
      "epoch:1/10, batch:92/508, loss:0.15656134486198425\n",
      "epoch:1/10, batch:93/508, loss:0.12136391550302505\n",
      "epoch:1/10, batch:94/508, loss:0.0762263759970665\n",
      "epoch:1/10, batch:95/508, loss:0.05167662352323532\n",
      "epoch:1/10, batch:96/508, loss:0.08286251872777939\n",
      "epoch:1/10, batch:97/508, loss:0.11194101721048355\n",
      "epoch:1/10, batch:98/508, loss:0.09669239073991776\n",
      "epoch:1/10, batch:99/508, loss:0.056707851588726044\n",
      "epoch:1/10, batch:100/508, loss:0.12711378931999207\n",
      "evaluating ....\n",
      "Dev Performance P@5 0.401242236025\n",
      "Dev Performance P@1 0.534161490683\n",
      "Dev Performance MAP 0.6322392796794208\n",
      "Dev Performance MRR 0.665773818902244\n",
      "------------------\n",
      "Test Performance P@5 0.377358490566\n",
      "Test Performance P@1 0.553459119497\n",
      "Test Performance MAP 0.6373235356280916\n",
      "Test Performance MRR 0.6909422044724925\n",
      "epoch:1/10, batch:101/508, loss:0.13125580549240112\n",
      "epoch:1/10, batch:102/508, loss:0.05956730246543884\n",
      "epoch:1/10, batch:103/508, loss:0.1365373581647873\n",
      "epoch:1/10, batch:104/508, loss:0.1930154711008072\n",
      "epoch:1/10, batch:105/508, loss:0.10307034105062485\n",
      "epoch:1/10, batch:106/508, loss:0.08513674139976501\n",
      "epoch:1/10, batch:107/508, loss:0.09382668882608414\n",
      "epoch:1/10, batch:108/508, loss:0.11004967242479324\n",
      "epoch:1/10, batch:109/508, loss:0.09546678513288498\n",
      "epoch:1/10, batch:110/508, loss:0.09889370203018188\n",
      "epoch:1/10, batch:111/508, loss:0.09249966591596603\n",
      "epoch:1/10, batch:112/508, loss:0.11335443705320358\n",
      "epoch:1/10, batch:113/508, loss:0.07320060580968857\n",
      "epoch:1/10, batch:114/508, loss:0.11562378704547882\n",
      "epoch:1/10, batch:115/508, loss:0.10292332619428635\n",
      "epoch:1/10, batch:116/508, loss:0.08245676010847092\n",
      "epoch:1/10, batch:117/508, loss:0.1132686510682106\n",
      "epoch:1/10, batch:118/508, loss:0.08106248080730438\n",
      "epoch:1/10, batch:119/508, loss:0.0560537688434124\n",
      "epoch:1/10, batch:120/508, loss:0.11922872811555862\n",
      "epoch:1/10, batch:121/508, loss:0.07680200785398483\n",
      "epoch:1/10, batch:122/508, loss:0.1209745705127716\n",
      "epoch:1/10, batch:123/508, loss:0.10672225803136826\n",
      "epoch:1/10, batch:124/508, loss:0.052520908415317535\n",
      "epoch:1/10, batch:125/508, loss:0.07695029675960541\n",
      "evaluating ....\n",
      "Dev Performance P@5 0.40248447205\n",
      "Dev Performance P@1 0.509316770186\n",
      "Dev Performance MAP 0.6200647195599989\n",
      "Dev Performance MRR 0.6509461552939816\n",
      "------------------\n",
      "Test Performance P@5 0.379874213836\n",
      "Test Performance P@1 0.553459119497\n",
      "Test Performance MAP 0.6346629004432877\n",
      "Test Performance MRR 0.6909233847913093\n",
      "epoch:1/10, batch:126/508, loss:0.0894295871257782\n",
      "epoch:1/10, batch:127/508, loss:0.07956308126449585\n",
      "epoch:1/10, batch:128/508, loss:0.07538924366235733\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1/10, batch:129/508, loss:0.07693225145339966\n",
      "epoch:1/10, batch:130/508, loss:0.13850216567516327\n",
      "epoch:1/10, batch:131/508, loss:0.1792794018983841\n",
      "epoch:1/10, batch:132/508, loss:0.09958145767450333\n",
      "epoch:1/10, batch:133/508, loss:0.08008226007223129\n",
      "epoch:1/10, batch:134/508, loss:0.1331702023744583\n",
      "epoch:1/10, batch:135/508, loss:0.07687525451183319\n",
      "epoch:1/10, batch:136/508, loss:0.08159011602401733\n",
      "epoch:1/10, batch:137/508, loss:0.08214867860078812\n",
      "epoch:1/10, batch:138/508, loss:0.08653660118579865\n",
      "epoch:1/10, batch:139/508, loss:0.08022867143154144\n",
      "epoch:1/10, batch:140/508, loss:0.0887611135840416\n",
      "epoch:1/10, batch:141/508, loss:0.12374649196863174\n",
      "epoch:1/10, batch:142/508, loss:0.08957083523273468\n",
      "epoch:1/10, batch:143/508, loss:0.07674036175012589\n",
      "epoch:1/10, batch:144/508, loss:0.07492111623287201\n",
      "epoch:1/10, batch:145/508, loss:0.06743984669446945\n",
      "epoch:1/10, batch:146/508, loss:0.09372374415397644\n",
      "epoch:1/10, batch:147/508, loss:0.07385239005088806\n",
      "epoch:1/10, batch:148/508, loss:0.0731256827712059\n",
      "epoch:1/10, batch:149/508, loss:0.08190370351076126\n",
      "epoch:1/10, batch:150/508, loss:0.07792524248361588\n",
      "evaluating ....\n",
      "Dev Performance P@5 0.39751552795\n",
      "Dev Performance P@1 0.509316770186\n",
      "Dev Performance MAP 0.6152610100714928\n",
      "Dev Performance MRR 0.650499382381918\n",
      "------------------\n",
      "Test Performance P@5 0.372327044025\n",
      "Test Performance P@1 0.540880503145\n",
      "Test Performance MAP 0.6338418638511351\n",
      "Test Performance MRR 0.6842842168313867\n",
      "epoch:1/10, batch:151/508, loss:0.1624450981616974\n",
      "epoch:1/10, batch:152/508, loss:0.10843338817358017\n",
      "epoch:1/10, batch:153/508, loss:0.11153457313776016\n",
      "epoch:1/10, batch:154/508, loss:0.11004538834095001\n",
      "epoch:1/10, batch:155/508, loss:0.10926898568868637\n",
      "epoch:1/10, batch:156/508, loss:0.09694527834653854\n",
      "epoch:1/10, batch:157/508, loss:0.07450110465288162\n",
      "epoch:1/10, batch:158/508, loss:0.11218985915184021\n",
      "epoch:1/10, batch:159/508, loss:0.1028914749622345\n",
      "epoch:1/10, batch:160/508, loss:0.09550872445106506\n",
      "epoch:1/10, batch:161/508, loss:0.10349217802286148\n",
      "epoch:1/10, batch:162/508, loss:0.1310771256685257\n",
      "epoch:1/10, batch:163/508, loss:0.11152733862400055\n",
      "epoch:1/10, batch:164/508, loss:0.13888685405254364\n",
      "epoch:1/10, batch:165/508, loss:0.08619707077741623\n",
      "epoch:1/10, batch:166/508, loss:0.1276395618915558\n",
      "epoch:1/10, batch:167/508, loss:0.08837952464818954\n",
      "epoch:1/10, batch:168/508, loss:0.08712615072727203\n",
      "epoch:1/10, batch:169/508, loss:0.053584251552820206\n",
      "epoch:1/10, batch:170/508, loss:0.09967946261167526\n",
      "epoch:1/10, batch:171/508, loss:0.11760330200195312\n",
      "epoch:1/10, batch:172/508, loss:0.09466119855642319\n",
      "epoch:1/10, batch:173/508, loss:0.10036542266607285\n",
      "epoch:1/10, batch:174/508, loss:0.0862615779042244\n",
      "epoch:1/10, batch:175/508, loss:0.0821487307548523\n",
      "evaluating ....\n",
      "Dev Performance P@5 0.40248447205\n",
      "Dev Performance P@1 0.521739130435\n",
      "Dev Performance MAP 0.6259343678962637\n",
      "Dev Performance MRR 0.6547749265916969\n",
      "------------------\n",
      "Test Performance P@5 0.363522012579\n",
      "Test Performance P@1 0.553459119497\n",
      "Test Performance MAP 0.6363696793853503\n",
      "Test Performance MRR 0.6946979351150155\n",
      "epoch:1/10, batch:176/508, loss:0.0974736139178276\n",
      "epoch:1/10, batch:177/508, loss:0.07315541803836823\n",
      "epoch:1/10, batch:178/508, loss:0.09990042448043823\n",
      "epoch:1/10, batch:179/508, loss:0.0783471167087555\n",
      "epoch:1/10, batch:180/508, loss:0.132586270570755\n",
      "epoch:1/10, batch:181/508, loss:0.10325029492378235\n",
      "epoch:1/10, batch:182/508, loss:0.06321092694997787\n",
      "epoch:1/10, batch:183/508, loss:0.13183388113975525\n",
      "epoch:1/10, batch:184/508, loss:0.09698713570833206\n",
      "epoch:1/10, batch:185/508, loss:0.11108685284852982\n",
      "epoch:1/10, batch:186/508, loss:0.0925474464893341\n",
      "epoch:1/10, batch:187/508, loss:0.16167770326137543\n",
      "epoch:1/10, batch:188/508, loss:0.10035780072212219\n",
      "epoch:1/10, batch:189/508, loss:0.08594485372304916\n",
      "epoch:1/10, batch:190/508, loss:0.09673699736595154\n",
      "epoch:1/10, batch:191/508, loss:0.09460017085075378\n",
      "epoch:1/10, batch:192/508, loss:0.1279289722442627\n",
      "epoch:1/10, batch:193/508, loss:0.06262768805027008\n",
      "epoch:1/10, batch:194/508, loss:0.08235662430524826\n",
      "epoch:1/10, batch:195/508, loss:0.10781591385602951\n",
      "epoch:1/10, batch:196/508, loss:0.0631660670042038\n",
      "epoch:1/10, batch:197/508, loss:0.11381223052740097\n",
      "epoch:1/10, batch:198/508, loss:0.11841119825839996\n",
      "epoch:1/10, batch:199/508, loss:0.11672071367502213\n",
      "epoch:1/10, batch:200/508, loss:0.09759689122438431\n",
      "evaluating ....\n",
      "Dev Performance P@5 0.40248447205\n",
      "Dev Performance P@1 0.527950310559\n",
      "Dev Performance MAP 0.6259260528065057\n",
      "Dev Performance MRR 0.6635609671049933\n",
      "------------------\n",
      "Test Performance P@5 0.358490566038\n",
      "Test Performance P@1 0.522012578616\n",
      "Test Performance MAP 0.6242936390826697\n",
      "Test Performance MRR 0.6757284288536113\n",
      "epoch:1/10, batch:201/508, loss:0.13990454375743866\n",
      "epoch:1/10, batch:202/508, loss:0.1074666902422905\n",
      "epoch:1/10, batch:203/508, loss:0.0748860314488411\n",
      "epoch:1/10, batch:204/508, loss:0.08867499977350235\n",
      "epoch:1/10, batch:205/508, loss:0.08859983831644058\n",
      "epoch:1/10, batch:206/508, loss:0.12255876511335373\n",
      "epoch:1/10, batch:207/508, loss:0.07415802776813507\n",
      "epoch:1/10, batch:208/508, loss:0.08056632429361343\n",
      "epoch:1/10, batch:209/508, loss:0.06468657404184341\n",
      "epoch:1/10, batch:210/508, loss:0.13608482480049133\n",
      "epoch:1/10, batch:211/508, loss:0.10920488834381104\n",
      "epoch:1/10, batch:212/508, loss:0.15231971442699432\n",
      "epoch:1/10, batch:213/508, loss:0.054703712463378906\n",
      "epoch:1/10, batch:214/508, loss:0.10963921993970871\n",
      "epoch:1/10, batch:215/508, loss:0.10852828621864319\n",
      "epoch:1/10, batch:216/508, loss:0.0546637624502182\n",
      "epoch:1/10, batch:217/508, loss:0.09320856630802155\n",
      "epoch:1/10, batch:218/508, loss:0.09688805788755417\n",
      "epoch:1/10, batch:219/508, loss:0.04873654991388321\n",
      "epoch:1/10, batch:220/508, loss:0.06656661629676819\n",
      "epoch:1/10, batch:221/508, loss:0.07249920815229416\n",
      "epoch:1/10, batch:222/508, loss:0.10229188203811646\n",
      "epoch:1/10, batch:223/508, loss:0.1331184208393097\n",
      "epoch:1/10, batch:224/508, loss:0.07479099929332733\n",
      "epoch:1/10, batch:225/508, loss:0.09535697102546692\n",
      "evaluating ....\n",
      "Dev Performance P@5 0.403726708075\n",
      "Dev Performance P@1 0.540372670807\n",
      "Dev Performance MAP 0.6345699973814173\n",
      "Dev Performance MRR 0.6674000688752241\n",
      "------------------\n",
      "Test Performance P@5 0.362264150943\n",
      "Test Performance P@1 0.503144654088\n",
      "Test Performance MAP 0.6247515137985375\n",
      "Test Performance MRR 0.6669377847133557\n",
      "epoch:1/10, batch:226/508, loss:0.06968484818935394\n",
      "epoch:1/10, batch:227/508, loss:0.07189162820577621\n",
      "epoch:1/10, batch:228/508, loss:0.09150063991546631\n",
      "epoch:1/10, batch:229/508, loss:0.14240628480911255\n",
      "epoch:1/10, batch:230/508, loss:0.05970524251461029\n",
      "epoch:1/10, batch:231/508, loss:0.07065564393997192\n",
      "epoch:1/10, batch:232/508, loss:0.1421992927789688\n",
      "epoch:1/10, batch:233/508, loss:0.12315484136343002\n",
      "epoch:1/10, batch:234/508, loss:0.05538032948970795\n",
      "epoch:1/10, batch:235/508, loss:0.12702953815460205\n",
      "epoch:1/10, batch:236/508, loss:0.08818347007036209\n",
      "epoch:1/10, batch:237/508, loss:0.10781201720237732\n",
      "epoch:1/10, batch:238/508, loss:0.14951004087924957\n",
      "epoch:1/10, batch:239/508, loss:0.11266972869634628\n",
      "epoch:1/10, batch:240/508, loss:0.13263975083827972\n",
      "epoch:1/10, batch:241/508, loss:0.08920857310295105\n",
      "epoch:1/10, batch:242/508, loss:0.08240243047475815\n",
      "epoch:1/10, batch:243/508, loss:0.06847730278968811\n",
      "epoch:1/10, batch:244/508, loss:0.09919171035289764\n",
      "epoch:1/10, batch:245/508, loss:0.036874182522296906\n",
      "epoch:1/10, batch:246/508, loss:0.09153592586517334\n",
      "epoch:1/10, batch:247/508, loss:0.07544687390327454\n",
      "epoch:1/10, batch:248/508, loss:0.06987521797418594\n",
      "epoch:1/10, batch:249/508, loss:0.09178340435028076\n",
      "epoch:1/10, batch:250/508, loss:0.1385684460401535\n",
      "evaluating ....\n",
      "Dev Performance P@5 0.393788819876\n",
      "Dev Performance P@1 0.534161490683\n",
      "Dev Performance MAP 0.6306999173851299\n",
      "Dev Performance MRR 0.6617561380486116\n",
      "------------------\n",
      "Test Performance P@5 0.366037735849\n",
      "Test Performance P@1 0.522012578616\n",
      "Test Performance MAP 0.6315066408085728\n",
      "Test Performance MRR 0.672320365095936\n",
      "epoch:1/10, batch:251/508, loss:0.06447917968034744\n",
      "epoch:1/10, batch:252/508, loss:0.05694538354873657\n",
      "epoch:1/10, batch:253/508, loss:0.11975753307342529\n",
      "epoch:1/10, batch:254/508, loss:0.10379277914762497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1/10, batch:255/508, loss:0.06415783613920212\n",
      "epoch:1/10, batch:256/508, loss:0.11833930760622025\n",
      "epoch:1/10, batch:257/508, loss:0.1142667680978775\n",
      "epoch:1/10, batch:258/508, loss:0.12293071299791336\n",
      "epoch:1/10, batch:259/508, loss:0.13567987084388733\n",
      "epoch:1/10, batch:260/508, loss:0.13079805672168732\n",
      "epoch:1/10, batch:261/508, loss:0.0925537720322609\n",
      "epoch:1/10, batch:262/508, loss:0.04757543280720711\n",
      "epoch:1/10, batch:263/508, loss:0.07988748699426651\n",
      "epoch:1/10, batch:264/508, loss:0.14379945397377014\n",
      "epoch:1/10, batch:265/508, loss:0.055405762046575546\n",
      "epoch:1/10, batch:266/508, loss:0.11227753013372421\n",
      "epoch:1/10, batch:267/508, loss:0.11142270267009735\n",
      "epoch:1/10, batch:268/508, loss:0.11688664555549622\n",
      "epoch:1/10, batch:269/508, loss:0.07529149204492569\n",
      "epoch:1/10, batch:270/508, loss:0.09170898795127869\n",
      "epoch:1/10, batch:271/508, loss:0.11453507095575333\n",
      "epoch:1/10, batch:272/508, loss:0.07868916541337967\n",
      "epoch:1/10, batch:273/508, loss:0.07169362157583237\n",
      "epoch:1/10, batch:274/508, loss:0.0596579946577549\n",
      "epoch:1/10, batch:275/508, loss:0.07881525158882141\n",
      "evaluating ....\n",
      "Dev Performance P@5 0.390062111801\n",
      "Dev Performance P@1 0.540372670807\n",
      "Dev Performance MAP 0.6327591935259509\n",
      "Dev Performance MRR 0.6693389081587839\n",
      "------------------\n",
      "Test Performance P@5 0.363522012579\n",
      "Test Performance P@1 0.540880503145\n",
      "Test Performance MAP 0.6416920774834587\n",
      "Test Performance MRR 0.6854914707198717\n",
      "epoch:1/10, batch:276/508, loss:0.08226849883794785\n",
      "epoch:1/10, batch:277/508, loss:0.1152099221944809\n",
      "epoch:1/10, batch:278/508, loss:0.09281931072473526\n",
      "epoch:1/10, batch:279/508, loss:0.06164008751511574\n",
      "epoch:1/10, batch:280/508, loss:0.08368285745382309\n",
      "epoch:1/10, batch:281/508, loss:0.0432741641998291\n",
      "epoch:1/10, batch:282/508, loss:0.09423283487558365\n",
      "epoch:1/10, batch:283/508, loss:0.09790488332509995\n",
      "epoch:1/10, batch:284/508, loss:0.08785443753004074\n",
      "epoch:1/10, batch:285/508, loss:0.1145646944642067\n",
      "epoch:1/10, batch:286/508, loss:0.09085416793823242\n",
      "epoch:1/10, batch:287/508, loss:0.08456757664680481\n",
      "epoch:1/10, batch:288/508, loss:0.055509988218545914\n",
      "epoch:1/10, batch:289/508, loss:0.1195489689707756\n",
      "epoch:1/10, batch:290/508, loss:0.07321751862764359\n",
      "epoch:1/10, batch:291/508, loss:0.10535091161727905\n",
      "epoch:1/10, batch:292/508, loss:0.0860862284898758\n",
      "epoch:1/10, batch:293/508, loss:0.11577928811311722\n",
      "epoch:1/10, batch:294/508, loss:0.11330744624137878\n",
      "epoch:1/10, batch:295/508, loss:0.10578912496566772\n",
      "epoch:1/10, batch:296/508, loss:0.08704356104135513\n",
      "epoch:1/10, batch:297/508, loss:0.10163417458534241\n",
      "epoch:1/10, batch:298/508, loss:0.11783529818058014\n",
      "epoch:1/10, batch:299/508, loss:0.09636133164167404\n",
      "epoch:1/10, batch:300/508, loss:0.09384574741125107\n",
      "evaluating ....\n",
      "Dev Performance P@5 0.393788819876\n",
      "Dev Performance P@1 0.534161490683\n",
      "Dev Performance MAP 0.6291466861397834\n",
      "Dev Performance MRR 0.6665545893030363\n",
      "------------------\n",
      "Test Performance P@5 0.372327044025\n",
      "Test Performance P@1 0.547169811321\n",
      "Test Performance MAP 0.6377153489493214\n",
      "Test Performance MRR 0.6869080398251204\n",
      "epoch:1/10, batch:301/508, loss:0.14816628396511078\n",
      "epoch:1/10, batch:302/508, loss:0.06777849793434143\n",
      "epoch:1/10, batch:303/508, loss:0.10971201211214066\n",
      "epoch:1/10, batch:304/508, loss:0.12385252863168716\n",
      "epoch:1/10, batch:305/508, loss:0.11284875869750977\n",
      "epoch:1/10, batch:306/508, loss:0.07754956185817719\n",
      "epoch:1/10, batch:307/508, loss:0.07397990673780441\n",
      "epoch:1/10, batch:308/508, loss:0.09293182939291\n",
      "epoch:1/10, batch:309/508, loss:0.10540611296892166\n",
      "epoch:1/10, batch:310/508, loss:0.07447684556245804\n",
      "epoch:1/10, batch:311/508, loss:0.07883291691541672\n",
      "epoch:1/10, batch:312/508, loss:0.10305911302566528\n",
      "epoch:1/10, batch:313/508, loss:0.07122624665498734\n",
      "epoch:1/10, batch:314/508, loss:0.06796037405729294\n",
      "epoch:1/10, batch:315/508, loss:0.10510843992233276\n",
      "epoch:1/10, batch:316/508, loss:0.11927977204322815\n",
      "epoch:1/10, batch:317/508, loss:0.16670529544353485\n",
      "epoch:1/10, batch:318/508, loss:0.10255254060029984\n",
      "epoch:1/10, batch:319/508, loss:0.10608616471290588\n",
      "epoch:1/10, batch:320/508, loss:0.08450935781002045\n",
      "epoch:1/10, batch:321/508, loss:0.08520770072937012\n",
      "epoch:1/10, batch:322/508, loss:0.07506417483091354\n",
      "epoch:1/10, batch:323/508, loss:0.08651882410049438\n",
      "epoch:1/10, batch:324/508, loss:0.113801509141922\n",
      "epoch:1/10, batch:325/508, loss:0.09859250485897064\n",
      "evaluating ....\n",
      "Dev Performance P@5 0.388819875776\n",
      "Dev Performance P@1 0.503105590062\n",
      "Dev Performance MAP 0.6145820400245012\n",
      "Dev Performance MRR 0.6462430624423682\n",
      "------------------\n",
      "Test Performance P@5 0.37106918239\n",
      "Test Performance P@1 0.547169811321\n",
      "Test Performance MAP 0.6372899849623698\n",
      "Test Performance MRR 0.6837746166445273\n",
      "epoch:1/10, batch:326/508, loss:0.07642469555139542\n",
      "epoch:1/10, batch:327/508, loss:0.08500109612941742\n",
      "epoch:1/10, batch:328/508, loss:0.08344951272010803\n",
      "epoch:1/10, batch:329/508, loss:0.10230544954538345\n",
      "epoch:1/10, batch:330/508, loss:0.13010594248771667\n",
      "epoch:1/10, batch:331/508, loss:0.12112222611904144\n",
      "epoch:1/10, batch:332/508, loss:0.10692213475704193\n",
      "epoch:1/10, batch:333/508, loss:0.10823137313127518\n",
      "epoch:1/10, batch:334/508, loss:0.1593254655599594\n",
      "epoch:1/10, batch:335/508, loss:0.07663996517658234\n",
      "epoch:1/10, batch:336/508, loss:0.0775010883808136\n",
      "epoch:1/10, batch:337/508, loss:0.09196067601442337\n",
      "epoch:1/10, batch:338/508, loss:0.0944981500506401\n",
      "epoch:1/10, batch:339/508, loss:0.05747395008802414\n",
      "epoch:1/10, batch:340/508, loss:0.09366963803768158\n",
      "epoch:1/10, batch:341/508, loss:0.10907474160194397\n",
      "epoch:1/10, batch:342/508, loss:0.08733305335044861\n",
      "epoch:1/10, batch:343/508, loss:0.17843356728553772\n",
      "epoch:1/10, batch:344/508, loss:0.08832713961601257\n",
      "epoch:1/10, batch:345/508, loss:0.16761134564876556\n",
      "epoch:1/10, batch:346/508, loss:0.051258787512779236\n",
      "epoch:1/10, batch:347/508, loss:0.09421289712190628\n",
      "epoch:1/10, batch:348/508, loss:0.11243560910224915\n",
      "epoch:1/10, batch:349/508, loss:0.11298510432243347\n",
      "epoch:1/10, batch:350/508, loss:0.13147388398647308\n",
      "evaluating ....\n",
      "Dev Performance P@5 0.393788819876\n",
      "Dev Performance P@1 0.509316770186\n",
      "Dev Performance MAP 0.6154263255526277\n",
      "Dev Performance MRR 0.6556854291326341\n",
      "------------------\n",
      "Test Performance P@5 0.37106918239\n",
      "Test Performance P@1 0.534591194969\n",
      "Test Performance MAP 0.6391548314648258\n",
      "Test Performance MRR 0.6805302657309201\n",
      "epoch:1/10, batch:351/508, loss:0.10716447234153748\n",
      "epoch:1/10, batch:352/508, loss:0.09357216954231262\n",
      "epoch:1/10, batch:353/508, loss:0.08933387696743011\n",
      "epoch:1/10, batch:354/508, loss:0.09180402755737305\n",
      "epoch:1/10, batch:355/508, loss:0.1461373120546341\n",
      "epoch:1/10, batch:356/508, loss:0.1257781982421875\n",
      "epoch:1/10, batch:357/508, loss:0.08339527994394302\n",
      "epoch:1/10, batch:358/508, loss:0.08890422433614731\n",
      "epoch:1/10, batch:359/508, loss:0.09692772477865219\n",
      "epoch:1/10, batch:360/508, loss:0.07889004796743393\n",
      "epoch:1/10, batch:361/508, loss:0.09892306476831436\n",
      "epoch:1/10, batch:362/508, loss:0.06454852223396301\n",
      "epoch:1/10, batch:363/508, loss:0.12997353076934814\n",
      "epoch:1/10, batch:364/508, loss:0.1631273776292801\n",
      "epoch:1/10, batch:365/508, loss:0.08251228928565979\n",
      "epoch:1/10, batch:366/508, loss:0.07014946639537811\n",
      "epoch:1/10, batch:367/508, loss:0.10118184238672256\n",
      "epoch:1/10, batch:368/508, loss:0.07613415271043777\n",
      "epoch:1/10, batch:369/508, loss:0.11859096586704254\n",
      "epoch:1/10, batch:370/508, loss:0.10052816569805145\n",
      "epoch:1/10, batch:371/508, loss:0.1160757765173912\n",
      "epoch:1/10, batch:372/508, loss:0.11541244387626648\n",
      "epoch:1/10, batch:373/508, loss:0.10327842086553574\n",
      "epoch:1/10, batch:374/508, loss:0.12329556792974472\n",
      "epoch:1/10, batch:375/508, loss:0.05470722168684006\n",
      "evaluating ....\n",
      "Dev Performance P@5 0.4\n",
      "Dev Performance P@1 0.496894409938\n",
      "Dev Performance MAP 0.6075673662572748\n",
      "Dev Performance MRR 0.6398255151863118\n",
      "------------------\n",
      "Test Performance P@5 0.369811320755\n",
      "Test Performance P@1 0.559748427673\n",
      "Test Performance MAP 0.6395953633254509\n",
      "Test Performance MRR 0.6937720091890895\n",
      "epoch:1/10, batch:376/508, loss:0.1994979828596115\n",
      "epoch:1/10, batch:377/508, loss:0.08845891058444977\n",
      "epoch:1/10, batch:378/508, loss:0.08019281178712845\n",
      "epoch:1/10, batch:379/508, loss:0.0711992010474205\n",
      "epoch:1/10, batch:380/508, loss:0.10963355004787445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1/10, batch:381/508, loss:0.05859388783574104\n",
      "epoch:1/10, batch:382/508, loss:0.1103350892663002\n",
      "epoch:1/10, batch:383/508, loss:0.08043211698532104\n",
      "epoch:1/10, batch:384/508, loss:0.06245983764529228\n",
      "epoch:1/10, batch:385/508, loss:0.11905223876237869\n",
      "epoch:1/10, batch:386/508, loss:0.07976298034191132\n",
      "epoch:1/10, batch:387/508, loss:0.0702722817659378\n",
      "epoch:1/10, batch:388/508, loss:0.09807973355054855\n",
      "epoch:1/10, batch:389/508, loss:0.13630197942256927\n",
      "epoch:1/10, batch:390/508, loss:0.06581863760948181\n",
      "epoch:1/10, batch:391/508, loss:0.06663747876882553\n",
      "epoch:1/10, batch:392/508, loss:0.08755485713481903\n",
      "epoch:1/10, batch:393/508, loss:0.11267269402742386\n",
      "epoch:1/10, batch:394/508, loss:0.10319242626428604\n",
      "epoch:1/10, batch:395/508, loss:0.17395727336406708\n",
      "epoch:1/10, batch:396/508, loss:0.1303078532218933\n",
      "epoch:1/10, batch:397/508, loss:0.07981802523136139\n",
      "epoch:1/10, batch:398/508, loss:0.07730397582054138\n",
      "epoch:1/10, batch:399/508, loss:0.14149557054042816\n",
      "epoch:1/10, batch:400/508, loss:0.12315766513347626\n",
      "evaluating ....\n",
      "Dev Performance P@5 0.395031055901\n",
      "Dev Performance P@1 0.496894409938\n",
      "Dev Performance MAP 0.6058301246500685\n",
      "Dev Performance MRR 0.6452599875462498\n",
      "------------------\n",
      "Test Performance P@5 0.37106918239\n",
      "Test Performance P@1 0.553459119497\n",
      "Test Performance MAP 0.6387575776758749\n",
      "Test Performance MRR 0.6895438680816038\n",
      "epoch:1/10, batch:401/508, loss:0.11837024986743927\n",
      "epoch:1/10, batch:402/508, loss:0.05888724699616432\n",
      "epoch:1/10, batch:403/508, loss:0.13006164133548737\n",
      "epoch:1/10, batch:404/508, loss:0.1686321347951889\n",
      "epoch:1/10, batch:405/508, loss:0.06937120854854584\n",
      "epoch:1/10, batch:406/508, loss:0.10224567353725433\n",
      "epoch:1/10, batch:407/508, loss:0.05967991426587105\n",
      "epoch:1/10, batch:408/508, loss:0.12808305025100708\n",
      "epoch:1/10, batch:409/508, loss:0.08562085032463074\n",
      "epoch:1/10, batch:410/508, loss:0.11354140937328339\n",
      "epoch:1/10, batch:411/508, loss:0.08273926377296448\n",
      "epoch:1/10, batch:412/508, loss:0.08542955666780472\n",
      "epoch:1/10, batch:413/508, loss:0.08456280082464218\n",
      "epoch:1/10, batch:414/508, loss:0.11509141325950623\n",
      "epoch:1/10, batch:415/508, loss:0.09231362491846085\n",
      "epoch:1/10, batch:416/508, loss:0.11966162174940109\n",
      "epoch:1/10, batch:417/508, loss:0.12701845169067383\n",
      "epoch:1/10, batch:418/508, loss:0.0661483183503151\n",
      "epoch:1/10, batch:419/508, loss:0.11187730729579926\n",
      "epoch:1/10, batch:420/508, loss:0.08338803052902222\n",
      "epoch:1/10, batch:421/508, loss:0.07053614407777786\n",
      "epoch:1/10, batch:422/508, loss:0.0929945558309555\n",
      "epoch:1/10, batch:423/508, loss:0.10476625710725784\n",
      "epoch:1/10, batch:424/508, loss:0.12665851414203644\n",
      "epoch:1/10, batch:425/508, loss:0.08253790438175201\n",
      "evaluating ....\n",
      "Dev Performance P@5 0.39751552795\n",
      "Dev Performance P@1 0.515527950311\n",
      "Dev Performance MAP 0.617642680485363\n",
      "Dev Performance MRR 0.6544663781190999\n",
      "------------------\n",
      "Test Performance P@5 0.367295597484\n",
      "Test Performance P@1 0.509433962264\n",
      "Test Performance MAP 0.6235824862704501\n",
      "Test Performance MRR 0.6637411734978765\n",
      "epoch:1/10, batch:426/508, loss:0.1250309944152832\n",
      "epoch:1/10, batch:427/508, loss:0.10237441956996918\n",
      "epoch:1/10, batch:428/508, loss:0.05496278405189514\n",
      "epoch:1/10, batch:429/508, loss:0.09450282156467438\n",
      "epoch:1/10, batch:430/508, loss:0.10918121784925461\n",
      "epoch:1/10, batch:431/508, loss:0.0900556743144989\n",
      "epoch:1/10, batch:432/508, loss:0.07696489989757538\n",
      "epoch:1/10, batch:433/508, loss:0.13119779527187347\n",
      "epoch:1/10, batch:434/508, loss:0.10728594660758972\n",
      "epoch:1/10, batch:435/508, loss:0.07828196883201599\n",
      "epoch:1/10, batch:436/508, loss:0.13936762511730194\n",
      "epoch:1/10, batch:437/508, loss:0.15792220830917358\n",
      "epoch:1/10, batch:438/508, loss:0.09188902378082275\n",
      "epoch:1/10, batch:439/508, loss:0.08271249383687973\n",
      "epoch:1/10, batch:440/508, loss:0.08671700954437256\n",
      "epoch:1/10, batch:441/508, loss:0.10332031548023224\n",
      "epoch:1/10, batch:442/508, loss:0.16569675505161285\n",
      "epoch:1/10, batch:443/508, loss:0.08391588181257248\n",
      "epoch:1/10, batch:444/508, loss:0.1087866798043251\n",
      "epoch:1/10, batch:445/508, loss:0.12024001032114029\n",
      "epoch:1/10, batch:446/508, loss:0.08655011653900146\n",
      "epoch:1/10, batch:447/508, loss:0.10059767216444016\n",
      "epoch:1/10, batch:448/508, loss:0.07434983551502228\n",
      "epoch:1/10, batch:449/508, loss:0.08601171523332596\n",
      "epoch:1/10, batch:450/508, loss:0.05607173964381218\n",
      "evaluating ....\n",
      "Dev Performance P@5 0.39751552795\n",
      "Dev Performance P@1 0.540372670807\n",
      "Dev Performance MAP 0.6268474810383652\n",
      "Dev Performance MRR 0.6670033296586091\n",
      "------------------\n",
      "Test Performance P@5 0.366037735849\n",
      "Test Performance P@1 0.528301886792\n",
      "Test Performance MAP 0.6365618977431653\n",
      "Test Performance MRR 0.6834649158631283\n",
      "epoch:1/10, batch:451/508, loss:0.10967805981636047\n",
      "epoch:1/10, batch:452/508, loss:0.07550826668739319\n",
      "epoch:1/10, batch:453/508, loss:0.14838644862174988\n",
      "epoch:1/10, batch:454/508, loss:0.09776127338409424\n",
      "epoch:1/10, batch:455/508, loss:0.06876566261053085\n",
      "epoch:1/10, batch:456/508, loss:0.09670577943325043\n",
      "epoch:1/10, batch:457/508, loss:0.08377943187952042\n",
      "epoch:1/10, batch:458/508, loss:0.0904071256518364\n",
      "epoch:1/10, batch:459/508, loss:0.08577993512153625\n",
      "epoch:1/10, batch:460/508, loss:0.14836235344409943\n",
      "epoch:1/10, batch:461/508, loss:0.07486501336097717\n",
      "epoch:1/10, batch:462/508, loss:0.08990495651960373\n",
      "epoch:1/10, batch:463/508, loss:0.11242879927158356\n",
      "epoch:1/10, batch:464/508, loss:0.08045274764299393\n",
      "epoch:1/10, batch:465/508, loss:0.09903677552938461\n",
      "epoch:1/10, batch:466/508, loss:0.10722941160202026\n",
      "epoch:1/10, batch:467/508, loss:0.10600501298904419\n",
      "epoch:1/10, batch:468/508, loss:0.10172662138938904\n",
      "epoch:1/10, batch:469/508, loss:0.09449373185634613\n",
      "epoch:1/10, batch:470/508, loss:0.10592979192733765\n",
      "epoch:1/10, batch:471/508, loss:0.09737037867307663\n",
      "epoch:1/10, batch:472/508, loss:0.1127714216709137\n",
      "epoch:1/10, batch:473/508, loss:0.11091208457946777\n",
      "epoch:1/10, batch:474/508, loss:0.06139582395553589\n",
      "epoch:1/10, batch:475/508, loss:0.06611786037683487\n",
      "evaluating ....\n",
      "Dev Performance P@5 0.398757763975\n",
      "Dev Performance P@1 0.534161490683\n",
      "Dev Performance MAP 0.6148099259207116\n",
      "Dev Performance MRR 0.6605567422063586\n",
      "------------------\n",
      "Test Performance P@5 0.372327044025\n",
      "Test Performance P@1 0.547169811321\n",
      "Test Performance MAP 0.6362077158055849\n",
      "Test Performance MRR 0.6891670284917555\n",
      "epoch:1/10, batch:476/508, loss:0.09776248037815094\n",
      "epoch:1/10, batch:477/508, loss:0.11659985780715942\n",
      "epoch:1/10, batch:478/508, loss:0.09308762848377228\n",
      "epoch:1/10, batch:479/508, loss:0.07372993230819702\n",
      "epoch:1/10, batch:480/508, loss:0.06303109228610992\n",
      "epoch:1/10, batch:481/508, loss:0.08141070604324341\n",
      "epoch:1/10, batch:482/508, loss:0.16243454813957214\n",
      "epoch:1/10, batch:483/508, loss:0.1456579864025116\n",
      "epoch:1/10, batch:484/508, loss:0.057357266545295715\n",
      "epoch:1/10, batch:485/508, loss:0.03755265846848488\n",
      "epoch:1/10, batch:486/508, loss:0.08830154687166214\n",
      "epoch:1/10, batch:487/508, loss:0.0937575101852417\n",
      "epoch:1/10, batch:488/508, loss:0.1548076868057251\n",
      "epoch:1/10, batch:489/508, loss:0.10728415846824646\n",
      "epoch:1/10, batch:490/508, loss:0.11865005642175674\n",
      "epoch:1/10, batch:491/508, loss:0.09125390648841858\n",
      "epoch:1/10, batch:492/508, loss:0.07812958210706711\n",
      "epoch:1/10, batch:493/508, loss:0.08429903537034988\n",
      "epoch:1/10, batch:494/508, loss:0.08683633804321289\n",
      "epoch:1/10, batch:495/508, loss:0.10924214124679565\n",
      "epoch:1/10, batch:496/508, loss:0.13982021808624268\n",
      "epoch:1/10, batch:497/508, loss:0.08618638664484024\n",
      "epoch:1/10, batch:498/508, loss:0.0685567855834961\n",
      "epoch:1/10, batch:499/508, loss:0.10091830790042877\n",
      "epoch:1/10, batch:500/508, loss:0.10691659152507782\n",
      "evaluating ....\n",
      "Dev Performance P@5 0.4\n",
      "Dev Performance P@1 0.515527950311\n",
      "Dev Performance MAP 0.6191719875684701\n",
      "Dev Performance MRR 0.6569621914844785\n",
      "------------------\n",
      "Test Performance P@5 0.369811320755\n",
      "Test Performance P@1 0.522012578616\n",
      "Test Performance MAP 0.6312904534290196\n",
      "Test Performance MRR 0.6747322613757569\n",
      "epoch:1/10, batch:501/508, loss:0.05582166463136673\n",
      "epoch:1/10, batch:502/508, loss:0.13156992197036743\n",
      "epoch:1/10, batch:503/508, loss:0.09172121435403824\n",
      "epoch:1/10, batch:504/508, loss:0.08156996965408325\n",
      "epoch:1/10, batch:505/508, loss:0.15214143693447113\n",
      "epoch:1/10, batch:506/508, loss:0.09394927322864532\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1/10, batch:507/508, loss:0.08553814888000488\n",
      "epoch:1/10, batch:508/508, loss:0.06603405624628067\n",
      "epoch:2/10, batch:1/508, loss:0.0494852289557457\n",
      "epoch:2/10, batch:2/508, loss:0.09168995916843414\n",
      "epoch:2/10, batch:3/508, loss:0.1366259753704071\n",
      "epoch:2/10, batch:4/508, loss:0.06433810293674469\n",
      "epoch:2/10, batch:5/508, loss:0.11617773026227951\n",
      "epoch:2/10, batch:6/508, loss:0.10645077377557755\n",
      "epoch:2/10, batch:7/508, loss:0.06478830426931381\n",
      "epoch:2/10, batch:8/508, loss:0.057500630617141724\n",
      "epoch:2/10, batch:9/508, loss:0.09177706390619278\n",
      "epoch:2/10, batch:10/508, loss:0.05528249964118004\n",
      "epoch:2/10, batch:11/508, loss:0.08676209300756454\n",
      "epoch:2/10, batch:12/508, loss:0.13217806816101074\n",
      "epoch:2/10, batch:13/508, loss:0.039482150226831436\n",
      "epoch:2/10, batch:14/508, loss:0.08643946796655655\n",
      "epoch:2/10, batch:15/508, loss:0.06677962094545364\n",
      "epoch:2/10, batch:16/508, loss:0.06086498126387596\n",
      "epoch:2/10, batch:17/508, loss:0.09507714956998825\n",
      "epoch:2/10, batch:18/508, loss:0.11380793899297714\n",
      "epoch:2/10, batch:19/508, loss:0.04666818305850029\n",
      "epoch:2/10, batch:20/508, loss:0.057266030460596085\n",
      "epoch:2/10, batch:21/508, loss:0.0672847330570221\n",
      "epoch:2/10, batch:22/508, loss:0.07443054765462875\n",
      "epoch:2/10, batch:23/508, loss:0.0779985561966896\n",
      "epoch:2/10, batch:24/508, loss:0.08234412223100662\n",
      "epoch:2/10, batch:25/508, loss:0.06520631164312363\n",
      "evaluating ....\n",
      "Dev Performance P@5 0.398757763975\n",
      "Dev Performance P@1 0.540372670807\n",
      "Dev Performance MAP 0.6244728072223983\n",
      "Dev Performance MRR 0.6644885221748575\n",
      "------------------\n",
      "Test Performance P@5 0.372327044025\n",
      "Test Performance P@1 0.509433962264\n",
      "Test Performance MAP 0.6260328976371611\n",
      "Test Performance MRR 0.6714494211515064\n",
      "epoch:2/10, batch:26/508, loss:0.0807989090681076\n",
      "epoch:2/10, batch:27/508, loss:0.11255399882793427\n",
      "epoch:2/10, batch:28/508, loss:0.09776899963617325\n",
      "epoch:2/10, batch:29/508, loss:0.07382635027170181\n",
      "epoch:2/10, batch:30/508, loss:0.11887882649898529\n",
      "epoch:2/10, batch:31/508, loss:0.09537364542484283\n",
      "epoch:2/10, batch:32/508, loss:0.12578260898590088\n",
      "epoch:2/10, batch:33/508, loss:0.09201749414205551\n",
      "epoch:2/10, batch:34/508, loss:0.08207583427429199\n",
      "epoch:2/10, batch:35/508, loss:0.08587627112865448\n",
      "epoch:2/10, batch:36/508, loss:0.10125857591629028\n",
      "epoch:2/10, batch:37/508, loss:0.0720670223236084\n",
      "epoch:2/10, batch:38/508, loss:0.0940249115228653\n",
      "epoch:2/10, batch:39/508, loss:0.06687455624341965\n",
      "epoch:2/10, batch:40/508, loss:0.06111526116728783\n",
      "epoch:2/10, batch:41/508, loss:0.07293219119310379\n",
      "epoch:2/10, batch:42/508, loss:0.10222618281841278\n",
      "epoch:2/10, batch:43/508, loss:0.10493946820497513\n",
      "epoch:2/10, batch:44/508, loss:0.0833999440073967\n",
      "epoch:2/10, batch:45/508, loss:0.06425470113754272\n",
      "epoch:2/10, batch:46/508, loss:0.07838782668113708\n",
      "epoch:2/10, batch:47/508, loss:0.115348681807518\n",
      "epoch:2/10, batch:48/508, loss:0.11875244230031967\n",
      "epoch:2/10, batch:49/508, loss:0.08406629413366318\n",
      "epoch:2/10, batch:50/508, loss:0.05369865149259567\n",
      "evaluating ....\n",
      "Dev Performance P@5 0.39751552795\n",
      "Dev Performance P@1 0.527950310559\n",
      "Dev Performance MAP 0.6238889898825349\n",
      "Dev Performance MRR 0.66053357443109\n",
      "------------------\n",
      "Test Performance P@5 0.37358490566\n",
      "Test Performance P@1 0.540880503145\n",
      "Test Performance MAP 0.6364376562801574\n",
      "Test Performance MRR 0.6910613899690363\n",
      "epoch:2/10, batch:51/508, loss:0.05779200419783592\n",
      "epoch:2/10, batch:52/508, loss:0.037133775651454926\n",
      "epoch:2/10, batch:53/508, loss:0.09366869926452637\n",
      "epoch:2/10, batch:54/508, loss:0.074076808989048\n",
      "epoch:2/10, batch:55/508, loss:0.07186826318502426\n",
      "epoch:2/10, batch:56/508, loss:0.04781918227672577\n",
      "epoch:2/10, batch:57/508, loss:0.11173315346240997\n",
      "epoch:2/10, batch:58/508, loss:0.08209291100502014\n",
      "epoch:2/10, batch:59/508, loss:0.10264778882265091\n",
      "epoch:2/10, batch:60/508, loss:0.08833697438240051\n",
      "epoch:2/10, batch:61/508, loss:0.0942598357796669\n",
      "epoch:2/10, batch:62/508, loss:0.07479603588581085\n",
      "epoch:2/10, batch:63/508, loss:0.09394000470638275\n",
      "epoch:2/10, batch:64/508, loss:0.12235111743211746\n",
      "epoch:2/10, batch:65/508, loss:0.09793756157159805\n",
      "epoch:2/10, batch:66/508, loss:0.0790124386548996\n",
      "epoch:2/10, batch:67/508, loss:0.07406698167324066\n",
      "epoch:2/10, batch:68/508, loss:0.07375633716583252\n",
      "epoch:2/10, batch:69/508, loss:0.05559702217578888\n",
      "epoch:2/10, batch:70/508, loss:0.06789081543684006\n",
      "epoch:2/10, batch:71/508, loss:0.0990455225110054\n",
      "epoch:2/10, batch:72/508, loss:0.07365027070045471\n",
      "epoch:2/10, batch:73/508, loss:0.07880620658397675\n",
      "epoch:2/10, batch:74/508, loss:0.09211309999227524\n",
      "epoch:2/10, batch:75/508, loss:0.09839669615030289\n",
      "evaluating ....\n",
      "Dev Performance P@5 0.396273291925\n",
      "Dev Performance P@1 0.55900621118\n",
      "Dev Performance MAP 0.6377799035636932\n",
      "Dev Performance MRR 0.674984225472899\n",
      "------------------\n",
      "Test Performance P@5 0.378616352201\n",
      "Test Performance P@1 0.547169811321\n",
      "Test Performance MAP 0.639069097972119\n",
      "Test Performance MRR 0.6886445774976064\n",
      "epoch:2/10, batch:76/508, loss:0.11905147135257721\n",
      "epoch:2/10, batch:77/508, loss:0.14102624356746674\n",
      "epoch:2/10, batch:78/508, loss:0.07363811135292053\n",
      "epoch:2/10, batch:79/508, loss:0.11119291931390762\n",
      "epoch:2/10, batch:80/508, loss:0.10394217073917389\n",
      "epoch:2/10, batch:81/508, loss:0.06397148966789246\n",
      "epoch:2/10, batch:82/508, loss:0.06368187814950943\n",
      "epoch:2/10, batch:83/508, loss:0.10095935314893723\n",
      "epoch:2/10, batch:84/508, loss:0.09607250988483429\n",
      "epoch:2/10, batch:85/508, loss:0.04952017962932587\n"
     ]
    }
   ],
   "source": [
    "model_margin_p3 = EmbeddingLayer(200, HIDDEN_DIM, 'lstm')\n",
    "train('lstm', model, batch_size=25, num_epoch=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qids = list(train_idx_set.keys())[:25]\n",
    "t, b, tl, bl = process_contxt_batch(batch_x_qids, train_idx_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_layer = EmbeddingLayer(200, 240, 'lstm')\n",
    "embedding_layer.title_hidden = embedding_layer.init_hidden(t.shape[1])\n",
    "embedding_layer.body_hidden = embedding_layer.init_hidden(b.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "title_qs = Variable(torch.FloatTensor(t))\n",
    "body_qs = Variable(torch.FloatTensor(b))\n",
    "embeddings = embedding_layer(title_qs, body_qs, tl, bl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target = Variable(torch.LongTensor([1,1] + [0]*20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = torch.nn.MultiMarginLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 1.0260\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(embeddings[:22], target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "blocked_embeddings = embeddings.view(-1, 22, 240)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q_vecs = blocked_embeddings[:,0,:]\n",
    "pos_vecs = blocked_embeddings[:,1,:]\n",
    "neg_vecs = blocked_embeddings[:,2:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_scores = torch.sum(q_vecs * pos_vecs, dim=1) / (torch.sqrt(torch.sum(q_vecs ** 2, dim=1)) \\\n",
    "                                    * torch.sqrt(torch.sum(pos_vecs ** 2, dim=1)))\n",
    "neg_scores = torch.sum(torch.unsqueeze(q_vecs, dim=1) * neg_vecs, dim=2) \\\n",
    "/ (torch.unsqueeze(torch.sqrt(torch.sum(q_vecs ** 2, dim=1)), dim=1) * torch.sqrt(torch.sum( neg_vecs ** 2, dim=2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Variable containing:\n",
       "  0.9956\n",
       " [torch.FloatTensor of size 1], Variable containing:\n",
       "  0.9919\n",
       "  0.9907\n",
       "  0.9932\n",
       "  0.9865\n",
       "  0.9866\n",
       "  0.9890\n",
       "  0.9923\n",
       "  0.9835\n",
       "  0.9876\n",
       "  0.9871\n",
       "  0.9884\n",
       "  0.9846\n",
       "  0.9868\n",
       "  0.9855\n",
       "  0.9879\n",
       "  0.9884\n",
       "  0.9886\n",
       "  0.9864\n",
       "  0.9892\n",
       "  0.9893\n",
       " [torch.FloatTensor of size 20])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_scores[0], neg_scores[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q_vecs = blocked_embeddings[:,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pn_vecs = blocked_embeddings[:,1:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores = torch.sum(torch.unsqueeze(q_vecs, dim=1) * pn_vecs, dim=2) \\\n",
    "/ (torch.unsqueeze(torch.sqrt(torch.sum(q_vecs ** 2, dim=1)), dim=1) * torch.sqrt(torch.sum( pn_vecs ** 2, dim=2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "\n",
       "Columns 0 to 9 \n",
       " 0.9956  0.9919  0.9907  0.9932  0.9865  0.9866  0.9890  0.9923  0.9835  0.9876\n",
       " 0.9903  0.9830  0.9841  0.9789  0.9809  0.9809  0.9718  0.9705  0.9728  0.9721\n",
       " 0.9930  0.9897  0.9855  0.9897  0.9890  0.9820  0.9798  0.9854  0.9760  0.9834\n",
       " 0.9867  0.9819  0.9823  0.9876  0.9819  0.9851  0.9798  0.9759  0.9894  0.9874\n",
       " 0.9920  0.9864  0.9870  0.9819  0.9849  0.9806  0.9822  0.9809  0.9870  0.9651\n",
       " 0.9929  0.9910  0.9900  0.9828  0.9876  0.9922  0.9914  0.9906  0.9916  0.9895\n",
       " 0.9964  0.9934  0.9812  0.9916  0.9866  0.9881  0.9790  0.9898  0.9876  0.9864\n",
       " 0.9941  0.9806  0.9755  0.9790  0.9821  0.9760  0.9773  0.9871  0.9782  0.9830\n",
       " 0.9853  0.9871  0.9801  0.9824  0.9804  0.9677  0.9870  0.9842  0.9805  0.9844\n",
       " 0.9833  0.9861  0.9755  0.9871  0.9849  0.9815  0.9762  0.9896  0.9809  0.9874\n",
       " 0.9978  0.9925  0.9840  0.9818  0.9911  0.9924  0.9908  0.9857  0.9918  0.9911\n",
       " 0.9929  0.9882  0.9847  0.9853  0.9857  0.9858  0.9930  0.9908  0.9923  0.9881\n",
       " 0.9881  0.9908  0.9887  0.9878  0.9877  0.9852  0.9876  0.9875  0.9878  0.9873\n",
       " 0.9941  0.9718  0.9880  0.9866  0.9894  0.9810  0.9894  0.9928  0.9832  0.9922\n",
       " 0.9857  0.9832  0.9874  0.9907  0.9878  0.9881  0.9878  0.9877  0.9826  0.9878\n",
       " 0.9917  0.9776  0.9906  0.9915  0.9870  0.9858  0.9824  0.9867  0.9892  0.9889\n",
       " 0.9926  0.9806  0.9822  0.9721  0.9717  0.9754  0.9817  0.9772  0.9818  0.9768\n",
       " 0.9888  0.9916  0.9767  0.9896  0.9891  0.9891  0.9799  0.9851  0.9910  0.9912\n",
       " 0.9904  0.9853  0.9862  0.9824  0.9775  0.9788  0.9832  0.9874  0.9832  0.9833\n",
       " 0.9882  0.9857  0.9847  0.9908  0.9834  0.9855  0.9839  0.9839  0.9882  0.9851\n",
       " 0.9869  0.9797  0.9802  0.9834  0.9892  0.9775  0.9922  0.9856  0.9932  0.9829\n",
       " 0.9936  0.9886  0.9883  0.9793  0.9771  0.9819  0.9905  0.9857  0.9895  0.9874\n",
       " 0.9854  0.9784  0.9733  0.9772  0.9780  0.9729  0.9887  0.9831  0.9861  0.9847\n",
       " 0.9885  0.9808  0.9821  0.9888  0.9886  0.9861  0.9833  0.9899  0.9807  0.9873\n",
       " 0.9842  0.9899  0.9851  0.9729  0.9902  0.9812  0.9837  0.9799  0.9812  0.9857\n",
       " 0.9853  0.9888  0.9779  0.9866  0.9866  0.9916  0.9811  0.9771  0.9847  0.9886\n",
       " 0.9909  0.9790  0.9820  0.9906  0.9866  0.9908  0.9673  0.9829  0.9872  0.9835\n",
       " 0.9866  0.9818  0.9803  0.9812  0.9809  0.9847  0.9906  0.9926  0.9909  0.9745\n",
       " 0.9815  0.9877  0.9837  0.9788  0.9866  0.9877  0.9877  0.9812  0.9913  0.9811\n",
       " 0.9858  0.9673  0.9847  0.9902  0.9821  0.9809  0.9851  0.9812  0.9890  0.9866\n",
       " 0.9911  0.9729  0.9885  0.9861  0.9897  0.9782  0.9837  0.9857  0.9812  0.9745\n",
       " 0.9903  0.9838  0.9899  0.9748  0.9886  0.9713  0.9820  0.9838  0.9818  0.9748\n",
       " 0.9882  0.9818  0.9872  0.9782  0.9812  0.9861  0.9909  0.9908  0.9873  0.9916\n",
       " 0.9842  0.9752  0.9680  0.9735  0.9854  0.9790  0.9808  0.9716  0.9726  0.9759\n",
       " 0.9915  0.9843  0.9775  0.9790  0.9812  0.9893  0.9862  0.9866  0.9866  0.9809\n",
       " 0.9950  0.9826  0.9882  0.9827  0.9793  0.9836  0.9804  0.9810  0.9793  0.9884\n",
       " 0.9865  0.9839  0.9710  0.9776  0.9841  0.9857  0.9797  0.9875  0.9776  0.9736\n",
       " 0.9956  0.9832  0.9713  0.9912  0.9799  0.9856  0.9938  0.9798  0.9867  0.9828\n",
       " 0.9914  0.9853  0.9910  0.9896  0.9877  0.9839  0.9824  0.9932  0.9916  0.9873\n",
       " 0.9823  0.9819  0.9822  0.9857  0.9762  0.9808  0.9783  0.9827  0.9859  0.9828\n",
       " 0.9927  0.9923  0.9855  0.9806  0.9878  0.9864  0.9898  0.9923  0.9790  0.9872\n",
       "\n",
       "Columns 10 to 19 \n",
       " 0.9871  0.9884  0.9846  0.9868  0.9855  0.9879  0.9884  0.9886  0.9864  0.9892\n",
       " 0.9819  0.9744  0.9797  0.9801  0.9940  0.9867  0.9652  0.9769  0.9825  0.9850\n",
       " 0.9878  0.9796  0.9871  0.9772  0.9796  0.9794  0.9899  0.9790  0.9796  0.9872\n",
       " 0.9782  0.9870  0.9811  0.9772  0.9798  0.9794  0.9897  0.9845  0.9764  0.9835\n",
       " 0.9838  0.9855  0.9809  0.9844  0.9840  0.9851  0.9771  0.9920  0.9783  0.9895\n",
       " 0.9837  0.9914  0.9886  0.9836  0.9745  0.9870  0.9915  0.9818  0.9917  0.9907\n",
       " 0.9838  0.9906  0.9919  0.9866  0.9802  0.9887  0.9832  0.9741  0.9787  0.9827\n",
       " 0.9801  0.9771  0.9771  0.9769  0.9858  0.9782  0.9729  0.9769  0.9754  0.9768\n",
       " 0.9775  0.9831  0.9808  0.9747  0.9785  0.9808  0.9747  0.9750  0.9770  0.9747\n",
       " 0.9708  0.9786  0.9779  0.9766  0.9832  0.9849  0.9662  0.9838  0.9839  0.9799\n",
       " 0.9840  0.9930  0.9871  0.9818  0.9883  0.9901  0.9910  0.9869  0.9840  0.9929\n",
       " 0.9833  0.9882  0.9694  0.9882  0.9863  0.9885  0.9847  0.9882  0.9899  0.9887\n",
       " 0.9866  0.9922  0.9929  0.9913  0.9886  0.9790  0.9852  0.9861  0.9866  0.9813\n",
       " 0.9903  0.9848  0.9848  0.9918  0.9911  0.9826  0.9878  0.9853  0.9894  0.9908\n",
       " 0.9877  0.9877  0.9832  0.9884  0.9902  0.9846  0.9853  0.9735  0.9873  0.9928\n",
       " 0.9951  0.9906  0.9824  0.9828  0.9926  0.9914  0.9903  0.9920  0.9873  0.9920\n",
       " 0.9730  0.9798  0.9780  0.9706  0.9792  0.9819  0.9763  0.9794  0.9730  0.9782\n",
       " 0.9893  0.9862  0.9876  0.9874  0.9871  0.9847  0.9877  0.9846  0.9828  0.9905\n",
       " 0.9877  0.9872  0.9853  0.9877  0.9884  0.9809  0.9872  0.9815  0.9872  0.9804\n",
       " 0.9839  0.9872  0.9825  0.9908  0.9786  0.9771  0.9874  0.9797  0.9837  0.9830\n",
       " 0.9879  0.9897  0.9811  0.9908  0.9874  0.9851  0.9860  0.9893  0.9860  0.9932\n",
       " 0.9886  0.9838  0.9859  0.9814  0.9786  0.9895  0.9874  0.9852  0.9893  0.9882\n",
       " 0.9821  0.9877  0.9812  0.9771  0.9812  0.9837  0.9779  0.9793  0.9810  0.9917\n",
       " 0.9873  0.9918  0.9713  0.9916  0.9897  0.9866  0.9810  0.9835  0.9890  0.9916\n",
       " 0.9851  0.9866  0.9917  0.9755  0.9706  0.9873  0.9835  0.9877  0.9877  0.9868\n",
       " 0.9830  0.9906  0.9713  0.9896  0.9916  0.9830  0.9814  0.9889  0.9745  0.9814\n",
       " 0.9713  0.9861  0.9873  0.9830  0.9820  0.9917  0.9807  0.9888  0.9783  0.9772\n",
       " 0.9812  0.9892  0.9897  0.9713  0.9812  0.9913  0.9871  0.9886  0.9812  0.9851\n",
       " 0.9812  0.9790  0.9851  0.9837  0.9888  0.9748  0.9873  0.9818  0.9818  0.9713\n",
       " 0.9829  0.9816  0.9888  0.9729  0.9868  0.9811  0.9902  0.9818  0.9808  0.9888\n",
       " 0.9888  0.9908  0.9847  0.9863  0.9909  0.9837  0.9771  0.9906  0.9799  0.9812\n",
       " 0.9887  0.9873  0.9783  0.9811  0.9812  0.9801  0.9811  0.9771  0.9748  0.9706\n",
       " 0.9908  0.9916  0.9866  0.9866  0.9908  0.9863  0.9866  0.9808  0.9896  0.9918\n",
       " 0.9761  0.9820  0.9822  0.9761  0.9791  0.9707  0.9766  0.9792  0.9765  0.9762\n",
       " 0.9811  0.9847  0.9808  0.9755  0.9826  0.9705  0.9848  0.9836  0.9729  0.9925\n",
       " 0.9864  0.9729  0.9867  0.9867  0.9884  0.9794  0.9916  0.9790  0.9508  0.9803\n",
       " 0.9852  0.9787  0.9764  0.9864  0.9854  0.9787  0.9829  0.9852  0.9796  0.9826\n",
       " 0.9909  0.9730  0.9743  0.9867  0.9805  0.9874  0.9791  0.9906  0.9879  0.9912\n",
       " 0.9787  0.9899  0.9865  0.9856  0.9896  0.9813  0.9894  0.9865  0.9866  0.9799\n",
       " 0.9860  0.9810  0.9830  0.9824  0.9800  0.9848  0.9840  0.9824  0.9783  0.9868\n",
       " 0.9854  0.9846  0.9893  0.9916  0.9903  0.9870  0.9891  0.9857  0.9876  0.9790\n",
       "\n",
       "Columns 20 to 20 \n",
       " 0.9893\n",
       " 0.9780\n",
       " 0.9779\n",
       " 0.9760\n",
       " 0.9915\n",
       " 0.9877\n",
       " 0.9893\n",
       " 0.9798\n",
       " 0.9782\n",
       " 0.9813\n",
       " 0.9809\n",
       " 0.9861\n",
       " 0.9926\n",
       " 0.9932\n",
       " 0.9898\n",
       " 0.9881\n",
       " 0.9736\n",
       " 0.9881\n",
       " 0.9860\n",
       " 0.9786\n",
       " 0.9795\n",
       " 0.9847\n",
       " 0.9808\n",
       " 0.9829\n",
       " 0.9875\n",
       " 0.9809\n",
       " 0.9847\n",
       " 0.9877\n",
       " 0.9793\n",
       " 0.9830\n",
       " 0.9877\n",
       " 0.9855\n",
       " 0.9899\n",
       " 0.9808\n",
       " 0.9878\n",
       " 0.9775\n",
       " 0.9840\n",
       " 0.9853\n",
       " 0.9917\n",
       " 0.9762\n",
       " 0.9885\n",
       "[torch.FloatTensor of size 41x21]"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = torch.nn.MultiMarginLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.9469\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = Variable(torch.zeros(scores.size(0)).type(torch.LongTensor)) \n",
    "criterion(scores, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
