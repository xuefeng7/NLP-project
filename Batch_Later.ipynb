{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import string\n",
    "import numpy as np; np.random.seed(7)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_map = {}\n",
    "with open('data/vectors_pruned.200.txt', 'r') as src:\n",
    "    src = src.read().strip().split('\\n')\n",
    "    for line in src:\n",
    "        wv = line.strip().split(' ')\n",
    "        word = wv.pop(0)\n",
    "        w2v_map[word] = np.array(list(map(float, wv)))\n",
    "\n",
    "w2i_map = {}\n",
    "w2v_matrix = np.zeros(( len((w2v_map.keys())), 200 ))\n",
    "for i, (key, val) in enumerate(w2v_map.items()):\n",
    "    w2i_map[key] = i\n",
    "    w2v_matrix[i] = val\n",
    "\n",
    "def w2v(w):\n",
    "    return w2v_matrix[w2i_map[w]]\n",
    "\n",
    "def sen2w(sen):\n",
    "    processed = []\n",
    "    sen = sen.strip().split()\n",
    "    if len(sen) > 100:\n",
    "        sen = sen[:100]\n",
    "    for w in sen:\n",
    "        #ignore date\n",
    "        if re.match(r'\\d{1,}-\\d{1,}-\\d{1,}', w):\n",
    "            continue\n",
    "        if re.match(r'\\d{1,}:\\d{1,}', w):\n",
    "            continue\n",
    "        \n",
    "        if w in w2i_map:\n",
    "            processed += [w]\n",
    "        else:\n",
    "            separated = re.findall(r\"[^\\W\\d_]+|\\d+|[=`%$\\^\\-@;\\[&_*>\\].<~|+\\d+]\", w)\n",
    "            if len(set(separated)) == 1:\n",
    "                continue\n",
    "            if separated.count('*') > 3 or separated.count('=') > 3:\n",
    "                continue\n",
    "            for separate_w in separated:\n",
    "                if separate_w in w2i_map:\n",
    "                    processed += [separate_w]\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fixed context len = 125\n",
    "context_repre = {}\n",
    "with open('data/text_tokenized.txt', 'r') as src:\n",
    "    src = src.read().strip().split('\\n')\n",
    "    for line in src:\n",
    "        context = line.strip().split('\\t')\n",
    "        qid = context.pop(0)\n",
    "        if len(context) == 1:\n",
    "            context_repre[int(qid)] = {'t': sen2w(context[0]), 'b': None}\n",
    "        else:\n",
    "            context_repre[int(qid)] = {'t':sen2w(context[0]), 'b': sen2w(context[1])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_set_pair_with_idx(df):\n",
    "    idx_set = {}\n",
    "    for idx, row in df.iterrows():\n",
    "        idx_set[row['Q']] = {'pos': np.array(list(map(int, row['Q+'].split(' ')))), \\\n",
    "                             'neg': np.array(list(map(int, row['Q-'].split(' '))))}\n",
    "    return idx_set\n",
    "\n",
    "train_df = pd.read_csv('data/train_random.txt', header=None, delimiter='\\t', names=['Q','Q+','Q-'])\n",
    "train_idx_set = build_set_pair_with_idx(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>Q+</th>\n",
       "      <th>Q-</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>262144</td>\n",
       "      <td>211039</td>\n",
       "      <td>227387 413633 113297 356390 256881 145638 2962...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>491522</td>\n",
       "      <td>65911</td>\n",
       "      <td>155119 402211 310669 383107 131731 299465 1633...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>240299</td>\n",
       "      <td>168608 390642</td>\n",
       "      <td>368007 70009 48077 376760 438005 228888 142340...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>196614</td>\n",
       "      <td>205184</td>\n",
       "      <td>334471 163710 376791 441664 159963 406360 4300...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>360457</td>\n",
       "      <td>321532</td>\n",
       "      <td>151863 501857 217578 470017 125838 31836 42066...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Q             Q+                                                 Q-\n",
       "0  262144         211039  227387 413633 113297 356390 256881 145638 2962...\n",
       "1  491522          65911  155119 402211 310669 383107 131731 299465 1633...\n",
       "2  240299  168608 390642  368007 70009 48077 376760 438005 228888 142340...\n",
       "3  196614         205184  334471 163710 376791 441664 159963 406360 4300...\n",
       "4  360457         321532  151863 501857 217578 470017 125838 31836 42066..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def contxt2vec(title, body=None):\n",
    "    \n",
    "    if body == None:\n",
    "        body = []\n",
    "    \n",
    "    title_v = np.zeros( (len(title), 200) )\n",
    "    \n",
    "    for i, t in enumerate(title):\n",
    "        title_v[i] = w2v(t)\n",
    "    \n",
    "    if len(body) > 0:\n",
    "        body_v = np.zeros( (len(body), 200) )\n",
    "        for i, b in enumerate(body):\n",
    "            body_v[i] = w2v(b)\n",
    "    \n",
    "        return title_v, body_v\n",
    "    \n",
    "    return title_v, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_contxt_batch(qids, idx_set):\n",
    "    \n",
    "    batch_title, batch_body = [], []\n",
    "    max_title_len, max_body_len = 0, 0\n",
    "    title_len, body_len = [], []\n",
    "    counter = 0\n",
    "#     y = []\n",
    "    \n",
    "    for qid in qids:\n",
    "        \n",
    "        q_title, q_body = context_repre[qid]['t'], context_repre[qid]['b']\n",
    "        q_pos = idx_set[qid]['pos']\n",
    "\n",
    "        for qid_pos in q_pos:\n",
    "\n",
    "            # query Q\n",
    "            title_len += [len(q_title)]\n",
    "            batch_title += [ q_title ]\n",
    "            max_title_len = max(max_title_len, len(q_title))\n",
    "            if not q_body:\n",
    "                body_len += [len(q_title)]\n",
    "                batch_body += [ q_title ]\n",
    "            else:\n",
    "                batch_body += [ q_body ]\n",
    "                body_len += [len(q_body)]\n",
    "                max_body_len = max(max_body_len, len(q_body))\n",
    "#             y += [1]\n",
    "            # pos Q\n",
    "            title, body = context_repre[qid_pos]['t'], context_repre[qid_pos]['b']\n",
    "            title_len += [len(title)]\n",
    "            batch_title += [ title ]\n",
    "            max_title_len = max(max_title_len, len(title))\n",
    "            if not body:\n",
    "                body_len += [len(title)]\n",
    "                batch_body += [ title ]\n",
    "            else:\n",
    "                batch_body += [ body ]\n",
    "                body_len += [len(body)]\n",
    "                max_body_len = max(max_body_len, len(body))\n",
    "#             y += [1]\n",
    "            # neg Q\n",
    "            q_neg = idx_set[qid]['neg']\n",
    "            q_neg_sample_indices = np.random.choice(range(100), size=20)\n",
    "            q_random_neg = q_neg[q_neg_sample_indices]\n",
    "            \n",
    "            for qid_neg in q_random_neg:\n",
    "                title, body = context_repre[qid_neg]['t'], context_repre[qid_neg]['b']\n",
    "                title_len += [len(title)]\n",
    "                batch_title += [ title ]\n",
    "                max_title_len = max(max_title_len, len(title))\n",
    "                if not body:\n",
    "                    body_len += [len(title)]\n",
    "                    batch_body += [ title ]\n",
    "                else:\n",
    "                    batch_body += [ body ]\n",
    "                    body_len += [len(body)]\n",
    "                    max_body_len = max(max_body_len, len(body))\n",
    "#                 y += [0]\n",
    "    # (max_seq_len, batch_size, feature_len)\n",
    "    padded_batch_title = np.zeros(( max_title_len, len(batch_title), 200)) \n",
    "    padded_batch_body = np.zeros(( max_body_len, len(batch_body),  200))\n",
    "    \n",
    "    for i, (title, body) in enumerate(zip(batch_title, batch_body)):\n",
    "        title_repre, body_repre = contxt2vec(title, body)\n",
    "        padded_batch_title[:title_len[i], i] = title_repre\n",
    "        padded_batch_body[:body_len[i], i] = body_repre\n",
    "    #np.array(y).reshape(-1,1)\n",
    "    return padded_batch_title, padded_batch_body, \\\n",
    "                np.array(title_len).reshape(-1,1), np.array(body_len).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_mask(seq_len):\n",
    "    mask = []\n",
    "    for i, s in enumerate(seq_len):\n",
    "        s_mask = np.zeros((np.max(seq_len), 1))\n",
    "        s_mask[:int(s)] = np.ones((int(s), 1))\n",
    "        mask += [s_mask]\n",
    "    return mask\n",
    "\n",
    "def build_mask3d(seq_len):\n",
    "    mask = np.zeros((np.max(seq_len), len(seq_len), 1))\n",
    "    for i, s in enumerate(seq_len):\n",
    "        mask[:int(s), i] = np.ones((int(s), 1))\n",
    "    return mask\n",
    "\n",
    "def criterion(embeddings):\n",
    "    \n",
    "    # a batch of embeddings\n",
    "#     num_block = embeddings.size()[0] // 22\n",
    "    blocked_embeddings = embeddings.view(-1, 22, 240)\n",
    "    q_vecs = blocked_embeddings[:,0,:]\n",
    "    pos_vecs = blocked_embeddings[:,1,:]\n",
    "    neg_vecs = blocked_embeddings[:,2:,:]\n",
    "    \n",
    "    pos_scores = torch.sum(q_vecs * pos_vecs, dim=1) / (torch.sqrt(torch.sum(q_vecs ** 2, dim=1)) \\\n",
    "                                               * torch.sqrt(torch.sum(pos_vecs ** 2, dim=1)))\n",
    "\n",
    "    neg_scores = torch.sum(torch.unsqueeze(q_vecs, dim=1) * neg_vecs, dim=2) \\\n",
    "    / (torch.unsqueeze(torch.sqrt(torch.sum(q_vecs ** 2, dim=1)),dim=1) * torch.sqrt(torch.sum( neg_vecs ** 2, dim=2)))\n",
    "    neg_scores = torch.max(neg_scores, dim=1)[0]\n",
    "    \n",
    "#     print (torch.mean(neg_scores - pos_scores))\n",
    "    \n",
    "    diff = neg_scores - pos_scores + 0.5\n",
    "    loss = torch.mean((diff > 0).float() * diff)\n",
    "#     for i in range(num_block):\n",
    "#         block_embeddings = embeddings[ i * 22: (i + 1) * 22 ]\n",
    "#         qs = block_embeddings[0]\n",
    "#         qs_ = block_embeddings[1: 22]\n",
    "#         cos_scores = cos_sim(qs.expand(21, 240), qs_)\n",
    "#         pos_score = cos_scores[0]\n",
    "#         neg_score = torch.max(cos_scores[1:])[0]\n",
    "#         diff = neg_score - pos_score + 1 # margin=1\n",
    "#         if diff.data[0] > 0:\n",
    "#             loss += diff\n",
    "            \n",
    "    return loss #loss / num_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_annotations(path, K_neg=20, prune_pos_cnt=10):\n",
    "    lst = [ ]\n",
    "    with open(path) as fin:\n",
    "        for line in fin:\n",
    "            parts = line.split(\"\\t\")\n",
    "            pid, pos, neg = parts[:3]\n",
    "            pos = pos.split()\n",
    "            neg = neg.split()\n",
    "            if len(pos) == 0 or (len(pos) > prune_pos_cnt and prune_pos_cnt != -1): continue\n",
    "            if K_neg != -1:\n",
    "                np.random.shuffle(neg)\n",
    "                neg = neg[:K_neg]\n",
    "            s = set()\n",
    "            qids = [ ]\n",
    "            qlabels = [ ]\n",
    "            for q in neg:\n",
    "                if q not in s:\n",
    "                    qids.append(q)\n",
    "                    qlabels.append(0 if q not in pos else 1)\n",
    "                    s.add(q)\n",
    "            for q in pos:\n",
    "                if q not in s:\n",
    "                    qids.append(q)\n",
    "                    qlabels.append(1)\n",
    "                    s.add(q)\n",
    "            lst.append((pid, qids, qlabels))\n",
    "\n",
    "    return lst\n",
    "\n",
    "# evaluation\n",
    "dev = read_annotations('data/dev.txt')\n",
    "dev_data = {}\n",
    "for item in dev:\n",
    "    qid = int(item[0])\n",
    "    dev_data[qid] = {}\n",
    "    dev_data[qid]['q'] = list(map(int, item[1]))\n",
    "    dev_data[qid]['label'] = item[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cos_sim(qv, qv_):\n",
    "    return torch.sum(qv * qv_, dim=1) / (torch.sqrt(torch.sum(qv ** 2, dim=1)) * torch.sqrt(torch.sum(qv_ ** 2, dim=1)))\n",
    "    \n",
    "# create eval batch \n",
    "def process_eval_batch(qid, data):\n",
    "    qid_dict = data[qid]\n",
    "    qs = qid_dict['q']\n",
    "    max_title_len, max_body_len = 0, 0\n",
    "    title_len, body_len = [], []\n",
    "    batch_title, batch_body = [], []\n",
    "    for qid_ in [qid] + qs:\n",
    "        title, body = context_repre[qid_]['t'], context_repre[qid_]['b']\n",
    "        title_len += [len(title)]\n",
    "        batch_title += [ title ]\n",
    "        max_title_len = max(max_title_len, len(title))\n",
    "        if not body:\n",
    "            body_len += [len(title)]\n",
    "            batch_body += [ title ]\n",
    "        else:\n",
    "            batch_body += [ body ]\n",
    "            body_len += [len(body)]\n",
    "            max_body_len = max(max_body_len, len(body))\n",
    "        \n",
    "    padded_batch_title = np.zeros(( max_title_len, len(batch_title), 200)) \n",
    "    padded_batch_body = np.zeros(( max_body_len, len(batch_body),  200))\n",
    "    \n",
    "    for i, (title, body) in enumerate(zip(batch_title, batch_body)):\n",
    "        title_repre, body_repre = contxt2vec(title, body)\n",
    "        padded_batch_title[:title_len[i], i] = title_repre\n",
    "        padded_batch_body[:body_len[i], i] = body_repre\n",
    "    \n",
    "    return padded_batch_title, padded_batch_body, \\\n",
    "           np.array(title_len).reshape(-1,1), np.array(body_len).reshape(-1,1) \n",
    "    \n",
    "def evaluate(embeddings): # (n x 240)\n",
    "    qs = embeddings[0]\n",
    "    qs_ = embeddings[1:]\n",
    "    cos_scores = cos_sim(qs.expand(len(embeddings)-1, 240), qs_)\n",
    "    return cos_scores\n",
    "\n",
    "def precision(at, labels):\n",
    "    res = []\n",
    "    for item in labels:\n",
    "        tmp = item[:at]\n",
    "        if any(val==1 for val in item):\n",
    "            res.append(np.sum(tmp) / len(tmp) if len(tmp) != 0 else 0.0)\n",
    "    return sum(res)/len(res) if len(res) != 0 else 0.0\n",
    "\n",
    "def MAP(labels):\n",
    "    scores = []\n",
    "    missing_MAP = 0\n",
    "    for item in labels:\n",
    "        temp = []\n",
    "        count = 0.0\n",
    "        for i,val in enumerate(item):\n",
    "            \n",
    "            if val == 1:\n",
    "                count += 1.0\n",
    "                temp.append(count/(i+1))\n",
    "            if len(temp) > 0:\n",
    "                scores.append(sum(temp) / len(temp))\n",
    "            else:\n",
    "                missing_MAP += 1\n",
    "    return sum(scores)/len(scores) if len(scores) > 0 else 0.0\n",
    "    \n",
    "def MRR(labels):\n",
    "    scores = []\n",
    "    for item in labels:\n",
    "        for i,val in enumerate(item):\n",
    "            if val == 1:\n",
    "                scores.append(1.0/(i+1))\n",
    "                break\n",
    "    return sum(scores)/len(scores) if len(scores) > 0 else 0.0\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EmbeddingLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, layer_type, kernel_size=None):\n",
    "        \n",
    "        super(EmbeddingLayer, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.kernel_size = kernel_size\n",
    "        \n",
    "        if layer_type == 'lstm':\n",
    "            \n",
    "            self.layer_type = 'lstm'\n",
    "            #self.title_embedding_layer = nn.LSTM(input_size, hidden_size)\n",
    "            #self.body_embedding_layer = nn.LSTM(input_size, hidden_size)\n",
    "            self.embedding_layer = nn.LSTM(input_size, hidden_size)\n",
    "            self.tanh = nn.Tanh()\n",
    "        \n",
    "        elif layer_type == 'cnn':\n",
    "            self.layer_type = 'cnn'\n",
    "            self.embedding_layer = nn.Sequential(\n",
    "                        nn.Conv1d(in_channels = 200,\n",
    "                                  out_channels = self.hidden_size,\n",
    "                                  kernel_size = kernel_size),\n",
    "                        nn.Tanh())\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return (Variable(torch.zeros(1, batch_size, self.hidden_size)), \\\n",
    "                Variable(torch.zeros(1, batch_size, self.hidden_size)))\n",
    "\n",
    "    def forward(self, title, body, title_len, body_len):\n",
    "            \n",
    "        if self.layer_type == 'lstm':\n",
    "            \n",
    "            \n",
    "            title_lstm_out, self.title_hidden = self.embedding_layer(title, (self.tanh(self.title_hidden[0]), \\\n",
    "                                                                   self.tanh(self.title_hidden[1])))\n",
    "            \n",
    "            body_lstm_out, self.body_hidden = self.embedding_layer(body, (self.tanh(self.body_hidden[0]), \\\n",
    "                                                                   self.tanh(self.body_hidden[1])))\n",
    "            \n",
    "            \n",
    "            title_mask = Variable(torch.FloatTensor(build_mask3d(title_len)))\n",
    "            title_embeddings = torch.sum(title_lstm_out * title_mask, dim=0) / torch.sum(title_mask, dim=0)\n",
    "            \n",
    "            body_mask = Variable(torch.FloatTensor(build_mask3d(body_len)))\n",
    "            body_embeddings = torch.sum(body_lstm_out * body_mask, dim=0) / torch.sum(body_mask, dim=0)\n",
    "            \n",
    "            embeddings = ( title_embeddings + body_embeddings ) / 2\n",
    "        \n",
    "            return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dev_map = {}\n",
    "for qid_ in dev_data.keys():\n",
    "    dev_map[qid_] = process_eval_batch(qid_, dev_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(layer_type, embedding_layer, batch_size=25, num_epoch=100, id_set=train_idx_set, eval=True):\n",
    "    \n",
    "#     if layer_type == 'lstm':\n",
    "#         embedding_layer = EmbeddingLayer(200, 240, 'lstm')\n",
    "#     elif layer_type == 'cnn':\n",
    "#         embedding_layer = EmbeddingLayer(200, 240, 'cnn', kernel_size=3)\n",
    "        \n",
    "    optimizer = torch.optim.Adam(embedding_layer.parameters(), lr=0.001)\n",
    "#     criterion = torch.nn.MultiMarginLoss()\n",
    "    \n",
    "    qids = list(id_set.keys())\n",
    "    num_batch = len(qids) // batch_size\n",
    "    \n",
    "    for epoch in range(1, num_epoch + 1):\n",
    "        \n",
    "        for batch_idx in range(1, num_batch + 1):\n",
    "            \n",
    "            batch_x_qids = qids[ ( batch_idx - 1 ) * batch_size: batch_idx * batch_size ]\n",
    "            batch_title, batch_body, title_len, body_len = process_contxt_batch(batch_x_qids, train_idx_set)\n",
    "            \n",
    "            if layer_type == 'lstm':\n",
    "                embedding_layer.title_hidden = embedding_layer.init_hidden(batch_title.shape[1])\n",
    "                embedding_layer.body_hidden = embedding_layer.init_hidden(batch_body.shape[1])\n",
    "            \n",
    "            title_qs = Variable(torch.FloatTensor(batch_title))#, requires_grad=True)\n",
    "            body_qs = Variable(torch.FloatTensor(batch_body))#, requires_grad=True)\n",
    "            \n",
    "            embeddings = embedding_layer(title_qs, body_qs, title_len, body_len)\n",
    "\n",
    "            blocked_embeddings = embeddings.view(-1, 22, 240)\n",
    "            q_vecs = blocked_embeddings[:,0,:]\n",
    "            pos_neg_vecs = blocked_embeddings[:,1:,:]\n",
    "            \n",
    "            # cosine similarity\n",
    "            scores = torch.sum(torch.unsqueeze(q_vecs, dim=1) * pos_neg_vecs, dim=2) \\\n",
    "                / (torch.unsqueeze(torch.sqrt(torch.sum(q_vecs ** 2, dim=1)), dim=1) *\\\n",
    "                   torch.sqrt(torch.sum( pos_neg_vecs ** 2, dim=2)))\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            loss = criterion(embeddings)\n",
    "            print ('epoch:{}/{}, batch:{}/{}, loss:{}'.format(epoch, num_epoch, batch_idx, num_batch, loss.data[0]))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "            if eval and batch_idx % 3 == 0: # lstm for now\n",
    "                labels = []\n",
    "                for qid_ in dev_data.keys():\n",
    "                    dev_title_batch, dev_body_batch, dev_title_len, dev_body_len = dev_map[qid_] # process_eval_batch(qid_, dev_data)\n",
    "                    embedding_layer.title_hidden = embedding_layer.init_hidden(dev_title_batch.shape[1])\n",
    "                    embedding_layer.body_hidden = embedding_layer.init_hidden(dev_body_batch.shape[1])\n",
    "                    dev_title_qs = Variable(torch.FloatTensor(dev_title_batch))\n",
    "                    dev_body_qs = Variable(torch.FloatTensor(dev_body_batch))\n",
    "                    embeddings = embedding_layer(dev_title_qs, dev_body_qs, dev_title_len, dev_body_len)\n",
    "                    cos_scores = evaluate(embeddings)\n",
    "                    labels.append(np.array(dev_data[qid_]['label'])[np.argsort(cos_scores.data.numpy())][::-1])\n",
    "\n",
    "                print ('Dev Performance P@5', precision(5, labels))\n",
    "                print ('Dev Performance P@1', precision(1, labels))\n",
    "                print ('Dev Performance MAP', MAP(labels))\n",
    "                print ('Dev Performance MRR', MRR(labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1/10, batch:1/508, loss:0.5023306608200073\n",
      "epoch:1/10, batch:2/508, loss:0.5037089586257935\n",
      "epoch:1/10, batch:3/508, loss:0.5006737112998962\n",
      "Dev Performance P@5 0.327950310559\n",
      "Dev Performance P@1 0.44099378882\n",
      "Dev Performance MAP 0.5665348505421848\n",
      "Dev Performance MRR 0.5963184066252641\n",
      "epoch:1/10, batch:4/508, loss:0.5004300475120544\n",
      "epoch:1/10, batch:5/508, loss:0.5011906623840332\n",
      "epoch:1/10, batch:6/508, loss:0.49970996379852295\n",
      "Dev Performance P@5 0.325465838509\n",
      "Dev Performance P@1 0.447204968944\n",
      "Dev Performance MAP 0.5669706184749115\n",
      "Dev Performance MRR 0.5955992647068163\n",
      "epoch:1/10, batch:7/508, loss:0.5006445050239563\n",
      "epoch:1/10, batch:8/508, loss:0.49955427646636963\n",
      "epoch:1/10, batch:9/508, loss:0.49984684586524963\n",
      "Dev Performance P@5 0.327950310559\n",
      "Dev Performance P@1 0.472049689441\n",
      "Dev Performance MAP 0.578376813652434\n",
      "Dev Performance MRR 0.6094053972743664\n",
      "epoch:1/10, batch:10/508, loss:0.49907150864601135\n",
      "epoch:1/10, batch:11/508, loss:0.4999648928642273\n",
      "epoch:1/10, batch:12/508, loss:0.5002502202987671\n",
      "Dev Performance P@5 0.337888198758\n",
      "Dev Performance P@1 0.453416149068\n",
      "Dev Performance MAP 0.5766336933062965\n",
      "Dev Performance MRR 0.599958647559988\n",
      "epoch:1/10, batch:13/508, loss:0.49897581338882446\n",
      "epoch:1/10, batch:14/508, loss:0.49979111552238464\n",
      "epoch:1/10, batch:15/508, loss:0.4992908835411072\n",
      "Dev Performance P@5 0.345341614907\n",
      "Dev Performance P@1 0.459627329193\n",
      "Dev Performance MAP 0.5754272399197257\n",
      "Dev Performance MRR 0.601985031235988\n",
      "epoch:1/10, batch:16/508, loss:0.4990004003047943\n",
      "epoch:1/10, batch:17/508, loss:0.5002651810646057\n",
      "epoch:1/10, batch:18/508, loss:0.500037670135498\n",
      "Dev Performance P@5 0.349068322981\n",
      "Dev Performance P@1 0.453416149068\n",
      "Dev Performance MAP 0.5755518691063509\n",
      "Dev Performance MRR 0.601582742013823\n",
      "epoch:1/10, batch:19/508, loss:0.4993022382259369\n",
      "epoch:1/10, batch:20/508, loss:0.4998335540294647\n",
      "epoch:1/10, batch:21/508, loss:0.49944254755973816\n",
      "Dev Performance P@5 0.355279503106\n",
      "Dev Performance P@1 0.465838509317\n",
      "Dev Performance MAP 0.5787104225383564\n",
      "Dev Performance MRR 0.6034890741220188\n",
      "epoch:1/10, batch:22/508, loss:0.5000391006469727\n",
      "epoch:1/10, batch:23/508, loss:0.4995197057723999\n",
      "epoch:1/10, batch:24/508, loss:0.4994243383407593\n",
      "Dev Performance P@5 0.345341614907\n",
      "Dev Performance P@1 0.459627329193\n",
      "Dev Performance MAP 0.5745492337748478\n",
      "Dev Performance MRR 0.5983014962718061\n",
      "epoch:1/10, batch:25/508, loss:0.4982353448867798\n",
      "epoch:1/10, batch:26/508, loss:0.5001642107963562\n",
      "epoch:1/10, batch:27/508, loss:0.49867674708366394\n",
      "Dev Performance P@5 0.346583850932\n",
      "Dev Performance P@1 0.478260869565\n",
      "Dev Performance MAP 0.5872264232575479\n",
      "Dev Performance MRR 0.6109930723448214\n",
      "epoch:1/10, batch:28/508, loss:0.49884137511253357\n",
      "epoch:1/10, batch:29/508, loss:0.49910104274749756\n",
      "epoch:1/10, batch:30/508, loss:0.4986686408519745\n",
      "Dev Performance P@5 0.340372670807\n",
      "Dev Performance P@1 0.478260869565\n",
      "Dev Performance MAP 0.5933116919731509\n",
      "Dev Performance MRR 0.6132928943021632\n",
      "epoch:1/10, batch:31/508, loss:0.5001353025436401\n",
      "epoch:1/10, batch:32/508, loss:0.4972870647907257\n",
      "epoch:1/10, batch:33/508, loss:0.49682292342185974\n",
      "Dev Performance P@5 0.352795031056\n",
      "Dev Performance P@1 0.459627329193\n",
      "Dev Performance MAP 0.5900120500058883\n",
      "Dev Performance MRR 0.607899582251062\n",
      "epoch:1/10, batch:34/508, loss:0.4978134036064148\n",
      "epoch:1/10, batch:35/508, loss:0.5014823079109192\n",
      "epoch:1/10, batch:36/508, loss:0.4979383051395416\n",
      "Dev Performance P@5 0.365217391304\n",
      "Dev Performance P@1 0.459627329193\n",
      "Dev Performance MAP 0.5879472736821839\n",
      "Dev Performance MRR 0.6079535102781358\n",
      "epoch:1/10, batch:37/508, loss:0.4992222785949707\n",
      "epoch:1/10, batch:38/508, loss:0.4936165511608124\n",
      "epoch:1/10, batch:39/508, loss:0.49807655811309814\n",
      "Dev Performance P@5 0.36397515528\n",
      "Dev Performance P@1 0.447204968944\n",
      "Dev Performance MAP 0.5841619598515569\n",
      "Dev Performance MRR 0.600896919525893\n",
      "epoch:1/10, batch:40/508, loss:0.4937758445739746\n",
      "epoch:1/10, batch:41/508, loss:0.5018978118896484\n",
      "epoch:1/10, batch:42/508, loss:0.48915648460388184\n",
      "Dev Performance P@5 0.365217391304\n",
      "Dev Performance P@1 0.453416149068\n",
      "Dev Performance MAP 0.5810825281214307\n",
      "Dev Performance MRR 0.6027832603599868\n",
      "epoch:1/10, batch:43/508, loss:0.48637640476226807\n",
      "epoch:1/10, batch:44/508, loss:0.49407249689102173\n",
      "epoch:1/10, batch:45/508, loss:0.48782816529273987\n",
      "Dev Performance P@5 0.373913043478\n",
      "Dev Performance P@1 0.503105590062\n",
      "Dev Performance MAP 0.6085039071039776\n",
      "Dev Performance MRR 0.6335684897248653\n",
      "epoch:1/10, batch:46/508, loss:0.47727149724960327\n",
      "epoch:1/10, batch:47/508, loss:0.489310622215271\n",
      "epoch:1/10, batch:48/508, loss:0.4798659086227417\n",
      "Dev Performance P@5 0.371428571429\n",
      "Dev Performance P@1 0.459627329193\n",
      "Dev Performance MAP 0.592391722137018\n",
      "Dev Performance MRR 0.6065616233318097\n",
      "epoch:1/10, batch:49/508, loss:0.46939754486083984\n",
      "epoch:1/10, batch:50/508, loss:0.4607292413711548\n",
      "epoch:1/10, batch:51/508, loss:0.418464332818985\n",
      "Dev Performance P@5 0.36397515528\n",
      "Dev Performance P@1 0.447204968944\n",
      "Dev Performance MAP 0.59293055546319\n",
      "Dev Performance MRR 0.6122197987333172\n",
      "epoch:1/10, batch:52/508, loss:0.4642743766307831\n",
      "epoch:1/10, batch:53/508, loss:0.4285283386707306\n",
      "epoch:1/10, batch:54/508, loss:0.44868797063827515\n",
      "Dev Performance P@5 0.365217391304\n",
      "Dev Performance P@1 0.465838509317\n",
      "Dev Performance MAP 0.5975483563905959\n",
      "Dev Performance MRR 0.6298515914929872\n",
      "epoch:1/10, batch:55/508, loss:0.484900563955307\n",
      "epoch:1/10, batch:56/508, loss:0.47022655606269836\n",
      "epoch:1/10, batch:57/508, loss:0.4509751498699188\n",
      "Dev Performance P@5 0.35652173913\n",
      "Dev Performance P@1 0.453416149068\n",
      "Dev Performance MAP 0.5843123060763922\n",
      "Dev Performance MRR 0.6173180106135897\n",
      "epoch:1/10, batch:58/508, loss:0.4151831269264221\n",
      "epoch:1/10, batch:59/508, loss:0.41980400681495667\n",
      "epoch:1/10, batch:60/508, loss:0.40268704295158386\n",
      "Dev Performance P@5 0.347826086957\n",
      "Dev Performance P@1 0.472049689441\n",
      "Dev Performance MAP 0.5861476265193182\n",
      "Dev Performance MRR 0.6229072687147221\n",
      "epoch:1/10, batch:61/508, loss:0.4336187541484833\n",
      "epoch:1/10, batch:62/508, loss:0.4656851887702942\n",
      "epoch:1/10, batch:63/508, loss:0.4348211884498596\n",
      "Dev Performance P@5 0.360248447205\n",
      "Dev Performance P@1 0.44099378882\n",
      "Dev Performance MAP 0.5760668529048102\n",
      "Dev Performance MRR 0.6052956931920644\n",
      "epoch:1/10, batch:64/508, loss:0.3550465703010559\n",
      "epoch:1/10, batch:65/508, loss:0.4182308614253998\n",
      "epoch:1/10, batch:66/508, loss:0.3740343153476715\n",
      "Dev Performance P@5 0.355279503106\n",
      "Dev Performance P@1 0.447204968944\n",
      "Dev Performance MAP 0.5836123781644631\n",
      "Dev Performance MRR 0.6116075685420244\n",
      "epoch:1/10, batch:67/508, loss:0.37764883041381836\n",
      "epoch:1/10, batch:68/508, loss:0.4311179220676422\n",
      "epoch:1/10, batch:69/508, loss:0.47311627864837646\n",
      "Dev Performance P@5 0.371428571429\n",
      "Dev Performance P@1 0.465838509317\n",
      "Dev Performance MAP 0.5989674772019569\n",
      "Dev Performance MRR 0.6234233578908407\n",
      "epoch:1/10, batch:70/508, loss:0.5079113841056824\n",
      "epoch:1/10, batch:71/508, loss:0.3651775121688843\n",
      "epoch:1/10, batch:72/508, loss:0.4051441252231598\n",
      "Dev Performance P@5 0.36149068323\n",
      "Dev Performance P@1 0.472049689441\n",
      "Dev Performance MAP 0.6008110577968669\n",
      "Dev Performance MRR 0.6311273925486087\n",
      "epoch:1/10, batch:73/508, loss:0.4549970328807831\n",
      "epoch:1/10, batch:74/508, loss:0.3944053053855896\n",
      "epoch:1/10, batch:75/508, loss:0.45975661277770996\n",
      "Dev Performance P@5 0.35900621118\n",
      "Dev Performance P@1 0.434782608696\n",
      "Dev Performance MAP 0.5859743730120435\n",
      "Dev Performance MRR 0.6050003299181232\n",
      "epoch:1/10, batch:76/508, loss:0.3475836217403412\n",
      "epoch:1/10, batch:77/508, loss:0.5308460593223572\n",
      "epoch:1/10, batch:78/508, loss:0.5342699885368347\n",
      "Dev Performance P@5 0.368944099379\n",
      "Dev Performance P@1 0.422360248447\n",
      "Dev Performance MAP 0.5797484505704537\n",
      "Dev Performance MRR 0.5947299275441403\n",
      "epoch:1/10, batch:79/508, loss:0.37817496061325073\n",
      "epoch:1/10, batch:80/508, loss:0.38826048374176025\n",
      "epoch:1/10, batch:81/508, loss:0.48096951842308044\n",
      "Dev Performance P@5 0.362732919255\n",
      "Dev Performance P@1 0.44099378882\n",
      "Dev Performance MAP 0.5815352182497739\n",
      "Dev Performance MRR 0.6055949367366497\n",
      "epoch:1/10, batch:82/508, loss:0.4080498218536377\n",
      "epoch:1/10, batch:83/508, loss:0.39704614877700806\n",
      "epoch:1/10, batch:84/508, loss:0.4310535490512848\n",
      "Dev Performance P@5 0.357763975155\n",
      "Dev Performance P@1 0.44099378882\n",
      "Dev Performance MAP 0.5878336303483052\n",
      "Dev Performance MRR 0.6087705169226909\n",
      "epoch:1/10, batch:85/508, loss:0.38935866951942444\n",
      "epoch:1/10, batch:86/508, loss:0.41138970851898193\n",
      "epoch:1/10, batch:87/508, loss:0.36581137776374817\n",
      "Dev Performance P@5 0.365217391304\n",
      "Dev Performance P@1 0.447204968944\n",
      "Dev Performance MAP 0.593833391219285\n",
      "Dev Performance MRR 0.6126785395418938\n",
      "epoch:1/10, batch:88/508, loss:0.3602173328399658\n",
      "epoch:1/10, batch:89/508, loss:0.3876989781856537\n",
      "epoch:1/10, batch:90/508, loss:0.40831029415130615\n",
      "Dev Performance P@5 0.370186335404\n",
      "Dev Performance P@1 0.434782608696\n",
      "Dev Performance MAP 0.587987323213548\n",
      "Dev Performance MRR 0.6044316241749563\n",
      "epoch:1/10, batch:91/508, loss:0.41862982511520386\n",
      "epoch:1/10, batch:92/508, loss:0.36060234904289246\n",
      "epoch:1/10, batch:93/508, loss:0.33234351873397827\n",
      "Dev Performance P@5 0.366459627329\n",
      "Dev Performance P@1 0.44099378882\n",
      "Dev Performance MAP 0.5811980692026845\n",
      "Dev Performance MRR 0.6043112795944364\n",
      "epoch:1/10, batch:94/508, loss:0.4011979103088379\n",
      "epoch:1/10, batch:95/508, loss:0.37499523162841797\n",
      "epoch:1/10, batch:96/508, loss:0.35085058212280273\n",
      "Dev Performance P@5 0.362732919255\n",
      "Dev Performance P@1 0.403726708075\n",
      "Dev Performance MAP 0.5751711965714659\n",
      "Dev Performance MRR 0.5815705742806103\n",
      "epoch:1/10, batch:97/508, loss:0.33336782455444336\n",
      "epoch:1/10, batch:98/508, loss:0.45585155487060547\n",
      "epoch:1/10, batch:99/508, loss:0.3511016368865967\n",
      "Dev Performance P@5 0.357763975155\n",
      "Dev Performance P@1 0.453416149068\n",
      "Dev Performance MAP 0.5909560713444004\n",
      "Dev Performance MRR 0.6135853844933129\n",
      "epoch:1/10, batch:100/508, loss:0.3332619369029999\n",
      "epoch:1/10, batch:101/508, loss:0.568055272102356\n",
      "epoch:1/10, batch:102/508, loss:0.4301847815513611\n",
      "Dev Performance P@5 0.35900621118\n",
      "Dev Performance P@1 0.465838509317\n",
      "Dev Performance MAP 0.6040659483809815\n",
      "Dev Performance MRR 0.6215114793256921\n",
      "epoch:1/10, batch:103/508, loss:0.36560240387916565\n",
      "epoch:1/10, batch:104/508, loss:0.4210207462310791\n",
      "epoch:1/10, batch:105/508, loss:0.402194619178772\n",
      "Dev Performance P@5 0.362732919255\n",
      "Dev Performance P@1 0.472049689441\n",
      "Dev Performance MAP 0.6001866884928114\n",
      "Dev Performance MRR 0.6289845937361465\n",
      "epoch:1/10, batch:106/508, loss:0.3627468943595886\n",
      "epoch:1/10, batch:107/508, loss:0.40840432047843933\n",
      "epoch:1/10, batch:108/508, loss:0.4286133050918579\n",
      "Dev Performance P@5 0.36397515528\n",
      "Dev Performance P@1 0.428571428571\n",
      "Dev Performance MAP 0.5820070295203651\n",
      "Dev Performance MRR 0.5998131957759286\n",
      "epoch:1/10, batch:109/508, loss:0.42948049306869507\n",
      "epoch:1/10, batch:110/508, loss:0.32002681493759155\n",
      "epoch:1/10, batch:111/508, loss:0.4658379554748535\n",
      "Dev Performance P@5 0.365217391304\n",
      "Dev Performance P@1 0.428571428571\n",
      "Dev Performance MAP 0.5790050347738361\n",
      "Dev Performance MRR 0.598990976242529\n",
      "epoch:1/10, batch:112/508, loss:0.48486268520355225\n",
      "epoch:1/10, batch:113/508, loss:0.3558460474014282\n",
      "epoch:1/10, batch:114/508, loss:0.4321812689304352\n",
      "Dev Performance P@5 0.366459627329\n",
      "Dev Performance P@1 0.447204968944\n",
      "Dev Performance MAP 0.5920535323905154\n",
      "Dev Performance MRR 0.6130863012074192\n",
      "epoch:1/10, batch:115/508, loss:0.3828161060810089\n",
      "epoch:1/10, batch:116/508, loss:0.33097729086875916\n",
      "epoch:1/10, batch:117/508, loss:0.42880669236183167\n",
      "Dev Performance P@5 0.36397515528\n",
      "Dev Performance P@1 0.44099378882\n",
      "Dev Performance MAP 0.586458658497541\n",
      "Dev Performance MRR 0.6098759357802105\n",
      "epoch:1/10, batch:118/508, loss:0.41216719150543213\n",
      "epoch:1/10, batch:119/508, loss:0.37519630789756775\n",
      "epoch:1/10, batch:120/508, loss:0.33298125863075256\n",
      "Dev Performance P@5 0.355279503106\n",
      "Dev Performance P@1 0.44099378882\n",
      "Dev Performance MAP 0.5910062661746271\n",
      "Dev Performance MRR 0.6111509719931347\n",
      "epoch:1/10, batch:121/508, loss:0.361935019493103\n",
      "epoch:1/10, batch:122/508, loss:0.3792624771595001\n",
      "epoch:1/10, batch:123/508, loss:0.34934201836586\n",
      "Dev Performance P@5 0.36397515528\n",
      "Dev Performance P@1 0.453416149068\n",
      "Dev Performance MAP 0.5921195636661417\n",
      "Dev Performance MRR 0.6140358606197115\n",
      "epoch:1/10, batch:124/508, loss:0.358748197555542\n",
      "epoch:1/10, batch:125/508, loss:0.35906216502189636\n",
      "epoch:1/10, batch:126/508, loss:0.3351052403450012\n",
      "Dev Performance P@5 0.365217391304\n",
      "Dev Performance P@1 0.453416149068\n",
      "Dev Performance MAP 0.5911919570690115\n",
      "Dev Performance MRR 0.6166420018904493\n",
      "epoch:1/10, batch:127/508, loss:0.3967163562774658\n",
      "epoch:1/10, batch:128/508, loss:0.38294050097465515\n",
      "epoch:1/10, batch:129/508, loss:0.3134218156337738\n",
      "Dev Performance P@5 0.366459627329\n",
      "Dev Performance P@1 0.465838509317\n",
      "Dev Performance MAP 0.597233680197952\n",
      "Dev Performance MRR 0.6219247306706206\n",
      "epoch:1/10, batch:130/508, loss:0.26609480381011963\n",
      "epoch:1/10, batch:131/508, loss:0.4436338245868683\n",
      "epoch:1/10, batch:132/508, loss:0.3364516496658325\n",
      "Dev Performance P@5 0.367701863354\n",
      "Dev Performance P@1 0.453416149068\n",
      "Dev Performance MAP 0.5942146215633546\n",
      "Dev Performance MRR 0.6110511494554242\n",
      "epoch:1/10, batch:133/508, loss:0.376832515001297\n",
      "epoch:1/10, batch:134/508, loss:0.3162343502044678\n",
      "epoch:1/10, batch:135/508, loss:0.3285302221775055\n",
      "Dev Performance P@5 0.366459627329\n",
      "Dev Performance P@1 0.484472049689\n",
      "Dev Performance MAP 0.6068016367132154\n",
      "Dev Performance MRR 0.628292956992263\n",
      "epoch:1/10, batch:136/508, loss:0.33660879731178284\n",
      "epoch:1/10, batch:137/508, loss:0.4099472165107727\n",
      "epoch:1/10, batch:138/508, loss:0.3518233895301819\n",
      "Dev Performance P@5 0.36397515528\n",
      "Dev Performance P@1 0.453416149068\n",
      "Dev Performance MAP 0.5952857545874416\n",
      "Dev Performance MRR 0.6219743593190797\n",
      "epoch:1/10, batch:139/508, loss:0.4080505967140198\n",
      "epoch:1/10, batch:140/508, loss:0.40226611495018005\n",
      "epoch:1/10, batch:141/508, loss:0.38161545991897583\n",
      "Dev Performance P@5 0.378881987578\n",
      "Dev Performance P@1 0.465838509317\n",
      "Dev Performance MAP 0.5960398079818953\n",
      "Dev Performance MRR 0.6252870084894199\n",
      "epoch:1/10, batch:142/508, loss:0.30963581800460815\n",
      "epoch:1/10, batch:143/508, loss:0.39460641145706177\n",
      "epoch:1/10, batch:144/508, loss:0.3252857029438019\n",
      "Dev Performance P@5 0.368944099379\n",
      "Dev Performance P@1 0.453416149068\n",
      "Dev Performance MAP 0.5889720914481813\n",
      "Dev Performance MRR 0.6124993105583165\n",
      "epoch:1/10, batch:145/508, loss:0.38249272108078003\n",
      "epoch:1/10, batch:146/508, loss:0.40076836943626404\n",
      "epoch:1/10, batch:147/508, loss:0.3284648358821869\n",
      "Dev Performance P@5 0.375155279503\n",
      "Dev Performance P@1 0.459627329193\n",
      "Dev Performance MAP 0.5933912658745553\n",
      "Dev Performance MRR 0.621615277679351\n"
     ]
    }
   ],
   "source": [
    "model = EmbeddingLayer(200, 240, 'lstm')\n",
    "scores, target, emb = train('lstm', model, batch_size=25, num_epoch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qids = list(train_idx_set.keys())[:25]\n",
    "t, b, tl, bl = process_contxt_batch(batch_x_qids, train_idx_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_layer = EmbeddingLayer(200, 240, 'lstm')\n",
    "embedding_layer.title_hidden = embedding_layer.init_hidden(t.shape[1])\n",
    "embedding_layer.body_hidden = embedding_layer.init_hidden(b.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "title_qs = Variable(torch.FloatTensor(t))\n",
    "body_qs = Variable(torch.FloatTensor(b))\n",
    "embeddings = embedding_layer(title_qs, body_qs, tl, bl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target = Variable(torch.LongTensor([1,1] + [0]*20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = torch.nn.MultiMarginLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 1.0260\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(embeddings[:22], target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "blocked_embeddings = embeddings.view(-1, 22, 240)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q_vecs = blocked_embeddings[:,0,:]\n",
    "pos_vecs = blocked_embeddings[:,1,:]\n",
    "neg_vecs = blocked_embeddings[:,2:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_scores = torch.sum(q_vecs * pos_vecs, dim=1) / (torch.sqrt(torch.sum(q_vecs ** 2, dim=1)) \\\n",
    "                                    * torch.sqrt(torch.sum(pos_vecs ** 2, dim=1)))\n",
    "neg_scores = torch.sum(torch.unsqueeze(q_vecs, dim=1) * neg_vecs, dim=2) \\\n",
    "/ (torch.unsqueeze(torch.sqrt(torch.sum(q_vecs ** 2, dim=1)), dim=1) * torch.sqrt(torch.sum( neg_vecs ** 2, dim=2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Variable containing:\n",
       "  0.9956\n",
       " [torch.FloatTensor of size 1], Variable containing:\n",
       "  0.9919\n",
       "  0.9907\n",
       "  0.9932\n",
       "  0.9865\n",
       "  0.9866\n",
       "  0.9890\n",
       "  0.9923\n",
       "  0.9835\n",
       "  0.9876\n",
       "  0.9871\n",
       "  0.9884\n",
       "  0.9846\n",
       "  0.9868\n",
       "  0.9855\n",
       "  0.9879\n",
       "  0.9884\n",
       "  0.9886\n",
       "  0.9864\n",
       "  0.9892\n",
       "  0.9893\n",
       " [torch.FloatTensor of size 20])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_scores[0], neg_scores[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q_vecs = blocked_embeddings[:,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pn_vecs = blocked_embeddings[:,1:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores = torch.sum(torch.unsqueeze(q_vecs, dim=1) * pn_vecs, dim=2) \\\n",
    "/ (torch.unsqueeze(torch.sqrt(torch.sum(q_vecs ** 2, dim=1)), dim=1) * torch.sqrt(torch.sum( pn_vecs ** 2, dim=2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "\n",
       "Columns 0 to 9 \n",
       " 0.9956  0.9919  0.9907  0.9932  0.9865  0.9866  0.9890  0.9923  0.9835  0.9876\n",
       " 0.9903  0.9830  0.9841  0.9789  0.9809  0.9809  0.9718  0.9705  0.9728  0.9721\n",
       " 0.9930  0.9897  0.9855  0.9897  0.9890  0.9820  0.9798  0.9854  0.9760  0.9834\n",
       " 0.9867  0.9819  0.9823  0.9876  0.9819  0.9851  0.9798  0.9759  0.9894  0.9874\n",
       " 0.9920  0.9864  0.9870  0.9819  0.9849  0.9806  0.9822  0.9809  0.9870  0.9651\n",
       " 0.9929  0.9910  0.9900  0.9828  0.9876  0.9922  0.9914  0.9906  0.9916  0.9895\n",
       " 0.9964  0.9934  0.9812  0.9916  0.9866  0.9881  0.9790  0.9898  0.9876  0.9864\n",
       " 0.9941  0.9806  0.9755  0.9790  0.9821  0.9760  0.9773  0.9871  0.9782  0.9830\n",
       " 0.9853  0.9871  0.9801  0.9824  0.9804  0.9677  0.9870  0.9842  0.9805  0.9844\n",
       " 0.9833  0.9861  0.9755  0.9871  0.9849  0.9815  0.9762  0.9896  0.9809  0.9874\n",
       " 0.9978  0.9925  0.9840  0.9818  0.9911  0.9924  0.9908  0.9857  0.9918  0.9911\n",
       " 0.9929  0.9882  0.9847  0.9853  0.9857  0.9858  0.9930  0.9908  0.9923  0.9881\n",
       " 0.9881  0.9908  0.9887  0.9878  0.9877  0.9852  0.9876  0.9875  0.9878  0.9873\n",
       " 0.9941  0.9718  0.9880  0.9866  0.9894  0.9810  0.9894  0.9928  0.9832  0.9922\n",
       " 0.9857  0.9832  0.9874  0.9907  0.9878  0.9881  0.9878  0.9877  0.9826  0.9878\n",
       " 0.9917  0.9776  0.9906  0.9915  0.9870  0.9858  0.9824  0.9867  0.9892  0.9889\n",
       " 0.9926  0.9806  0.9822  0.9721  0.9717  0.9754  0.9817  0.9772  0.9818  0.9768\n",
       " 0.9888  0.9916  0.9767  0.9896  0.9891  0.9891  0.9799  0.9851  0.9910  0.9912\n",
       " 0.9904  0.9853  0.9862  0.9824  0.9775  0.9788  0.9832  0.9874  0.9832  0.9833\n",
       " 0.9882  0.9857  0.9847  0.9908  0.9834  0.9855  0.9839  0.9839  0.9882  0.9851\n",
       " 0.9869  0.9797  0.9802  0.9834  0.9892  0.9775  0.9922  0.9856  0.9932  0.9829\n",
       " 0.9936  0.9886  0.9883  0.9793  0.9771  0.9819  0.9905  0.9857  0.9895  0.9874\n",
       " 0.9854  0.9784  0.9733  0.9772  0.9780  0.9729  0.9887  0.9831  0.9861  0.9847\n",
       " 0.9885  0.9808  0.9821  0.9888  0.9886  0.9861  0.9833  0.9899  0.9807  0.9873\n",
       " 0.9842  0.9899  0.9851  0.9729  0.9902  0.9812  0.9837  0.9799  0.9812  0.9857\n",
       " 0.9853  0.9888  0.9779  0.9866  0.9866  0.9916  0.9811  0.9771  0.9847  0.9886\n",
       " 0.9909  0.9790  0.9820  0.9906  0.9866  0.9908  0.9673  0.9829  0.9872  0.9835\n",
       " 0.9866  0.9818  0.9803  0.9812  0.9809  0.9847  0.9906  0.9926  0.9909  0.9745\n",
       " 0.9815  0.9877  0.9837  0.9788  0.9866  0.9877  0.9877  0.9812  0.9913  0.9811\n",
       " 0.9858  0.9673  0.9847  0.9902  0.9821  0.9809  0.9851  0.9812  0.9890  0.9866\n",
       " 0.9911  0.9729  0.9885  0.9861  0.9897  0.9782  0.9837  0.9857  0.9812  0.9745\n",
       " 0.9903  0.9838  0.9899  0.9748  0.9886  0.9713  0.9820  0.9838  0.9818  0.9748\n",
       " 0.9882  0.9818  0.9872  0.9782  0.9812  0.9861  0.9909  0.9908  0.9873  0.9916\n",
       " 0.9842  0.9752  0.9680  0.9735  0.9854  0.9790  0.9808  0.9716  0.9726  0.9759\n",
       " 0.9915  0.9843  0.9775  0.9790  0.9812  0.9893  0.9862  0.9866  0.9866  0.9809\n",
       " 0.9950  0.9826  0.9882  0.9827  0.9793  0.9836  0.9804  0.9810  0.9793  0.9884\n",
       " 0.9865  0.9839  0.9710  0.9776  0.9841  0.9857  0.9797  0.9875  0.9776  0.9736\n",
       " 0.9956  0.9832  0.9713  0.9912  0.9799  0.9856  0.9938  0.9798  0.9867  0.9828\n",
       " 0.9914  0.9853  0.9910  0.9896  0.9877  0.9839  0.9824  0.9932  0.9916  0.9873\n",
       " 0.9823  0.9819  0.9822  0.9857  0.9762  0.9808  0.9783  0.9827  0.9859  0.9828\n",
       " 0.9927  0.9923  0.9855  0.9806  0.9878  0.9864  0.9898  0.9923  0.9790  0.9872\n",
       "\n",
       "Columns 10 to 19 \n",
       " 0.9871  0.9884  0.9846  0.9868  0.9855  0.9879  0.9884  0.9886  0.9864  0.9892\n",
       " 0.9819  0.9744  0.9797  0.9801  0.9940  0.9867  0.9652  0.9769  0.9825  0.9850\n",
       " 0.9878  0.9796  0.9871  0.9772  0.9796  0.9794  0.9899  0.9790  0.9796  0.9872\n",
       " 0.9782  0.9870  0.9811  0.9772  0.9798  0.9794  0.9897  0.9845  0.9764  0.9835\n",
       " 0.9838  0.9855  0.9809  0.9844  0.9840  0.9851  0.9771  0.9920  0.9783  0.9895\n",
       " 0.9837  0.9914  0.9886  0.9836  0.9745  0.9870  0.9915  0.9818  0.9917  0.9907\n",
       " 0.9838  0.9906  0.9919  0.9866  0.9802  0.9887  0.9832  0.9741  0.9787  0.9827\n",
       " 0.9801  0.9771  0.9771  0.9769  0.9858  0.9782  0.9729  0.9769  0.9754  0.9768\n",
       " 0.9775  0.9831  0.9808  0.9747  0.9785  0.9808  0.9747  0.9750  0.9770  0.9747\n",
       " 0.9708  0.9786  0.9779  0.9766  0.9832  0.9849  0.9662  0.9838  0.9839  0.9799\n",
       " 0.9840  0.9930  0.9871  0.9818  0.9883  0.9901  0.9910  0.9869  0.9840  0.9929\n",
       " 0.9833  0.9882  0.9694  0.9882  0.9863  0.9885  0.9847  0.9882  0.9899  0.9887\n",
       " 0.9866  0.9922  0.9929  0.9913  0.9886  0.9790  0.9852  0.9861  0.9866  0.9813\n",
       " 0.9903  0.9848  0.9848  0.9918  0.9911  0.9826  0.9878  0.9853  0.9894  0.9908\n",
       " 0.9877  0.9877  0.9832  0.9884  0.9902  0.9846  0.9853  0.9735  0.9873  0.9928\n",
       " 0.9951  0.9906  0.9824  0.9828  0.9926  0.9914  0.9903  0.9920  0.9873  0.9920\n",
       " 0.9730  0.9798  0.9780  0.9706  0.9792  0.9819  0.9763  0.9794  0.9730  0.9782\n",
       " 0.9893  0.9862  0.9876  0.9874  0.9871  0.9847  0.9877  0.9846  0.9828  0.9905\n",
       " 0.9877  0.9872  0.9853  0.9877  0.9884  0.9809  0.9872  0.9815  0.9872  0.9804\n",
       " 0.9839  0.9872  0.9825  0.9908  0.9786  0.9771  0.9874  0.9797  0.9837  0.9830\n",
       " 0.9879  0.9897  0.9811  0.9908  0.9874  0.9851  0.9860  0.9893  0.9860  0.9932\n",
       " 0.9886  0.9838  0.9859  0.9814  0.9786  0.9895  0.9874  0.9852  0.9893  0.9882\n",
       " 0.9821  0.9877  0.9812  0.9771  0.9812  0.9837  0.9779  0.9793  0.9810  0.9917\n",
       " 0.9873  0.9918  0.9713  0.9916  0.9897  0.9866  0.9810  0.9835  0.9890  0.9916\n",
       " 0.9851  0.9866  0.9917  0.9755  0.9706  0.9873  0.9835  0.9877  0.9877  0.9868\n",
       " 0.9830  0.9906  0.9713  0.9896  0.9916  0.9830  0.9814  0.9889  0.9745  0.9814\n",
       " 0.9713  0.9861  0.9873  0.9830  0.9820  0.9917  0.9807  0.9888  0.9783  0.9772\n",
       " 0.9812  0.9892  0.9897  0.9713  0.9812  0.9913  0.9871  0.9886  0.9812  0.9851\n",
       " 0.9812  0.9790  0.9851  0.9837  0.9888  0.9748  0.9873  0.9818  0.9818  0.9713\n",
       " 0.9829  0.9816  0.9888  0.9729  0.9868  0.9811  0.9902  0.9818  0.9808  0.9888\n",
       " 0.9888  0.9908  0.9847  0.9863  0.9909  0.9837  0.9771  0.9906  0.9799  0.9812\n",
       " 0.9887  0.9873  0.9783  0.9811  0.9812  0.9801  0.9811  0.9771  0.9748  0.9706\n",
       " 0.9908  0.9916  0.9866  0.9866  0.9908  0.9863  0.9866  0.9808  0.9896  0.9918\n",
       " 0.9761  0.9820  0.9822  0.9761  0.9791  0.9707  0.9766  0.9792  0.9765  0.9762\n",
       " 0.9811  0.9847  0.9808  0.9755  0.9826  0.9705  0.9848  0.9836  0.9729  0.9925\n",
       " 0.9864  0.9729  0.9867  0.9867  0.9884  0.9794  0.9916  0.9790  0.9508  0.9803\n",
       " 0.9852  0.9787  0.9764  0.9864  0.9854  0.9787  0.9829  0.9852  0.9796  0.9826\n",
       " 0.9909  0.9730  0.9743  0.9867  0.9805  0.9874  0.9791  0.9906  0.9879  0.9912\n",
       " 0.9787  0.9899  0.9865  0.9856  0.9896  0.9813  0.9894  0.9865  0.9866  0.9799\n",
       " 0.9860  0.9810  0.9830  0.9824  0.9800  0.9848  0.9840  0.9824  0.9783  0.9868\n",
       " 0.9854  0.9846  0.9893  0.9916  0.9903  0.9870  0.9891  0.9857  0.9876  0.9790\n",
       "\n",
       "Columns 20 to 20 \n",
       " 0.9893\n",
       " 0.9780\n",
       " 0.9779\n",
       " 0.9760\n",
       " 0.9915\n",
       " 0.9877\n",
       " 0.9893\n",
       " 0.9798\n",
       " 0.9782\n",
       " 0.9813\n",
       " 0.9809\n",
       " 0.9861\n",
       " 0.9926\n",
       " 0.9932\n",
       " 0.9898\n",
       " 0.9881\n",
       " 0.9736\n",
       " 0.9881\n",
       " 0.9860\n",
       " 0.9786\n",
       " 0.9795\n",
       " 0.9847\n",
       " 0.9808\n",
       " 0.9829\n",
       " 0.9875\n",
       " 0.9809\n",
       " 0.9847\n",
       " 0.9877\n",
       " 0.9793\n",
       " 0.9830\n",
       " 0.9877\n",
       " 0.9855\n",
       " 0.9899\n",
       " 0.9808\n",
       " 0.9878\n",
       " 0.9775\n",
       " 0.9840\n",
       " 0.9853\n",
       " 0.9917\n",
       " 0.9762\n",
       " 0.9885\n",
       "[torch.FloatTensor of size 41x21]"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = torch.nn.MultiMarginLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.9469\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = Variable(torch.zeros(scores.size(0)).type(torch.LongTensor)) \n",
    "criterion(scores, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
