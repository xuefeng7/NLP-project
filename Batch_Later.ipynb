{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import string\n",
    "import numpy as np; np.random.seed(7)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_map = {}\n",
    "with open('data/vectors_pruned.200.txt', 'r') as src:\n",
    "    src = src.read().strip().split('\\n')\n",
    "    for line in src:\n",
    "        wv = line.strip().split(' ')\n",
    "        word = wv.pop(0)\n",
    "        w2v_map[word] = np.array(list(map(float, wv)))\n",
    "\n",
    "w2i_map = {}\n",
    "w2v_matrix = np.zeros(( len((w2v_map.keys())), 200 ))\n",
    "for i, (key, val) in enumerate(w2v_map.items()):\n",
    "    w2i_map[key] = i\n",
    "    w2v_matrix[i] = val\n",
    "\n",
    "def w2v(w):\n",
    "    return w2v_matrix[w2i_map[w]]\n",
    "\n",
    "def sen2w(sen):\n",
    "    processed = []\n",
    "    sen = sen.strip().split()\n",
    "    if len(sen) > 100:\n",
    "        sen = sen[:100]\n",
    "    for w in sen:\n",
    "        #ignore date\n",
    "        if re.match(r'\\d{1,}-\\d{1,}-\\d{1,}', w):\n",
    "            continue\n",
    "        if re.match(r'\\d{1,}:\\d{1,}', w):\n",
    "            continue\n",
    "        \n",
    "        if w in w2i_map:\n",
    "            processed += [w]\n",
    "        else:\n",
    "            separated = re.findall(r\"[^\\W\\d_]+|\\d+|[=`%$\\^\\-@;\\[&_*>\\].<~|+\\d+]\", w)\n",
    "            if len(set(separated)) == 1:\n",
    "                continue\n",
    "            if separated.count('*') > 3 or separated.count('=') > 3:\n",
    "                continue\n",
    "            for separate_w in separated:\n",
    "                if separate_w in w2i_map:\n",
    "                    processed += [separate_w]\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fixed context len = 125\n",
    "context_repre = {}\n",
    "with open('data/text_tokenized.txt', 'r') as src:\n",
    "    src = src.read().strip().split('\\n')\n",
    "    for line in src:\n",
    "        context = line.strip().split('\\t')\n",
    "        qid = context.pop(0)\n",
    "        if len(context) == 1:\n",
    "            context_repre[int(qid)] = {'t': sen2w(context[0]), 'b': None}\n",
    "        else:\n",
    "            context_repre[int(qid)] = {'t':sen2w(context[0]), 'b': sen2w(context[1])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_set_pair_with_idx(df):\n",
    "    idx_set = {}\n",
    "    for idx, row in df.iterrows():\n",
    "        idx_set[row['Q']] = {'pos': np.array(list(map(int, row['Q+'].split(' ')))), \\\n",
    "                             'neg': np.array(list(map(int, row['Q-'].split(' '))))}\n",
    "    return idx_set\n",
    "\n",
    "train_df = pd.read_csv('data/train_random.txt', header=None, delimiter='\\t', names=['Q','Q+','Q-'])\n",
    "train_idx_set = build_set_pair_with_idx(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>Q+</th>\n",
       "      <th>Q-</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>262144</td>\n",
       "      <td>211039</td>\n",
       "      <td>227387 413633 113297 356390 256881 145638 2962...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>491522</td>\n",
       "      <td>65911</td>\n",
       "      <td>155119 402211 310669 383107 131731 299465 1633...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>240299</td>\n",
       "      <td>168608 390642</td>\n",
       "      <td>368007 70009 48077 376760 438005 228888 142340...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>196614</td>\n",
       "      <td>205184</td>\n",
       "      <td>334471 163710 376791 441664 159963 406360 4300...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>360457</td>\n",
       "      <td>321532</td>\n",
       "      <td>151863 501857 217578 470017 125838 31836 42066...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Q             Q+                                                 Q-\n",
       "0  262144         211039  227387 413633 113297 356390 256881 145638 2962...\n",
       "1  491522          65911  155119 402211 310669 383107 131731 299465 1633...\n",
       "2  240299  168608 390642  368007 70009 48077 376760 438005 228888 142340...\n",
       "3  196614         205184  334471 163710 376791 441664 159963 406360 4300...\n",
       "4  360457         321532  151863 501857 217578 470017 125838 31836 42066..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def contxt2vec(title, body=None):\n",
    "    \n",
    "    if body == None:\n",
    "        body = []\n",
    "    \n",
    "    title_v = np.zeros( (len(title), 200) )\n",
    "    \n",
    "    for i, t in enumerate(title):\n",
    "        title_v[i] = w2v(t)\n",
    "    \n",
    "    if len(body) > 0:\n",
    "        body_v = np.zeros( (len(body), 200) )\n",
    "        for i, b in enumerate(body):\n",
    "            body_v[i] = w2v(b)\n",
    "    \n",
    "        return title_v, body_v\n",
    "    \n",
    "    return title_v, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_contxt_batch(qids, idx_set):\n",
    "    \n",
    "    batch_title, batch_body = [], []\n",
    "    max_title_len, max_body_len = 0, 0\n",
    "    title_len, body_len = [], []\n",
    "    counter = 0\n",
    "    y = []\n",
    "    \n",
    "    for qid in qids:\n",
    "        \n",
    "        q_title, q_body = context_repre[qid]['t'], context_repre[qid]['b']\n",
    "        q_pos = idx_set[qid]['pos']\n",
    "\n",
    "        for qid_pos in q_pos:\n",
    "\n",
    "            # query Q\n",
    "            title_len += [len(q_title)]\n",
    "            batch_title += [ q_title ]\n",
    "            max_title_len = max(max_title_len, len(q_title))\n",
    "            if not q_body:\n",
    "                body_len += [len(q_title)]\n",
    "                batch_body += [ q_title ]\n",
    "            else:\n",
    "                batch_body += [ q_body ]\n",
    "                body_len += [len(q_body)]\n",
    "                max_body_len = max(max_body_len, len(q_body))\n",
    "            y += [1]\n",
    "            # pos Q\n",
    "            title, body = context_repre[qid_pos]['t'], context_repre[qid_pos]['b']\n",
    "            title_len += [len(title)]\n",
    "            batch_title += [ title ]\n",
    "            max_title_len = max(max_title_len, len(title))\n",
    "            if not body:\n",
    "                body_len += [len(title)]\n",
    "                batch_body += [ title ]\n",
    "            else:\n",
    "                batch_body += [ body ]\n",
    "                body_len += [len(body)]\n",
    "                max_body_len = max(max_body_len, len(body))\n",
    "            y += [1]\n",
    "            # neg Q\n",
    "            q_neg = idx_set[qid]['neg']\n",
    "            q_neg_sample_indices = np.random.choice(range(100), size=20)\n",
    "            q_random_neg = q_neg[q_neg_sample_indices]\n",
    "            \n",
    "            for qid_neg in q_random_neg:\n",
    "                title, body = context_repre[qid_neg]['t'], context_repre[qid_neg]['b']\n",
    "                title_len += [len(title)]\n",
    "                batch_title += [ title ]\n",
    "                max_title_len = max(max_title_len, len(title))\n",
    "                if not body:\n",
    "                    body_len += [len(title)]\n",
    "                    batch_body += [ title ]\n",
    "                else:\n",
    "                    batch_body += [ body ]\n",
    "                    body_len += [len(body)]\n",
    "                    max_body_len = max(max_body_len, len(body))\n",
    "                y += [0]\n",
    "    # (max_seq_len, batch_size, feature_len)\n",
    "    padded_batch_title = np.zeros(( max_title_len, len(batch_title), 200)) \n",
    "    padded_batch_body = np.zeros(( max_body_len, len(batch_body),  200))\n",
    "    \n",
    "    for i, (title, body) in enumerate(zip(batch_title, batch_body)):\n",
    "        title_repre, body_repre = contxt2vec(title, body)\n",
    "        padded_batch_title[:title_len[i], i] = title_repre\n",
    "        padded_batch_body[:body_len[i], i] = body_repre\n",
    "    #np.array(y).reshape(-1,1)\n",
    "    return padded_batch_title, padded_batch_body, y,\\\n",
    "                np.array(title_len).reshape(-1,1), np.array(body_len).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_mask(seq_len):\n",
    "    mask = []\n",
    "    for i, s in enumerate(seq_len):\n",
    "        s_mask = np.zeros((np.max(seq_len), 1))\n",
    "        s_mask[:int(s)] = np.ones((int(s), 1))\n",
    "        mask += [s_mask]\n",
    "    return mask\n",
    "\n",
    "def build_mask3d(seq_len):\n",
    "    mask = np.zeros((np.max(seq_len), len(seq_len), 1))\n",
    "    for i, s in enumerate(seq_len):\n",
    "        mask[:int(s), i] = np.ones((int(s), 1))\n",
    "    return mask\n",
    "\n",
    "# def cos_sim(qv, qv_):\n",
    "#     return torch.sum(qv * qv_, dim=1) / (torch.sqrt(torch.sum(qv ** 2, dim=1)) * torch.sqrt(torch.sum(qv_ ** 2, dim=1)))\n",
    "\n",
    "def criterion(embeddings):\n",
    "    \n",
    "    # a batch of embeddings\n",
    "#     num_block = embeddings.size()[0] // 22\n",
    "    blocked_embeddings = embeddings.view(-1, 22, 240)\n",
    "    q_vecs = blocked_embeddings[:,0,:]\n",
    "    pos_vecs = blocked_embeddings[:,1,:]\n",
    "    neg_vecs = blocked_embeddings[:,2:,:]\n",
    "    \n",
    "    pos_scores = torch.sum(q_vecs * pos_vecs, dim=1) / (torch.sqrt(torch.sum(q_vecs ** 2, dim=1)) \\\n",
    "                                               * torch.sqrt(torch.sum(pos_vecs ** 2, dim=1)))\n",
    "\n",
    "    neg_scores = torch.sum(torch.unsqueeze(q_vecs, dim=1) * neg_vecs, dim=2) \\\n",
    "    / (torch.unsqueeze(torch.sqrt(torch.sum(q_vecs ** 2, dim=1)),dim=1) * torch.sqrt(torch.sum( neg_vecs ** 2, dim=2)))\n",
    "    neg_scores = torch.max(neg_scores, dim=1)[0]\n",
    "    \n",
    "    print (torch.mean(neg_scores - pos_scores))\n",
    "    \n",
    "    diff = neg_scores - pos_scores + 1.\n",
    "    loss = torch.mean((diff > 0).float() * diff)\n",
    "#     for i in range(num_block):\n",
    "#         block_embeddings = embeddings[ i * 22: (i + 1) * 22 ]\n",
    "#         qs = block_embeddings[0]\n",
    "#         qs_ = block_embeddings[1: 22]\n",
    "#         cos_scores = cos_sim(qs.expand(21, 240), qs_)\n",
    "#         pos_score = cos_scores[0]\n",
    "#         neg_score = torch.max(cos_scores[1:])[0]\n",
    "#         diff = neg_score - pos_score + 1 # margin=1\n",
    "#         if diff.data[0] > 0:\n",
    "#             loss += diff\n",
    "            \n",
    "    return loss #loss / num_block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EmbeddingLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, layer_type, kernel_size=None):\n",
    "        \n",
    "        super(EmbeddingLayer, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.kernel_size = kernel_size\n",
    "        \n",
    "        if layer_type == 'lstm':\n",
    "            \n",
    "            self.layer_type = 'lstm'\n",
    "            #self.title_embedding_layer = nn.LSTM(input_size, hidden_size)\n",
    "            #self.body_embedding_layer = nn.LSTM(input_size, hidden_size)\n",
    "            self.embedding_layer = nn.LSTM(input_size, hidden_size)\n",
    "            self.tanh = nn.Tanh()\n",
    "        \n",
    "        elif layer_type == 'cnn':\n",
    "            self.layer_type = 'cnn'\n",
    "            self.embedding_layer = nn.Sequential(\n",
    "                        nn.Conv1d(in_channels = 200,\n",
    "                                  out_channels = self.hidden_size,\n",
    "                                  kernel_size = kernel_size),\n",
    "                        nn.Tanh())\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return (Variable(torch.zeros(1, batch_size, self.hidden_size)), \\\n",
    "                Variable(torch.zeros(1, batch_size, self.hidden_size)))\n",
    "\n",
    "    def forward(self, title, body, title_len, body_len):\n",
    "            \n",
    "        if self.layer_type == 'lstm':\n",
    "            \n",
    "            \n",
    "            title_lstm_out, self.title_hidden = self.embedding_layer(title, (self.tanh(self.title_hidden[0]), \\\n",
    "                                                                   self.tanh(self.title_hidden[1])))\n",
    "            \n",
    "            body_lstm_out, self.body_hidden = self.embedding_layer(body, (self.tanh(self.body_hidden[0]), \\\n",
    "                                                                   self.tanh(self.body_hidden[1])))\n",
    "            \n",
    "            \n",
    "            title_mask = Variable(torch.FloatTensor(build_mask3d(title_len)))\n",
    "            title_embeddings = torch.sum(title_lstm_out * title_mask, dim=0) / torch.sum(title_mask, dim=0)\n",
    "            \n",
    "            body_mask = Variable(torch.FloatTensor(build_mask3d(body_len)))\n",
    "            body_embeddings = torch.sum(body_lstm_out * body_mask, dim=0) / torch.sum(body_mask, dim=0)\n",
    "            \n",
    "            embeddings = ( title_embeddings + body_embeddings ) / 2\n",
    "        \n",
    "            return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(layer_type, batch_size=25, num_epoch=100, id_set=train_idx_set):\n",
    "    \n",
    "    if layer_type == 'lstm':\n",
    "        embedding_layer = EmbeddingLayer(200, 240, 'lstm')\n",
    "    elif layer_type == 'cnn':\n",
    "        embedding_layer = EmbeddingLayer(200, 240, 'cnn', kernel_size=3)\n",
    "        \n",
    "    optimizer = torch.optim.Adam(embedding_layer.parameters(), lr=0.001)\n",
    "    criterion = torch.nn.MultiMarginLoss()\n",
    "    \n",
    "    qids = list(id_set.keys())\n",
    "    num_batch = len(qids) // batch_size\n",
    "    \n",
    "    for epoch in range(1, num_epoch + 1):\n",
    "        \n",
    "        for batch_idx in range(1, num_batch + 1):\n",
    "            \n",
    "            batch_x_qids = qids[ ( batch_idx - 1 ) * batch_size: batch_idx * batch_size ]\n",
    "            batch_title, batch_body, y, title_len, body_len = process_contxt_batch(batch_x_qids, train_idx_set)\n",
    "            \n",
    "            if layer_type == 'lstm':\n",
    "                embedding_layer.title_hidden = embedding_layer.init_hidden(batch_title.shape[1])\n",
    "                embedding_layer.body_hidden = embedding_layer.init_hidden(batch_body.shape[1])\n",
    "            \n",
    "            title_qs = Variable(torch.FloatTensor(batch_title))\n",
    "            body_qs = Variable(torch.FloatTensor(batch_body))\n",
    "            \n",
    "            embeddings = embedding_layer(title_qs, body_qs, title_len, body_len)\n",
    "            \n",
    "            blocked_embeddings = embeddings.view(-1, 22, 240)\n",
    "            q_vecs = blocked_embeddings[:,0,:]\n",
    "            pos_neg_vecs = blocked_embeddings[:,1:,:]\n",
    "            \n",
    "            # cosine similarity\n",
    "            scores = torch.sum(torch.unsqueeze(q_vecs, dim=1) * pos_neg_vecs, dim=2) \\\n",
    "                / (torch.unsqueeze(torch.sqrt(torch.sum(q_vecs ** 2, dim=1)), dim=1) *\\\n",
    "                   torch.sqrt(torch.sum( pos_neg_vecs ** 2, dim=2)))\n",
    "            target = Variable(torch.zeros(scores.size(0)).type(torch.LongTensor)) \n",
    "            loss = criterion(scores, target)\n",
    "            \n",
    "            target = Variable(torch.LongTensor(y))\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(embeddings, target)\n",
    "\n",
    "            print ('epoch:{}/{}, batch:{}/{}, loss:{}'.format(epoch, num_epoch, batch_idx, num_batch, loss.data[0]))\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1/10, batch:1/508, loss:0.9816491007804871\n",
      "epoch:1/10, batch:2/508, loss:0.9588315486907959\n",
      "epoch:1/10, batch:3/508, loss:0.9353597164154053\n",
      "epoch:1/10, batch:4/508, loss:0.9092717170715332\n",
      "epoch:1/10, batch:5/508, loss:0.8807125091552734\n",
      "epoch:1/10, batch:6/508, loss:0.8479568958282471\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-240-930f1b1e9c75>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'lstm'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-239-632a9c158c2e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(layer_type, batch_size, num_epoch, id_set)\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'epoch:{}/{}, batch:{}/{}, loss:{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \"\"\"\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 98\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train('lstm', num_epoch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qids = list(train_idx_set.keys())[:25]\n",
    "t, b, tl, bl = process_contxt_batch(batch_x_qids, train_idx_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = EmbeddingLayer(200, 240, 'lstm')\n",
    "embedding_layer.title_hidden = embedding_layer.init_hidden(t.shape[1])\n",
    "embedding_layer.body_hidden = embedding_layer.init_hidden(b.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "title_qs = Variable(torch.FloatTensor(t))\n",
    "body_qs = Variable(torch.FloatTensor(b))\n",
    "embeddings = embedding_layer(title_qs, body_qs, tl, bl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = Variable(torch.LongTensor([1,1] + [0]*20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.MultiMarginLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 1.0260\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(embeddings[:22], target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocked_embeddings = embeddings.view(-1, 22, 240)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_vecs = blocked_embeddings[:,0,:]\n",
    "pos_vecs = blocked_embeddings[:,1,:]\n",
    "neg_vecs = blocked_embeddings[:,2:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_scores = torch.sum(q_vecs * pos_vecs, dim=1) / (torch.sqrt(torch.sum(q_vecs ** 2, dim=1)) \\\n",
    "                                    * torch.sqrt(torch.sum(pos_vecs ** 2, dim=1)))\n",
    "neg_scores = torch.sum(torch.unsqueeze(q_vecs, dim=1) * neg_vecs, dim=2) \\\n",
    "/ (torch.unsqueeze(torch.sqrt(torch.sum(q_vecs ** 2, dim=1)), dim=1) * torch.sqrt(torch.sum( neg_vecs ** 2, dim=2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Variable containing:\n",
       "  0.9956\n",
       " [torch.FloatTensor of size 1], Variable containing:\n",
       "  0.9919\n",
       "  0.9907\n",
       "  0.9932\n",
       "  0.9865\n",
       "  0.9866\n",
       "  0.9890\n",
       "  0.9923\n",
       "  0.9835\n",
       "  0.9876\n",
       "  0.9871\n",
       "  0.9884\n",
       "  0.9846\n",
       "  0.9868\n",
       "  0.9855\n",
       "  0.9879\n",
       "  0.9884\n",
       "  0.9886\n",
       "  0.9864\n",
       "  0.9892\n",
       "  0.9893\n",
       " [torch.FloatTensor of size 20])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_scores[0], neg_scores[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_vecs = blocked_embeddings[:,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pn_vecs = blocked_embeddings[:,1:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = torch.sum(torch.unsqueeze(q_vecs, dim=1) * pn_vecs, dim=2) \\\n",
    "/ (torch.unsqueeze(torch.sqrt(torch.sum(q_vecs ** 2, dim=1)), dim=1) * torch.sqrt(torch.sum( pn_vecs ** 2, dim=2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "\n",
       "Columns 0 to 9 \n",
       " 0.9956  0.9919  0.9907  0.9932  0.9865  0.9866  0.9890  0.9923  0.9835  0.9876\n",
       " 0.9903  0.9830  0.9841  0.9789  0.9809  0.9809  0.9718  0.9705  0.9728  0.9721\n",
       " 0.9930  0.9897  0.9855  0.9897  0.9890  0.9820  0.9798  0.9854  0.9760  0.9834\n",
       " 0.9867  0.9819  0.9823  0.9876  0.9819  0.9851  0.9798  0.9759  0.9894  0.9874\n",
       " 0.9920  0.9864  0.9870  0.9819  0.9849  0.9806  0.9822  0.9809  0.9870  0.9651\n",
       " 0.9929  0.9910  0.9900  0.9828  0.9876  0.9922  0.9914  0.9906  0.9916  0.9895\n",
       " 0.9964  0.9934  0.9812  0.9916  0.9866  0.9881  0.9790  0.9898  0.9876  0.9864\n",
       " 0.9941  0.9806  0.9755  0.9790  0.9821  0.9760  0.9773  0.9871  0.9782  0.9830\n",
       " 0.9853  0.9871  0.9801  0.9824  0.9804  0.9677  0.9870  0.9842  0.9805  0.9844\n",
       " 0.9833  0.9861  0.9755  0.9871  0.9849  0.9815  0.9762  0.9896  0.9809  0.9874\n",
       " 0.9978  0.9925  0.9840  0.9818  0.9911  0.9924  0.9908  0.9857  0.9918  0.9911\n",
       " 0.9929  0.9882  0.9847  0.9853  0.9857  0.9858  0.9930  0.9908  0.9923  0.9881\n",
       " 0.9881  0.9908  0.9887  0.9878  0.9877  0.9852  0.9876  0.9875  0.9878  0.9873\n",
       " 0.9941  0.9718  0.9880  0.9866  0.9894  0.9810  0.9894  0.9928  0.9832  0.9922\n",
       " 0.9857  0.9832  0.9874  0.9907  0.9878  0.9881  0.9878  0.9877  0.9826  0.9878\n",
       " 0.9917  0.9776  0.9906  0.9915  0.9870  0.9858  0.9824  0.9867  0.9892  0.9889\n",
       " 0.9926  0.9806  0.9822  0.9721  0.9717  0.9754  0.9817  0.9772  0.9818  0.9768\n",
       " 0.9888  0.9916  0.9767  0.9896  0.9891  0.9891  0.9799  0.9851  0.9910  0.9912\n",
       " 0.9904  0.9853  0.9862  0.9824  0.9775  0.9788  0.9832  0.9874  0.9832  0.9833\n",
       " 0.9882  0.9857  0.9847  0.9908  0.9834  0.9855  0.9839  0.9839  0.9882  0.9851\n",
       " 0.9869  0.9797  0.9802  0.9834  0.9892  0.9775  0.9922  0.9856  0.9932  0.9829\n",
       " 0.9936  0.9886  0.9883  0.9793  0.9771  0.9819  0.9905  0.9857  0.9895  0.9874\n",
       " 0.9854  0.9784  0.9733  0.9772  0.9780  0.9729  0.9887  0.9831  0.9861  0.9847\n",
       " 0.9885  0.9808  0.9821  0.9888  0.9886  0.9861  0.9833  0.9899  0.9807  0.9873\n",
       " 0.9842  0.9899  0.9851  0.9729  0.9902  0.9812  0.9837  0.9799  0.9812  0.9857\n",
       " 0.9853  0.9888  0.9779  0.9866  0.9866  0.9916  0.9811  0.9771  0.9847  0.9886\n",
       " 0.9909  0.9790  0.9820  0.9906  0.9866  0.9908  0.9673  0.9829  0.9872  0.9835\n",
       " 0.9866  0.9818  0.9803  0.9812  0.9809  0.9847  0.9906  0.9926  0.9909  0.9745\n",
       " 0.9815  0.9877  0.9837  0.9788  0.9866  0.9877  0.9877  0.9812  0.9913  0.9811\n",
       " 0.9858  0.9673  0.9847  0.9902  0.9821  0.9809  0.9851  0.9812  0.9890  0.9866\n",
       " 0.9911  0.9729  0.9885  0.9861  0.9897  0.9782  0.9837  0.9857  0.9812  0.9745\n",
       " 0.9903  0.9838  0.9899  0.9748  0.9886  0.9713  0.9820  0.9838  0.9818  0.9748\n",
       " 0.9882  0.9818  0.9872  0.9782  0.9812  0.9861  0.9909  0.9908  0.9873  0.9916\n",
       " 0.9842  0.9752  0.9680  0.9735  0.9854  0.9790  0.9808  0.9716  0.9726  0.9759\n",
       " 0.9915  0.9843  0.9775  0.9790  0.9812  0.9893  0.9862  0.9866  0.9866  0.9809\n",
       " 0.9950  0.9826  0.9882  0.9827  0.9793  0.9836  0.9804  0.9810  0.9793  0.9884\n",
       " 0.9865  0.9839  0.9710  0.9776  0.9841  0.9857  0.9797  0.9875  0.9776  0.9736\n",
       " 0.9956  0.9832  0.9713  0.9912  0.9799  0.9856  0.9938  0.9798  0.9867  0.9828\n",
       " 0.9914  0.9853  0.9910  0.9896  0.9877  0.9839  0.9824  0.9932  0.9916  0.9873\n",
       " 0.9823  0.9819  0.9822  0.9857  0.9762  0.9808  0.9783  0.9827  0.9859  0.9828\n",
       " 0.9927  0.9923  0.9855  0.9806  0.9878  0.9864  0.9898  0.9923  0.9790  0.9872\n",
       "\n",
       "Columns 10 to 19 \n",
       " 0.9871  0.9884  0.9846  0.9868  0.9855  0.9879  0.9884  0.9886  0.9864  0.9892\n",
       " 0.9819  0.9744  0.9797  0.9801  0.9940  0.9867  0.9652  0.9769  0.9825  0.9850\n",
       " 0.9878  0.9796  0.9871  0.9772  0.9796  0.9794  0.9899  0.9790  0.9796  0.9872\n",
       " 0.9782  0.9870  0.9811  0.9772  0.9798  0.9794  0.9897  0.9845  0.9764  0.9835\n",
       " 0.9838  0.9855  0.9809  0.9844  0.9840  0.9851  0.9771  0.9920  0.9783  0.9895\n",
       " 0.9837  0.9914  0.9886  0.9836  0.9745  0.9870  0.9915  0.9818  0.9917  0.9907\n",
       " 0.9838  0.9906  0.9919  0.9866  0.9802  0.9887  0.9832  0.9741  0.9787  0.9827\n",
       " 0.9801  0.9771  0.9771  0.9769  0.9858  0.9782  0.9729  0.9769  0.9754  0.9768\n",
       " 0.9775  0.9831  0.9808  0.9747  0.9785  0.9808  0.9747  0.9750  0.9770  0.9747\n",
       " 0.9708  0.9786  0.9779  0.9766  0.9832  0.9849  0.9662  0.9838  0.9839  0.9799\n",
       " 0.9840  0.9930  0.9871  0.9818  0.9883  0.9901  0.9910  0.9869  0.9840  0.9929\n",
       " 0.9833  0.9882  0.9694  0.9882  0.9863  0.9885  0.9847  0.9882  0.9899  0.9887\n",
       " 0.9866  0.9922  0.9929  0.9913  0.9886  0.9790  0.9852  0.9861  0.9866  0.9813\n",
       " 0.9903  0.9848  0.9848  0.9918  0.9911  0.9826  0.9878  0.9853  0.9894  0.9908\n",
       " 0.9877  0.9877  0.9832  0.9884  0.9902  0.9846  0.9853  0.9735  0.9873  0.9928\n",
       " 0.9951  0.9906  0.9824  0.9828  0.9926  0.9914  0.9903  0.9920  0.9873  0.9920\n",
       " 0.9730  0.9798  0.9780  0.9706  0.9792  0.9819  0.9763  0.9794  0.9730  0.9782\n",
       " 0.9893  0.9862  0.9876  0.9874  0.9871  0.9847  0.9877  0.9846  0.9828  0.9905\n",
       " 0.9877  0.9872  0.9853  0.9877  0.9884  0.9809  0.9872  0.9815  0.9872  0.9804\n",
       " 0.9839  0.9872  0.9825  0.9908  0.9786  0.9771  0.9874  0.9797  0.9837  0.9830\n",
       " 0.9879  0.9897  0.9811  0.9908  0.9874  0.9851  0.9860  0.9893  0.9860  0.9932\n",
       " 0.9886  0.9838  0.9859  0.9814  0.9786  0.9895  0.9874  0.9852  0.9893  0.9882\n",
       " 0.9821  0.9877  0.9812  0.9771  0.9812  0.9837  0.9779  0.9793  0.9810  0.9917\n",
       " 0.9873  0.9918  0.9713  0.9916  0.9897  0.9866  0.9810  0.9835  0.9890  0.9916\n",
       " 0.9851  0.9866  0.9917  0.9755  0.9706  0.9873  0.9835  0.9877  0.9877  0.9868\n",
       " 0.9830  0.9906  0.9713  0.9896  0.9916  0.9830  0.9814  0.9889  0.9745  0.9814\n",
       " 0.9713  0.9861  0.9873  0.9830  0.9820  0.9917  0.9807  0.9888  0.9783  0.9772\n",
       " 0.9812  0.9892  0.9897  0.9713  0.9812  0.9913  0.9871  0.9886  0.9812  0.9851\n",
       " 0.9812  0.9790  0.9851  0.9837  0.9888  0.9748  0.9873  0.9818  0.9818  0.9713\n",
       " 0.9829  0.9816  0.9888  0.9729  0.9868  0.9811  0.9902  0.9818  0.9808  0.9888\n",
       " 0.9888  0.9908  0.9847  0.9863  0.9909  0.9837  0.9771  0.9906  0.9799  0.9812\n",
       " 0.9887  0.9873  0.9783  0.9811  0.9812  0.9801  0.9811  0.9771  0.9748  0.9706\n",
       " 0.9908  0.9916  0.9866  0.9866  0.9908  0.9863  0.9866  0.9808  0.9896  0.9918\n",
       " 0.9761  0.9820  0.9822  0.9761  0.9791  0.9707  0.9766  0.9792  0.9765  0.9762\n",
       " 0.9811  0.9847  0.9808  0.9755  0.9826  0.9705  0.9848  0.9836  0.9729  0.9925\n",
       " 0.9864  0.9729  0.9867  0.9867  0.9884  0.9794  0.9916  0.9790  0.9508  0.9803\n",
       " 0.9852  0.9787  0.9764  0.9864  0.9854  0.9787  0.9829  0.9852  0.9796  0.9826\n",
       " 0.9909  0.9730  0.9743  0.9867  0.9805  0.9874  0.9791  0.9906  0.9879  0.9912\n",
       " 0.9787  0.9899  0.9865  0.9856  0.9896  0.9813  0.9894  0.9865  0.9866  0.9799\n",
       " 0.9860  0.9810  0.9830  0.9824  0.9800  0.9848  0.9840  0.9824  0.9783  0.9868\n",
       " 0.9854  0.9846  0.9893  0.9916  0.9903  0.9870  0.9891  0.9857  0.9876  0.9790\n",
       "\n",
       "Columns 20 to 20 \n",
       " 0.9893\n",
       " 0.9780\n",
       " 0.9779\n",
       " 0.9760\n",
       " 0.9915\n",
       " 0.9877\n",
       " 0.9893\n",
       " 0.9798\n",
       " 0.9782\n",
       " 0.9813\n",
       " 0.9809\n",
       " 0.9861\n",
       " 0.9926\n",
       " 0.9932\n",
       " 0.9898\n",
       " 0.9881\n",
       " 0.9736\n",
       " 0.9881\n",
       " 0.9860\n",
       " 0.9786\n",
       " 0.9795\n",
       " 0.9847\n",
       " 0.9808\n",
       " 0.9829\n",
       " 0.9875\n",
       " 0.9809\n",
       " 0.9847\n",
       " 0.9877\n",
       " 0.9793\n",
       " 0.9830\n",
       " 0.9877\n",
       " 0.9855\n",
       " 0.9899\n",
       " 0.9808\n",
       " 0.9878\n",
       " 0.9775\n",
       " 0.9840\n",
       " 0.9853\n",
       " 0.9917\n",
       " 0.9762\n",
       " 0.9885\n",
       "[torch.FloatTensor of size 41x21]"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = torch.nn.MultiMarginLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.9469\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = Variable(torch.zeros(scores.size(0)).type(torch.LongTensor)) \n",
    "criterion(scores, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
