{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import string\n",
    "import numpy as np; np.random.seed(7)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_map = {}\n",
    "with open('data/vectors_pruned.200.txt', 'r') as src:\n",
    "    src = src.read().strip().split('\\n')\n",
    "    for line in src:\n",
    "        wv = line.strip().split(' ')\n",
    "        word = wv.pop(0)\n",
    "        w2v_map[word] = np.array(list(map(float, wv)))\n",
    "\n",
    "w2i_map = {}\n",
    "for i, key in enumerate(w2v_map.keys()):\n",
    "    w2i_map[key] = i\n",
    "\n",
    "w2v_matrix = np.zeros(( len((w2v_map.keys())), 200 ))\n",
    "counter = 0\n",
    "for _, val in w2v_map.items():\n",
    "    w2v_matrix[counter] = val\n",
    "    counter += 1\n",
    "\n",
    "def w2v(w):\n",
    "    return w2v_matrix[w2i_map[w]]\n",
    "\n",
    "def sen2w(sen):\n",
    "    processed = []\n",
    "    sen = sen.strip().split()\n",
    "    if len(sen) > 100:\n",
    "        sen = sen[:100]\n",
    "    for w in sen:\n",
    "        #ignore date\n",
    "        if re.match(r'\\d{1,}-\\d{1,}-\\d{1,}', w):\n",
    "            continue\n",
    "        if re.match(r'\\d{1,}:\\d{1,}', w):\n",
    "            continue\n",
    "        \n",
    "        if w in w2i_map:\n",
    "            processed += [w]\n",
    "        else:\n",
    "            separated = re.findall(r\"[^\\W\\d_]+|\\d+|[=`%$\\^\\-@;\\[&_*>\\].<~|+\\d+]\", w)\n",
    "            if len(set(separated)) == 1:\n",
    "                continue\n",
    "            if separated.count('*') > 3 or separated.count('=') > 3:\n",
    "                continue\n",
    "            for separate_w in separated:\n",
    "                if separate_w in w2i_map:\n",
    "                    processed += [separate_w]\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fixed context len = 125\n",
    "context_repre = {}\n",
    "with open('data/text_tokenized.txt', 'r') as src:\n",
    "    src = src.read().strip().split('\\n')\n",
    "    for line in src:\n",
    "        context = line.strip().split('\\t')\n",
    "        qid = context.pop(0)\n",
    "        if len(context) == 1:\n",
    "            context_repre[int(qid)] = {'t': sen2w(context[0]), 'b': None}\n",
    "        else:\n",
    "            context_repre[int(qid)] = {'t':sen2w(context[0]), 'b': sen2w(context[1])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_set_pair_with_idx(df):\n",
    "    idx_set = {}\n",
    "    for idx, row in df.iterrows():\n",
    "        idx_set[row['Q']] = {'pos': np.array(list(map(int, row['Q+'].split(' ')))), \\\n",
    "                             'neg': np.array(list(map(int, row['Q-'].split(' '))))}\n",
    "    return idx_set\n",
    "\n",
    "train_df = pd.read_csv('data/train_random.txt', header=None, delimiter='\\t', names=['Q','Q+','Q-'])\n",
    "train_idx_set = build_set_pair_with_idx(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def contxt2vec(title, body=None):\n",
    "    \n",
    "    if body == None:\n",
    "        body = []\n",
    "    \n",
    "    title_v = np.zeros( (len(title), 200) )\n",
    "    \n",
    "    for i, t in enumerate(title):\n",
    "        title_v[i] = w2v(t)\n",
    "    \n",
    "    if len(body) > 0:\n",
    "        body_v = np.zeros( (len(body), 200) )\n",
    "        for i, b in enumerate(body):\n",
    "            body_v[i] = w2v(b)\n",
    "    \n",
    "        return title_v, body_v\n",
    "    \n",
    "    return title_v, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_contxt_batch(qids, idx_set):\n",
    "    \n",
    "    batch_title, batch_body = [], []\n",
    "    max_title_len, max_body_len = 0, 0\n",
    "    title_len, body_len = [], []\n",
    "    counter = 0\n",
    "    \n",
    "    for qid in qids:\n",
    "        \n",
    "        q_title, q_body = context_repre[qid]['t'], context_repre[qid]['b']\n",
    "        q_pos = idx_set[qid]['pos']\n",
    "\n",
    "        for qid_pos in q_pos:\n",
    "\n",
    "            # query Q\n",
    "            title_len += [len(q_title)]\n",
    "            batch_title += [ q_title ]\n",
    "            max_title_len = max(max_title_len, len(q_title))\n",
    "            if not q_body:\n",
    "                body_len += [len(q_title)]\n",
    "                batch_body += [ q_title ]\n",
    "            else:\n",
    "                batch_body += [ q_body ]\n",
    "                body_len += [len(q_body)]\n",
    "                max_body_len = max(max_body_len, len(q_body))\n",
    "                \n",
    "            # pos Q\n",
    "            title, body = context_repre[qid_pos]['t'], context_repre[qid_pos]['b']\n",
    "            title_len += [len(title)]\n",
    "            batch_title += [ title ]\n",
    "            max_title_len = max(max_title_len, len(title))\n",
    "            if not body:\n",
    "                body_len += [len(title)]\n",
    "                batch_body += [ title ]\n",
    "            else:\n",
    "                batch_body += [ body ]\n",
    "                body_len += [len(body)]\n",
    "                max_body_len = max(max_body_len, len(body))\n",
    "        \n",
    "            # neg Q\n",
    "            \n",
    "            q_neg = idx_set[qid]['neg']\n",
    "            q_neg_sample_indices = np.random.choice(range(100), size=20)\n",
    "            q_random_neg = q_neg[q_neg_sample_indices]\n",
    "            \n",
    "            for qid_neg in q_random_neg:\n",
    "                title, body = context_repre[qid_neg]['t'], context_repre[qid_neg]['b']\n",
    "                title_len += [len(title)]\n",
    "                batch_title += [ title ]\n",
    "                max_title_len = max(max_title_len, len(title))\n",
    "                if not body:\n",
    "                    body_len += [len(title)]\n",
    "                    batch_body += [ title ]\n",
    "                else:\n",
    "                    batch_body += [ body ]\n",
    "                    body_len += [len(body)]\n",
    "                    max_body_len = max(max_body_len, len(body))\n",
    "    \n",
    "    # (max_seq_len, batch_size, feature_len)\n",
    "    padded_batch_title = np.zeros(( max_title_len, len(batch_title), 200)) \n",
    "    padded_batch_body = np.zeros(( max_body_len, len(batch_body),  200))\n",
    "    \n",
    "    for i, (title, body) in enumerate(zip(batch_title, batch_body)):\n",
    "        title_repre, body_repre = contxt2vec(title, body)\n",
    "        padded_batch_title[:title_len[i], i] = title_repre\n",
    "        padded_batch_body[:body_len[i], i] = body_repre\n",
    "    \n",
    "    return padded_batch_title, padded_batch_body, np.array(title_len).reshape(-1,1), np.array(body_len).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_mask(seq_len):\n",
    "    mask = []\n",
    "    for i, s in enumerate(seq_len):\n",
    "        s_mask = np.zeros((np.max(seq_len), 1))\n",
    "        s_mask[:int(s)] = np.ones((int(s), 1))\n",
    "        mask += [s_mask]\n",
    "    return mask\n",
    "\n",
    "def cos_sim(qv, qv_):\n",
    "    return torch.sum(qv * qv_, dim=1) / (torch.sqrt(torch.sum(qv ** 2, dim=1)) * torch.sqrt(torch.sum(qv_ ** 2, dim=1)))\n",
    "\n",
    "def criterion(embeddings):\n",
    "    \n",
    "    # a batch of embeddings\n",
    "    num_block = embeddings.size()[0] // 22\n",
    "    loss = 0\n",
    "    for i in range(num_block):\n",
    "        block_embeddings = embeddings[ i * 22: (i + 1) * 22 ]\n",
    "        qs = block_embeddings[0]\n",
    "        qs_ = block_embeddings[1: 22]\n",
    "        cos_scores = cos_sim(qs.expand(21, 240), qs_)\n",
    "        print (i, cos_scores)\n",
    "        pos_score = cos_scores[0]\n",
    "        neg_score = torch.max(cos_scores[1:])\n",
    "        diff = neg_score - pos_score + 1 # margin=1\n",
    "        if diff.data[0] > 0:\n",
    "            loss += diff\n",
    "            \n",
    "    return loss / num_block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EmbeddingLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, layer_type, kernel_size=None):\n",
    "        \n",
    "        super(EmbeddingLayer, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.kernel_size = kernel_size\n",
    "        \n",
    "        if layer_type == 'lstm':\n",
    "            self.layer_type = 'lstm'\n",
    "            self.embedding_layer = nn.LSTM(input_size, hidden_size)\n",
    "            self.tanh = nn.Tanh()\n",
    "        elif layer_type == 'cnn':\n",
    "            self.layer_type = 'cnn'\n",
    "            self.embedding_layer = nn.Sequential(\n",
    "                        nn.Conv1d(in_channels = 200,\n",
    "                                  out_channels = self.hidden_size,\n",
    "                                  kernel_size = kernel_size),\n",
    "                        nn.Tanh())\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return (Variable(torch.zeros(1, batch_size, self.hidden_size)), \\\n",
    "                Variable(torch.zeros(1, batch_size, self.hidden_size)))\n",
    "\n",
    "    def forward(self, context, seq_len):\n",
    "            \n",
    "        if self.layer_type == 'lstm':\n",
    "            \n",
    "            \n",
    "            lstm_out, self.hidden = self.embedding_layer(context, (self.tanh(self.hidden[0]), \\\n",
    "                                                                   self.tanh(self.hidden[1])))\n",
    "            \n",
    "            #mask = build_mask(seq_len)\n",
    "            #mask = Variable(torch.FloatTensor(mask)).view(lstm_out.size()[0], -1, 1)\n",
    "            #embeddings = torch.sum(lstm_out * mask, dim=0) / ( torch.sum(mask, dim=0) + 1e-8)\n",
    "\n",
    "            mask = build_mask(seq_len)\n",
    "            embeddings = torch.sum(lstm_out.view(-1, lstm_out.size()[0], self.hidden_size) \\\n",
    "                                   * Variable(torch.FloatTensor(mask)), dim=1) \\\n",
    "                / Variable(torch.FloatTensor(np.sum(mask, axis=1)))\n",
    "        \n",
    "#         elif self.layer_type == 'cnn':\n",
    "            \n",
    "#             cnn_out = self.embedding_layer(context.view(-1, context.size()[2], context.size()[1]))\n",
    "#             mask = build_mask(seq_len - self.kernel_size + 1, 124 - self.kernel_size + 1)\n",
    "#             embeddings = torch.sum(cnn_out.view(-1, 124 - self.kernel_size + 1, self.hidden_size) \\\n",
    "#                                    * Variable(torch.FloatTensor(mask)), dim=1) \\\n",
    "#                 / Variable(torch.FloatTensor(np.sum(mask, axis=1)))\n",
    "        \n",
    "            return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(layer_type, batch_size=25, num_epoch=100):\n",
    "    \n",
    "    if layer_type == 'lstm':\n",
    "        embedding_layer = EmbeddingLayer(200, 240, 'lstm')\n",
    "    elif layer_type == 'cnn':\n",
    "        embedding_layer = EmbeddingLayer(200, 240, 'cnn', kernel_size=3)\n",
    "        \n",
    "    optimizer = torch.optim.Adam(embedding_layer.parameters(), lr=0.001)\n",
    "    \n",
    "    qids = list(train_idx_set.keys())\n",
    "    num_batch = len(qids) // batch_size\n",
    "    \n",
    "    for epoch in range(1, num_epoch + 1):\n",
    "        \n",
    "        for batch_idx in range(1, num_batch + 1):\n",
    "            \n",
    "            batch_x_qids = qids[ ( batch_idx - 1 ) * batch_size: batch_idx * batch_size]\n",
    "            start = time.time()\n",
    "            print ('processing batch {}'.format(batch_idx))\n",
    "            batch_title, batch_body, title_len, body_len = process_contxt_batch(batch_x_qids, train_idx_set)\n",
    "            print ('processing batch costs:', time.time() - start)\n",
    "            \n",
    "            if layer_type == 'lstm':\n",
    "                embedding_layer.hidden = embedding_layer.init_hidden(batch_title.shape[1])\n",
    "            \n",
    "            start = time.time()\n",
    "            \n",
    "            title_qs = Variable(torch.FloatTensor(batch_title))\n",
    "            body_qs = Variable(torch.FloatTensor(batch_body))\n",
    "            \n",
    "            title_embeddings = embedding_layer(title_qs, title_len)\n",
    "            body_embeddings = embedding_layer(body_qs, body_len)\n",
    "            \n",
    "            contxt_embeddings = ( title_embeddings + body_embeddings ) / 2\n",
    "            print (contxt_embeddings[-22:])\n",
    "            print (contxt_embeddings[-22:][0])\n",
    "            print (contxt_embeddings[-22:][1:])\n",
    "            print ('embedding costs:', time.time() - start)\n",
    "            print (batch_x_qids)\n",
    "            \n",
    "#           optimizer.zero_grad()\n",
    "            loss = criterion(contxt_embeddings)\n",
    "            loss = criterion(contxt_embeddings[-22:])\n",
    "            ewif\n",
    "            print ('-------------------------------')\n",
    "            print ('epoch:{}/{}, batch:{}/{}, loss:{}'.format(epoch, num_epoch, batch_idx, num_batch, loss.data[0]))\n",
    "            print ('-------------------------------')\n",
    "            start = time.time()\n",
    "#             loss.backward()\n",
    "            print ('backprop costs:', time.time() - start)\n",
    "            #optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing batch 1\n",
      "processing batch costs: 0.7232809066772461\n",
      "Variable containing:\n",
      "1.00000e-02 *\n",
      " 5.9551  2.9252  1.3279  ...  -1.4215 -3.8446  1.5431\n",
      " 5.9538  2.9249  1.3295  ...  -1.4224 -3.8450  1.5441\n",
      " 5.9763  2.9276  1.2523  ...  -1.3533 -3.8471  1.4640\n",
      "          ...             ⋱             ...          \n",
      " 5.9539  2.9248  1.3295  ...  -1.4225 -3.8450  1.5440\n",
      " 5.9538  2.9249  1.3295  ...  -1.4225 -3.8450  1.5440\n",
      " 5.9537  2.9249  1.3294  ...  -1.4225 -3.8449  1.5440\n",
      "[torch.FloatTensor of size 22x240]\n",
      "\n",
      "Variable containing:\n",
      "1.00000e-02 *\n",
      "  5.9551\n",
      "  2.9252\n",
      "  1.3279\n",
      " -0.1030\n",
      "  2.2403\n",
      " -4.1920\n",
      " -4.8045\n",
      "  3.3753\n",
      "  0.0680\n",
      " -2.2641\n",
      "  5.2738\n",
      " -3.9334\n",
      "  0.3960\n",
      " -0.7644\n",
      " -0.9164\n",
      "  1.0876\n",
      " -2.7888\n",
      " -1.1460\n",
      " -0.5193\n",
      " -0.0699\n",
      "  4.2056\n",
      "  2.8347\n",
      " -0.4602\n",
      " -2.9824\n",
      " -0.7329\n",
      " -0.4016\n",
      "  1.3974\n",
      "  1.0244\n",
      "  1.8920\n",
      " -3.3099\n",
      " -5.2568\n",
      " -2.1121\n",
      " -3.6593\n",
      " -0.5019\n",
      "  0.4313\n",
      " -4.6740\n",
      " -1.1863\n",
      " -4.7807\n",
      "  2.9710\n",
      " -4.6896\n",
      " -1.3610\n",
      " -0.6176\n",
      "  2.0398\n",
      " -4.9888\n",
      "  0.4952\n",
      "  1.2210\n",
      " -0.2342\n",
      "  1.8112\n",
      " -2.7125\n",
      " -2.7167\n",
      "  1.0628\n",
      " -1.1816\n",
      "  3.0312\n",
      "  2.0534\n",
      " -4.9037\n",
      " -1.6059\n",
      "  4.0080\n",
      "  1.1237\n",
      "  0.2618\n",
      "  4.0468\n",
      " -1.5100\n",
      " -1.4935\n",
      " -4.5737\n",
      "  3.5029\n",
      "  0.3712\n",
      "  1.6071\n",
      " -2.4277\n",
      " -2.3282\n",
      "  0.3793\n",
      "  5.6551\n",
      " -4.4767\n",
      " -3.7131\n",
      "  6.0759\n",
      "  2.3227\n",
      " -1.5371\n",
      "  0.3721\n",
      " -1.1320\n",
      "  2.0403\n",
      "  1.1173\n",
      " -0.0633\n",
      "  4.1602\n",
      "  5.3701\n",
      " -2.5973\n",
      "  0.8652\n",
      "  0.0837\n",
      " -1.4840\n",
      "  2.3523\n",
      "  0.6360\n",
      "  4.6321\n",
      " -3.0613\n",
      "  3.2352\n",
      "  3.9566\n",
      "  2.8696\n",
      "  0.5510\n",
      " -2.4663\n",
      " -4.0158\n",
      " -0.3926\n",
      " -5.1788\n",
      "  0.6013\n",
      "  1.6245\n",
      " -1.0742\n",
      "  2.3506\n",
      "  3.9831\n",
      " -2.4585\n",
      " -2.5738\n",
      " -6.3700\n",
      "  0.5280\n",
      " -2.0032\n",
      "  3.9112\n",
      "  2.9074\n",
      " -2.3678\n",
      "  1.6089\n",
      " -1.2621\n",
      "  0.7409\n",
      "  3.8750\n",
      "  0.0746\n",
      "  0.7558\n",
      "  4.2813\n",
      "  1.1100\n",
      " -3.6626\n",
      " -4.5766\n",
      " -0.6735\n",
      " -2.5412\n",
      "  0.8227\n",
      " -0.8925\n",
      "  0.3113\n",
      " -1.6799\n",
      " -0.5733\n",
      " -1.1359\n",
      " -2.3344\n",
      " -0.9678\n",
      " -0.6697\n",
      " -0.7593\n",
      " -0.4893\n",
      " -3.0342\n",
      "  1.1175\n",
      "  0.3918\n",
      " -1.3667\n",
      "  1.8488\n",
      " -2.1025\n",
      " -1.0327\n",
      "  0.8843\n",
      "  2.2870\n",
      "  0.7734\n",
      " -0.3504\n",
      "  3.0550\n",
      "  5.8682\n",
      " -2.5170\n",
      "  3.5869\n",
      " -1.3776\n",
      " -3.2872\n",
      " -2.4093\n",
      " -2.4599\n",
      "  0.4332\n",
      "  0.9514\n",
      "  0.1299\n",
      "  0.5079\n",
      "  3.3836\n",
      " -1.0235\n",
      " -2.9286\n",
      "  3.1360\n",
      "  2.7935\n",
      "  0.2051\n",
      "  1.1110\n",
      " -1.5091\n",
      "  1.5321\n",
      " -3.1262\n",
      " -2.6694\n",
      "  4.3385\n",
      "  2.1324\n",
      "  2.4804\n",
      " -1.7392\n",
      "  1.5480\n",
      " -3.9486\n",
      "  4.8642\n",
      "  4.4796\n",
      "  4.0998\n",
      "  4.1960\n",
      "  5.9860\n",
      "  0.4330\n",
      "  0.2852\n",
      " -2.4289\n",
      " -0.8776\n",
      "  2.0720\n",
      "  3.8015\n",
      "  1.2594\n",
      "  2.5509\n",
      " -2.8359\n",
      "  0.0986\n",
      "  1.8329\n",
      " -5.5674\n",
      "  2.1443\n",
      "  5.7527\n",
      " -6.1587\n",
      "  2.1539\n",
      " -5.0390\n",
      "  2.6742\n",
      " -2.2169\n",
      "  2.8634\n",
      " -0.0287\n",
      "  1.3319\n",
      "  3.6465\n",
      "  2.6413\n",
      "  0.1796\n",
      " -3.9019\n",
      "  1.8002\n",
      "  1.9427\n",
      " -0.9820\n",
      " -3.5501\n",
      " -0.7754\n",
      " -1.2167\n",
      "  5.8199\n",
      " -0.4906\n",
      " -2.9758\n",
      " -1.4429\n",
      " -1.3387\n",
      "  1.2152\n",
      " -0.7820\n",
      " -2.2500\n",
      "  2.8362\n",
      " -2.1913\n",
      "  3.0369\n",
      " -0.8975\n",
      "  0.1906\n",
      " -3.1492\n",
      " -0.6698\n",
      " -0.2331\n",
      "  0.1685\n",
      "  3.3735\n",
      "  5.4774\n",
      "  0.8751\n",
      "  1.9111\n",
      "  6.0481\n",
      "  2.8450\n",
      "  0.8017\n",
      " -4.6562\n",
      "  4.2075\n",
      " -1.4215\n",
      " -3.8446\n",
      "  1.5431\n",
      "[torch.FloatTensor of size 240]\n",
      "\n",
      "Variable containing:\n",
      "1.00000e-02 *\n",
      " 5.9538  2.9249  1.3295  ...  -1.4224 -3.8450  1.5441\n",
      " 5.9763  2.9276  1.2523  ...  -1.3533 -3.8471  1.4640\n",
      " 5.9538  2.9248  1.3296  ...  -1.4224 -3.8451  1.5440\n",
      "          ...             ⋱             ...          \n",
      " 5.9539  2.9248  1.3295  ...  -1.4225 -3.8450  1.5440\n",
      " 5.9538  2.9249  1.3295  ...  -1.4225 -3.8450  1.5440\n",
      " 5.9537  2.9249  1.3294  ...  -1.4225 -3.8449  1.5440\n",
      "[torch.FloatTensor of size 21x240]\n",
      "\n",
      "embedding costs: 16.510040998458862\n",
      "[262144, 491522, 240299, 196614, 360457, 425996, 163842, 393230, 393231, 491536, 327698, 121116, 397172, 294938, 32795, 32798, 33, 34, 229411, 37, 294951, 496988, 196650, 294955, 73107]\n",
      "0 Variable containing:\n",
      " 0.9940\n",
      " 0.9954\n",
      " 0.9918\n",
      " 0.9895\n",
      " 0.9919\n",
      " 0.9897\n",
      " 0.9920\n",
      " 0.9898\n",
      " 0.9903\n",
      " 0.9887\n",
      " 0.9877\n",
      " 0.9893\n",
      " 0.9872\n",
      " 0.9885\n",
      " 0.9903\n",
      " 0.9883\n",
      " 0.9872\n",
      " 0.9895\n",
      " 0.9877\n",
      " 0.9839\n",
      " 0.9865\n",
      "[torch.FloatTensor of size 21]\n",
      "\n",
      "1 Variable containing:\n",
      " 0.9970\n",
      " 0.9920\n",
      " 0.9968\n",
      " 0.9966\n",
      " 0.9944\n",
      " 0.9965\n",
      " 0.9963\n",
      " 0.9930\n",
      " 0.9964\n",
      " 0.9954\n",
      " 0.9937\n",
      " 0.9926\n",
      " 0.9953\n",
      " 0.9930\n",
      " 0.9922\n",
      " 0.9912\n",
      " 0.9929\n",
      " 0.9918\n",
      " 0.9952\n",
      " 0.9955\n",
      " 0.9944\n",
      "[torch.FloatTensor of size 21]\n",
      "\n",
      "2 Variable containing:\n",
      " 0.9915\n",
      " 0.9897\n",
      " 0.9947\n",
      " 0.9929\n",
      " 0.9920\n",
      " 0.9960\n",
      " 0.9942\n",
      " 0.9942\n",
      " 0.9936\n",
      " 0.9877\n",
      " 0.9935\n",
      " 0.9831\n",
      " 0.9936\n",
      " 0.9908\n",
      " 0.9901\n",
      " 0.9910\n",
      " 0.9913\n",
      " 0.9917\n",
      " 0.9933\n",
      " 0.9939\n",
      " 0.9910\n",
      "[torch.FloatTensor of size 21]\n",
      "\n",
      "3 Variable containing:\n",
      " 0.9944\n",
      " 0.9865\n",
      " 0.9913\n",
      " 0.9898\n",
      " 0.9915\n",
      " 0.9931\n",
      " 0.9944\n",
      " 0.9917\n",
      " 0.9930\n",
      " 0.9901\n",
      " 0.9915\n",
      " 0.9911\n",
      " 0.9958\n",
      " 0.9906\n",
      " 0.9925\n",
      " 0.9848\n",
      " 0.9948\n",
      " 0.9922\n",
      " 0.9944\n",
      " 0.9924\n",
      " 0.9880\n",
      "[torch.FloatTensor of size 21]\n",
      "\n",
      "4 Variable containing:\n",
      " 0.9951\n",
      " 0.9923\n",
      " 0.9949\n",
      " 0.9944\n",
      " 0.9906\n",
      " 0.9934\n",
      " 0.9957\n",
      " 0.9927\n",
      " 0.9964\n",
      " 0.9935\n",
      " 0.9945\n",
      " 0.9939\n",
      " 0.9944\n",
      " 0.9922\n",
      " 0.9949\n",
      " 0.9946\n",
      " 0.9948\n",
      " 0.9946\n",
      " 0.9943\n",
      " 0.9941\n",
      " 0.9932\n",
      "[torch.FloatTensor of size 21]\n",
      "\n",
      "5 Variable containing:\n",
      " 0.9960\n",
      " 0.9962\n",
      " 0.9978\n",
      " 0.9959\n",
      " 0.9966\n",
      " 0.9966\n",
      " 0.9952\n",
      " 0.9957\n",
      " 0.9966\n",
      " 0.9943\n",
      " 0.9965\n",
      " 0.9956\n",
      " 0.9965\n",
      " 0.9972\n",
      " 0.9961\n",
      " 0.9959\n",
      " 0.9976\n",
      " 0.9915\n",
      " 0.9964\n",
      " 0.9947\n",
      " 0.9952\n",
      "[torch.FloatTensor of size 21]\n",
      "\n",
      "6 Variable containing:\n",
      " 0.9958\n",
      " 0.9967\n",
      " 0.9965\n",
      " 0.9937\n",
      " 0.9965\n",
      " 0.9968\n",
      " 0.9966\n",
      " 0.9965\n",
      " 0.9945\n",
      " 0.9941\n",
      " 0.9945\n",
      " 0.9974\n",
      " 0.9910\n",
      " 0.9966\n",
      " 0.9918\n",
      " 0.9950\n",
      " 0.9962\n",
      " 0.9956\n",
      " 0.9982\n",
      " 0.9960\n",
      " 0.9976\n",
      "[torch.FloatTensor of size 21]\n",
      "\n",
      "7 Variable containing:\n",
      " 0.9975\n",
      " 0.9956\n",
      " 0.9953\n",
      " 0.9966\n",
      " 0.9953\n",
      " 0.9971\n",
      " 0.9955\n",
      " 0.9964\n",
      " 0.9966\n",
      " 0.9961\n",
      " 0.9963\n",
      " 0.9932\n",
      " 0.9921\n",
      " 0.9947\n",
      " 0.9948\n",
      " 0.9965\n",
      " 0.9963\n",
      " 0.9955\n",
      " 0.9956\n",
      " 0.9951\n",
      " 0.9959\n",
      "[torch.FloatTensor of size 21]\n",
      "\n",
      "8 Variable containing:\n",
      " 0.9942\n",
      " 0.9960\n",
      " 0.9939\n",
      " 0.9934\n",
      " 0.9956\n",
      " 0.9963\n",
      " 0.9961\n",
      " 0.9959\n",
      " 0.9958\n",
      " 0.9955\n",
      " 0.9932\n",
      " 0.9925\n",
      " 0.9968\n",
      " 0.9925\n",
      " 0.9947\n",
      " 0.9965\n",
      " 0.9945\n",
      " 0.9943\n",
      " 0.9957\n",
      " 0.9946\n",
      " 0.9951\n",
      "[torch.FloatTensor of size 21]\n",
      "\n",
      "9 Variable containing:\n",
      " 0.9967\n",
      " 0.9933\n",
      " 0.9964\n",
      " 0.9916\n",
      " 0.9973\n",
      " 0.9978\n",
      " 0.9959\n",
      " 0.9960\n",
      " 0.9946\n",
      " 0.9963\n",
      " 0.9958\n",
      " 0.9922\n",
      " 0.9968\n",
      " 0.9925\n",
      " 0.9971\n",
      " 0.9969\n",
      " 0.9949\n",
      " 0.9955\n",
      " 0.9963\n",
      " 0.9967\n",
      " 0.9973\n",
      "[torch.FloatTensor of size 21]\n",
      "\n",
      "10 Variable containing:\n",
      " 0.9973\n",
      " 0.9956\n",
      " 0.9972\n",
      " 0.9970\n",
      " 0.9969\n",
      " 0.9973\n",
      " 0.9960\n",
      " 0.9973\n",
      " 0.9973\n",
      " 0.9959\n",
      " 0.9975\n",
      " 0.9977\n",
      " 0.9969\n",
      " 0.9969\n",
      " 0.9974\n",
      " 0.9985\n",
      " 0.9936\n",
      " 0.9975\n",
      " 0.9968\n",
      " 0.9954\n",
      " 0.9961\n",
      "[torch.FloatTensor of size 21]\n",
      "\n",
      "11 Variable containing:\n",
      " 0.9971\n",
      " 0.9957\n",
      " 0.9981\n",
      " 0.9967\n",
      " 0.9967\n",
      " 0.9975\n",
      " 0.9970\n",
      " 0.9953\n",
      " 0.9960\n",
      " 0.9962\n",
      " 0.9971\n",
      " 0.9941\n",
      " 0.9976\n",
      " 0.9941\n",
      " 0.9983\n",
      " 0.9946\n",
      " 0.9970\n",
      " 0.9958\n",
      " 0.9961\n",
      " 0.9968\n",
      " 0.9935\n",
      "[torch.FloatTensor of size 21]\n",
      "\n",
      "12 Variable containing:\n",
      " 0.9967\n",
      " 0.9956\n",
      " 0.9982\n",
      " 0.9946\n",
      " 0.9959\n",
      " 0.9971\n",
      " 0.9979\n",
      " 0.9916\n",
      " 0.9975\n",
      " 0.9976\n",
      " 0.9978\n",
      " 0.9986\n",
      " 0.9973\n",
      " 0.9955\n",
      " 0.9980\n",
      " 0.9981\n",
      " 0.9982\n",
      " 0.9976\n",
      " 0.9985\n",
      " 0.9976\n",
      " 0.9957\n",
      "[torch.FloatTensor of size 21]\n",
      "\n",
      "13 Variable containing:\n",
      " 0.9986\n",
      " 0.9981\n",
      " 0.9986\n",
      " 0.9982\n",
      " 0.9980\n",
      " 0.9979\n",
      " 0.9963\n",
      " 0.9974\n",
      " 0.9976\n",
      " 0.9985\n",
      " 0.9985\n",
      " 0.9978\n",
      " 0.9988\n",
      " 0.9972\n",
      " 0.9981\n",
      " 0.9955\n",
      " 0.9961\n",
      " 0.9975\n",
      " 0.9968\n",
      " 0.9960\n",
      " 0.9968\n",
      "[torch.FloatTensor of size 21]\n",
      "\n",
      "14 Variable containing:\n",
      " 0.9971\n",
      " 0.9982\n",
      " 0.9985\n",
      " 0.9987\n",
      " 0.9965\n",
      " 0.9978\n",
      " 0.9977\n",
      " 0.9960\n",
      " 0.9973\n",
      " 0.9979\n",
      " 0.9978\n",
      " 0.9971\n",
      " 0.9974\n",
      " 0.9975\n",
      " 0.9975\n",
      " 0.9973\n",
      " 0.9971\n",
      " 0.9966\n",
      " 0.9950\n",
      " 0.9972\n",
      " 0.9955\n",
      "[torch.FloatTensor of size 21]\n",
      "\n",
      "15 Variable containing:\n",
      " 0.9982\n",
      " 0.9959\n",
      " 0.9970\n",
      " 0.9979\n",
      " 0.9974\n",
      " 0.9984\n",
      " 0.9968\n",
      " 0.9983\n",
      " 0.9975\n",
      " 0.9975\n",
      " 0.9973\n",
      " 0.9977\n",
      " 0.9980\n",
      " 0.9982\n",
      " 0.9982\n",
      " 0.9983\n",
      " 0.9980\n",
      " 0.9969\n",
      " 0.9964\n",
      " 0.9978\n",
      " 0.9977\n",
      "[torch.FloatTensor of size 21]\n",
      "\n",
      "16 Variable containing:\n",
      " 0.9988\n",
      " 0.9988\n",
      " 0.9992\n",
      " 0.9973\n",
      " 0.9987\n",
      " 0.9986\n",
      " 0.9991\n",
      " 0.9967\n",
      " 0.9991\n",
      " 0.9975\n",
      " 0.9989\n",
      " 0.9994\n",
      " 0.9992\n",
      " 0.9988\n",
      " 0.9991\n",
      " 0.9983\n",
      " 0.9991\n",
      " 0.9984\n",
      " 0.9987\n",
      " 0.9988\n",
      " 0.9981\n",
      "[torch.FloatTensor of size 21]\n",
      "\n",
      "17 Variable containing:\n",
      " 0.9976\n",
      " 0.9983\n",
      " 0.9982\n",
      " 0.9983\n",
      " 0.9987\n",
      " 0.9988\n",
      " 0.9987\n",
      " 0.9988\n",
      " 0.9990\n",
      " 0.9969\n",
      " 0.9980\n",
      " 0.9986\n",
      " 0.9987\n",
      " 0.9988\n",
      " 0.9975\n",
      " 0.9991\n",
      " 0.9985\n",
      " 0.9989\n",
      " 0.9982\n",
      " 0.9994\n",
      " 0.9988\n",
      "[torch.FloatTensor of size 21]\n",
      "\n",
      "18 Variable containing:\n",
      " 0.9996\n",
      " 0.9999\n",
      " 0.9988\n",
      " 0.9989\n",
      " 0.9999\n",
      " 0.9973\n",
      " 0.9983\n",
      " 0.9993\n",
      " 0.9998\n",
      " 0.9973\n",
      " 0.9999\n",
      " 0.9978\n",
      " 0.9999\n",
      " 0.9994\n",
      " 0.9984\n",
      " 0.9998\n",
      " 0.9983\n",
      " 0.9987\n",
      " 0.9998\n",
      " 0.9997\n",
      " 0.9999\n",
      "[torch.FloatTensor of size 21]\n",
      "\n",
      "19 Variable containing:\n",
      " 0.9988\n",
      " 0.9992\n",
      " 0.9994\n",
      " 0.9995\n",
      " 0.9995\n",
      " 0.9989\n",
      " 0.9994\n",
      " 0.9996\n",
      " 0.9962\n",
      " 0.9996\n",
      " 0.9995\n",
      " 0.9995\n",
      " 0.9993\n",
      " 0.9993\n",
      " 0.9994\n",
      " 0.9994\n",
      " 0.9995\n",
      " 0.9996\n",
      " 0.9994\n",
      " 0.9995\n",
      " 0.9995\n",
      "[torch.FloatTensor of size 21]\n",
      "\n",
      "20 Variable containing:\n",
      " 0.9994\n",
      " 0.9996\n",
      " 0.9995\n",
      " 0.9996\n",
      " 0.9996\n",
      " 0.9992\n",
      " 0.9997\n",
      " 0.9997\n",
      " 0.9994\n",
      " 0.9995\n",
      " 0.9994\n",
      " 0.9996\n",
      " 0.9995\n",
      " 0.9995\n",
      " 0.9995\n",
      " 0.9994\n",
      " 0.9995\n",
      " 0.9995\n",
      " 0.9989\n",
      " 0.9996\n",
      " 0.9988\n",
      "[torch.FloatTensor of size 21]\n",
      "\n",
      "21 Variable containing:\n",
      " 0.9998\n",
      " 0.9996\n",
      " 0.9997\n",
      " 0.9996\n",
      " 0.9954\n",
      " 0.9998\n",
      " 0.9992\n",
      " 0.9996\n",
      " 0.9993\n",
      " 0.9998\n",
      " 0.9998\n",
      " 0.9999\n",
      " 0.9998\n",
      " 0.9998\n",
      " 0.9986\n",
      " 0.9995\n",
      " 0.9999\n",
      " 0.9996\n",
      " 0.9998\n",
      " 0.9999\n",
      " 0.9985\n",
      "[torch.FloatTensor of size 21]\n",
      "\n",
      "22 Variable containing:\n",
      " 0.9985\n",
      " 0.9997\n",
      " 0.9996\n",
      " 0.9997\n",
      " 0.9992\n",
      " 0.9996\n",
      " 0.9995\n",
      " 0.9996\n",
      " 0.9997\n",
      " 0.9997\n",
      " 0.9996\n",
      " 0.9997\n",
      " 0.9989\n",
      " 0.9997\n",
      " 0.9993\n",
      " 0.9994\n",
      " 0.9997\n",
      " 0.9997\n",
      " 0.9992\n",
      " 0.9997\n",
      " 0.9996\n",
      "[torch.FloatTensor of size 21]\n",
      "\n",
      "23 Variable containing:\n",
      " 0.9997\n",
      " 0.9996\n",
      " 0.9997\n",
      " 0.9997\n",
      " 0.9996\n",
      " 0.9997\n",
      " 0.9997\n",
      " 0.9996\n",
      " 0.9997\n",
      " 0.9995\n",
      " 0.9995\n",
      " 0.9996\n",
      " 0.9997\n",
      " 0.9997\n",
      " 0.9997\n",
      " 0.9997\n",
      " 0.9997\n",
      " 0.9998\n",
      " 0.9997\n",
      " 0.9997\n",
      " 0.9997\n",
      "[torch.FloatTensor of size 21]\n",
      "\n",
      "24 Variable containing:\n",
      " 0.9996\n",
      " 1.0000\n",
      " 1.0000\n",
      " 0.9980\n",
      " 1.0000\n",
      " 1.0000\n",
      " 0.9998\n",
      " 0.9999\n",
      " 0.9994\n",
      " 0.9998\n",
      " 1.0000\n",
      " 0.9980\n",
      " 1.0000\n",
      " 0.9996\n",
      " 1.0000\n",
      " 0.9997\n",
      " 1.0000\n",
      " 1.0000\n",
      " 0.9996\n",
      " 0.9999\n",
      " 1.0000\n",
      "[torch.FloatTensor of size 21]\n",
      "\n",
      "25 Variable containing:\n",
      " 0.9999\n",
      " 0.9993\n",
      " 0.9976\n",
      " 1.0000\n",
      " 1.0000\n",
      " 0.9997\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 0.9913\n",
      " 0.9997\n",
      " 1.0000\n",
      " 0.9986\n",
      " 1.0000\n",
      " 0.9990\n",
      " 1.0000\n",
      " 0.9995\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 0.9999\n",
      "[torch.FloatTensor of size 21]\n",
      "\n",
      "26 Variable containing:\n",
      " 0.9996\n",
      " 0.9997\n",
      " 0.9998\n",
      " 0.9996\n",
      " 0.9996\n",
      " 0.9996\n",
      " 0.9996\n",
      " 0.9996\n",
      " 0.9997\n",
      " 0.9996\n",
      " 0.9996\n",
      " 0.9996\n",
      " 0.9996\n",
      " 0.9996\n",
      " 0.9996\n",
      " 0.9996\n",
      " 0.9996\n",
      " 0.9995\n",
      " 0.9996\n",
      " 0.9994\n",
      " 0.9996\n",
      "[torch.FloatTensor of size 21]\n",
      "\n",
      "27 Variable containing:\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 0.9997\n",
      " 0.9999\n",
      " 1.0000\n",
      " 0.9999\n",
      " 0.9998\n",
      " 0.9999\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 0.9999\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      "[torch.FloatTensor of size 21]\n",
      "\n",
      "28 Variable containing:\n",
      " 0.9997\n",
      " 0.9997\n",
      " 0.9997\n",
      " 0.9998\n",
      " 0.9997\n",
      " 0.9997\n",
      " 0.9997\n",
      " 0.9997\n",
      " 0.9997\n",
      " 0.9996\n",
      " 0.9988\n",
      " 0.9996\n",
      " 0.9997\n",
      " 0.9998\n",
      " 0.9997\n",
      " 0.9997\n",
      " 0.9997\n",
      " 0.9997\n",
      " 0.9997\n",
      " 0.9997\n",
      " 0.9997\n",
      "[torch.FloatTensor of size 21]\n",
      "\n",
      "29 Variable containing:\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 0.9999\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 0.9994\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 0.9999\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      "[torch.FloatTensor of size 21]\n",
      "\n",
      "30 Variable containing:\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 0.9993\n",
      " 0.9998\n",
      " 0.9999\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 0.9996\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 0.9997\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      "[torch.FloatTensor of size 21]\n",
      "\n",
      "31 Variable containing:\n",
      " 0.9994\n",
      " 0.9994\n",
      " 0.9994\n",
      " 0.9994\n",
      " 0.9994\n",
      " 0.9994\n",
      " 0.9994\n",
      " 0.9994\n",
      " 0.9994\n",
      " 0.9986\n",
      " 0.9995\n",
      " 0.9994\n",
      " 0.9994\n",
      " 0.9995\n",
      " 0.9994\n",
      " 0.9992\n",
      " 0.9994\n",
      " 0.9994\n",
      " 0.9995\n",
      " 0.9994\n",
      " 0.9994\n",
      "[torch.FloatTensor of size 21]\n",
      "\n",
      "32 Variable containing:\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 0.9998\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 0.9967\n",
      " 1.0000\n",
      " 1.0000\n",
      " 0.9997\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 0.9998\n",
      " 1.0000\n",
      " 1.0000\n",
      " 0.9999\n",
      " 1.0000\n",
      "[torch.FloatTensor of size 21]\n",
      "\n",
      "33 Variable containing:\n",
      " 0.9984\n",
      " 0.9984\n",
      " 0.9984\n",
      " 0.9991\n",
      " 0.9985\n",
      " 0.9985\n",
      " 0.9985\n",
      " 0.9985\n",
      " 0.9985\n",
      " 0.9985\n",
      " 0.9985\n",
      " 0.9984\n",
      " 0.9984\n",
      " 0.9984\n",
      " 0.9985\n",
      " 0.9985\n",
      " 0.9985\n",
      " 0.9985\n",
      " 0.9985\n",
      " 0.9984\n",
      " 0.9986\n",
      "[torch.FloatTensor of size 21]\n",
      "\n",
      "34 Variable containing:\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 0.9999\n",
      " 1.0000\n",
      " 0.9996\n",
      " 0.9992\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      "[torch.FloatTensor of size 21]\n",
      "\n",
      "35 Variable containing:\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      "[torch.FloatTensor of size 21]\n",
      "\n",
      "36 Variable containing:\n",
      " 1.0000\n",
      " 1.0000\n",
      " 0.9999\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 0.9995\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      "[torch.FloatTensor of size 21]\n",
      "\n",
      "37 Variable containing:\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      "[torch.FloatTensor of size 21]\n",
      "\n",
      "38 Variable containing:\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      "[torch.FloatTensor of size 21]\n",
      "\n",
      "39 Variable containing:\n",
      " 0.9997\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      "[torch.FloatTensor of size 21]\n",
      "\n",
      "40 Variable containing:\n",
      " 1.0000\n",
      " 0.9998\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      "[torch.FloatTensor of size 21]\n",
      "\n",
      "0 Variable containing:\n",
      " 1.0000\n",
      " 0.9998\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      "[torch.FloatTensor of size 21]\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ewif' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-131-d6be9ebd6959>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'lstm'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-129-865b792a4716>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(layer_type, batch_size, num_epoch)\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontxt_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontxt_embeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m22\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mewif\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m             \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'-------------------------------'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'epoch:{}/{}, batch:{}/{}, loss:{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ewif' is not defined"
     ]
    }
   ],
   "source": [
    "train('lstm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qids = list(train_idx_set.keys())[0:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_title, batch_body, title_len, body_len = process_contxt_batch(qids, train_idx_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "title_qs = Variable(torch.FloatTensor(batch_title))\n",
    "body_qs = Variable(torch.FloatTensor(batch_body))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_layer = EmbeddingLayer(200, 240, 'lstm')\n",
    "embedding_layer.hidden = embedding_layer.init_hidden(batch_title.shape[1])\n",
    "\n",
    "title_embeddings = embedding_layer(title_qs, title_len)\n",
    "body_embeddings = embedding_layer(body_qs, body_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c = (title_embeddings + body_embeddings) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "1.00000e-02 *\n",
       "-0.5825  2.1472 -0.2239  ...   0.7355 -3.4969 -3.2186\n",
       "-0.5013  2.1868 -0.0168  ...   0.6186 -3.0579 -3.0639\n",
       "-0.3884  1.8186 -0.0859  ...   0.8862 -3.6700 -3.2304\n",
       "          ...             ⋱             ...          \n",
       "-0.6985  2.2384  0.0876  ...   0.6600 -3.3936 -3.2297\n",
       "-0.5910  2.4134 -0.1488  ...   0.8422 -4.6101 -3.5646\n",
       "-0.0674  2.2767  0.0316  ...   0.6252 -3.9558 -3.3540\n",
       "[torch.FloatTensor of size 22x240]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c[-22:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[262144,\n",
       " 491522,\n",
       " 240299,\n",
       " 196614,\n",
       " 360457,\n",
       " 425996,\n",
       " 163842,\n",
       " 393230,\n",
       " 393231,\n",
       " 491536,\n",
       " 327698,\n",
       " 121116,\n",
       " 397172,\n",
       " 294938,\n",
       " 32795,\n",
       " 32798,\n",
       " 33,\n",
       " 34,\n",
       " 229411,\n",
       " 37,\n",
       " 294951,\n",
       " 496988,\n",
       " 196650,\n",
       " 294955,\n",
       " 73107]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_idx_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': array([ 66935, 426300,  70518, 232599, 523074, 210772, 229721, 421797,\n",
       "        124315,  69105, 248750,  70052, 206788, 248606, 294643,   2340,\n",
       "         57833, 237004, 245933, 256280, 346767,   5084,  16776,  55312,\n",
       "        100586,  53443, 136515, 132076, 340324, 157574, 378966,  94040,\n",
       "        342049, 286386, 256085, 406437, 426281, 367553, 416229, 276123,\n",
       "        397066, 116051, 231825, 306014, 253084,  29443, 110394,   6424,\n",
       "        407490,  53489, 489072, 205811,  21303, 377515, 208037,  66606,\n",
       "        206277, 246931, 335508, 507425, 393560, 429556, 352150, 331768,\n",
       "         67155,   6307, 192036, 396483, 315431, 376003, 448796,  87677,\n",
       "        391839, 160160, 278122, 360817, 510617,  73323, 331227,  70037,\n",
       "        417418, 367338,  92541, 520528, 237451, 230574, 248240, 322035,\n",
       "        445612, 124040, 516399, 150639, 453936, 405564, 452362, 326452,\n",
       "         82445, 167025, 104373, 486989]), 'pos': array([82395])}"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_idx_set[73107]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'after upgrading to 11.10 i ca n t change the launcher icon size to smaller . i tried to change it using ccsm like described in how can i configure unity but the changes take no effects restarted tried sudo - still big icons . in the previous version of ubuntu this solution worked . how can i change the icon size btw how can i know which unity 2d vd 3d i running and switch between . edit looks like i using unity 2d'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(context_repre[73107]['b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i using ubuntu one my laptop for over a half of year now and i upgraded it to 13.10 a month ago . it did n t have any major problems before . but today after few hours of work it stopped responding completly and i had to restart it . i logged on and about minute later it freezed again . now it happens every time laptop boots up . keyboard and mouse does n t work either .'"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(context_repre[407490]['b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "t, b = contxt2vec(context_repre[405564]['t'], context_repre[405564]['b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10, 200), (98, 200))"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape, b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Variable containing:\n",
      " 0.9983\n",
      " 0.9980\n",
      " 0.9977\n",
      " 0.9977\n",
      " 0.9981\n",
      " 0.9970\n",
      " 0.9974\n",
      " 0.9981\n",
      " 0.9970\n",
      " 0.9970\n",
      " 0.9925\n",
      " 0.9975\n",
      " 0.9974\n",
      " 0.9978\n",
      " 0.9979\n",
      " 0.9974\n",
      " 0.9974\n",
      " 0.9973\n",
      " 0.9979\n",
      " 0.9934\n",
      " 0.9949\n",
      "[torch.FloatTensor of size 21]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.9998\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion(c[-22:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20951"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2i_map['doesnot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.066099, -0.072827,  0.07531 , ..., -0.043089,  0.053407,\n",
       "          0.004687],\n",
       "        [ 0.066099, -0.072827,  0.07531 , ..., -0.043089,  0.053407,\n",
       "          0.004687],\n",
       "        [ 0.029121, -0.017085, -0.048705, ...,  0.000808,  0.03283 ,\n",
       "         -0.099112],\n",
       "        ..., \n",
       "        [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,  0.      ],\n",
       "        [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,  0.      ],\n",
       "        [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,  0.      ]]),\n",
       " array([[ 0.091238, -0.121687, -0.018059, ...,  0.16513 , -0.005217,\n",
       "          0.059096],\n",
       "        [ 0.091238, -0.121687, -0.018059, ...,  0.16513 , -0.005217,\n",
       "          0.059096],\n",
       "        [ 0.06426 , -0.095106,  0.043716, ..., -0.013996, -0.030572,\n",
       "         -0.026798],\n",
       "        ..., \n",
       "        [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,  0.      ],\n",
       "        [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,  0.      ],\n",
       "        [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,  0.      ]]))"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_body[:,-22],batch_body[:,-19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.9983\n",
       " 0.9980\n",
       " 0.9977\n",
       " 0.9977\n",
       " 0.9981\n",
       " 0.9970\n",
       " 0.9974\n",
       " 0.9981\n",
       " 0.9970\n",
       " 0.9970\n",
       " 0.9925\n",
       " 0.9975\n",
       " 0.9974\n",
       " 0.9978\n",
       " 0.9979\n",
       " 0.9974\n",
       " 0.9974\n",
       " 0.9973\n",
       " 0.9979\n",
       " 0.9934\n",
       " 0.9949\n",
       "[torch.FloatTensor of size 21]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim(c[-22].expand(21,240), c[-21:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
