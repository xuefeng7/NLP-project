{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import string\n",
    "import numpy as np; np.random.seed(7)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loadings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word 2 vec repre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_map = {}\n",
    "with open('data/vectors_pruned.200.txt', 'r') as src:\n",
    "    src = src.read().strip().split('\\n')\n",
    "    for line in src:\n",
    "        wv = line.strip().split(' ')\n",
    "        word = wv.pop(0)\n",
    "        w2v_map[word] = np.array(list(map(float, wv)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2i_map = {}\n",
    "for i, key in enumerate(w2v_map.keys()):\n",
    "    w2i_map[key] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pkl.dump(w2i_map, open('data/word_idx_map.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### map Q idx to context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_matrix = np.zeros(( len((w2v_map.keys())), 200 ))\n",
    "counter = 0\n",
    "for _, val in w2v_map.items():\n",
    "    w2v_matrix[counter] = val\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pkl.dump(w2i_map, open('data/w2v_matrix.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def w2v(w):\n",
    "    return w2v_matrix[w2i_map[w]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sen2w(sen):\n",
    "    processed = []\n",
    "    sen = re.sub(r'[!#\\'(),/:?\\{}]', ' ', sen).strip().split()\n",
    "    if len(sen) > 100:\n",
    "        sen = sen[:100]\n",
    "    for w in sen:\n",
    "        # ignore date\n",
    "        if re.match(r'\\d{1,}-\\d{1,}-\\d{1,}', w):\n",
    "            continue\n",
    "        if re.match(r'\\d{1,}:\\d{1,}', w):\n",
    "            continue\n",
    "        if w in w2i_map:\n",
    "            processed += [w]\n",
    "        else:\n",
    "            separated = re.findall(r\"[^\\W\\d_]+|\\d+|[=`%$\\^\\-@;\\[&_*>\\].<~|+\\d+]\", w)\n",
    "            if len(set(separated)) == 1:\n",
    "                continue\n",
    "            if separated.count('*') > 3 or separated.count('=') > 3:\n",
    "                continue\n",
    "            for separate_w in separated:\n",
    "                if separate_w in w2i_map:\n",
    "                    processed += [separate_w]\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fixed context len = 125\n",
    "context_repre = {}\n",
    "with open('data/text_tokenized.txt', 'r') as src:\n",
    "    src = src.read().strip().split('\\n')\n",
    "    for line in src:\n",
    "        context = line.strip().split('\\t')\n",
    "        qid = context.pop(0)\n",
    "        if len(context) == 1:\n",
    "            context_repre[int(qid)] = {'t': sen2w(context[0]), 'b': None}\n",
    "        else:\n",
    "            len_title = len(context[0])\n",
    "            if len_title >= 125:\n",
    "                context_repre[int(qid)] = {'t':sen2w(context[0])[:125], 'b': None}\n",
    "            else:\n",
    "                context_repre[int(qid)] = {'t':sen2w(context[0]), 'b': sen2w(context[1])[:125 - len_title]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### len of context ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_lens = []\n",
    "for k, v in context_repre.items():\n",
    "    t, b = v['t'], v['b']\n",
    "    if not v['b']:\n",
    "        b = []\n",
    "    all_lens += [len(t)+len(b)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all(np.array(all_lens) < 125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/train_random.txt', header=None, delimiter='\\t', names=['Q','Q+','Q-'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>Q+</th>\n",
       "      <th>Q-</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>262144</td>\n",
       "      <td>211039</td>\n",
       "      <td>227387 413633 113297 356390 256881 145638 2962...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>491522</td>\n",
       "      <td>65911</td>\n",
       "      <td>155119 402211 310669 383107 131731 299465 1633...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>240299</td>\n",
       "      <td>168608 390642</td>\n",
       "      <td>368007 70009 48077 376760 438005 228888 142340...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>196614</td>\n",
       "      <td>205184</td>\n",
       "      <td>334471 163710 376791 441664 159963 406360 4300...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>360457</td>\n",
       "      <td>321532</td>\n",
       "      <td>151863 501857 217578 470017 125838 31836 42066...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Q             Q+                                                 Q-\n",
       "0  262144         211039  227387 413633 113297 356390 256881 145638 2962...\n",
       "1  491522          65911  155119 402211 310669 383107 131731 299465 1633...\n",
       "2  240299  168608 390642  368007 70009 48077 376760 438005 228888 142340...\n",
       "3  196614         205184  334471 163710 376791 441664 159963 406360 4300...\n",
       "4  360457         321532  151863 501857 217578 470017 125838 31836 42066..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_set_pair_with_idx(df):\n",
    "    idx_set = {}\n",
    "    for idx, row in df.iterrows():\n",
    "        idx_set[row['Q']] = {'pos': np.array(list(map(int, row['Q+'].split(' ')))), \\\n",
    "                             'neg': np.array(list(map(int, row['Q-'].split(' '))))}\n",
    "    return idx_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_idx_set = build_set_pair_with_idx(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': array([368007,  70009,  48077, 376760, 438005, 228888, 142340, 220049,\n",
       "        195789,  25591, 503498,  35125, 282665, 350677,  67132, 492121,\n",
       "        521770, 482854, 314882, 460162, 474768,  47095, 441111, 430424,\n",
       "         55776, 371296, 471245, 523727, 472488,  79961,  76016, 245183,\n",
       "        212299, 417533, 328855, 405600, 342727, 437437, 392462, 351849,\n",
       "        161396, 497477, 183319, 393544, 293781, 205739, 450857, 362082,\n",
       "        297814, 228122, 145335, 223978, 235373, 429337, 421932, 385761,\n",
       "        214356, 158411, 498088, 416208, 518985, 163666, 282313, 306557,\n",
       "        202189, 207846, 128929,  64066, 110792, 516967, 288842, 101977,\n",
       "        128678, 402999, 199440, 281229, 447477, 210418,  47234, 224765,\n",
       "        168359, 286331,  34844, 369064, 420539, 349599, 472814, 335544,\n",
       "        450513, 312684, 505425,  81283, 174562, 456927, 328250, 165416,\n",
       "         46207, 269400, 468467, 215983]), 'pos': array([168608, 390642])}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_idx_set[240299]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def contxt2vec(title, body=None):\n",
    "    \n",
    "    if body == None:\n",
    "        body = []\n",
    "    \n",
    "    v = np.zeros( (len(title) + len(body), 200) )\n",
    "    counter = 0\n",
    "    \n",
    "    for t in title:\n",
    "        v[counter] = w2v(t)\n",
    "        counter += 1\n",
    "\n",
    "    for b in body:\n",
    "        v[counter] = w2v(b)\n",
    "        counter += 1\n",
    "    \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_batch(qids, idx_set):\n",
    "    \n",
    "    total_pos_len = 0\n",
    "    for qid in qids:\n",
    "        total_pos_len += len(idx_set[qid]['pos'])\n",
    "    \n",
    "    # per batch element x: vstack, [query_Q x 1; pos_Q x 1; neg_Q x 20]\n",
    "    # per batch element y: vstack, [query_Q=-1; pos_Q=1; neg_Q=0]\n",
    "    batch_x = np.zeros(( total_pos_len * 22, 124, 200 ))\n",
    "    seq_len = np.zeros(total_pos_len * 22)\n",
    "    \n",
    "    counter = 0\n",
    "    for qid in qids:\n",
    "        \n",
    "        q_title, q_body = context_repre[qid]['t'], context_repre[qid]['b']\n",
    "        q_pos = idx_set[qid]['pos']\n",
    "\n",
    "        # usually one sample\n",
    "        for qid_pos in q_pos:\n",
    "            title, body = context_repre[qid_pos]['t'], context_repre[qid_pos]['b']\n",
    "            # query Q\n",
    "            if not q_body:\n",
    "                q_seq_len = len(q_title)\n",
    "            else:\n",
    "                q_seq_len = len(q_title) + len(q_body)\n",
    "            seq_len[counter] = q_seq_len\n",
    "            batch_x[counter, :q_seq_len] = contxt2vec(q_title, q_body)\n",
    "            counter += 1\n",
    "            # pos Q\n",
    "            if not body:\n",
    "                pos_q_seq_len = len(title)\n",
    "            else:\n",
    "                pos_q_seq_len = len(title) + len(body)\n",
    "            seq_len[counter] = pos_q_seq_len\n",
    "            batch_x[counter, :pos_q_seq_len] = contxt2vec(title, body)\n",
    "            counter += 1\n",
    "        \n",
    "            q_neg = idx_set[qid]['neg']\n",
    "            q_neg_sample_indices = np.random.choice(range(100), size=20)\n",
    "            q_random_neg = q_neg[q_neg_sample_indices]\n",
    "            # neg Q\n",
    "            for qid_neg in q_random_neg:\n",
    "                title, body = context_repre[qid_neg]['t'], context_repre[qid_neg]['b']\n",
    "                if not body:\n",
    "                    neg_q_seq_len = len(title)\n",
    "                else:\n",
    "                    neg_q_seq_len = len(title) + len(body)\n",
    "                seq_len[counter] = neg_q_seq_len\n",
    "                batch_x[counter, : neg_q_seq_len] = contxt2vec(title, body)\n",
    "                counter += 1\n",
    "    \n",
    "    return batch_x, seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dev = read_annotations('data/dev.txt')\n",
    "dev_data = {}\n",
    "for item in dev:\n",
    "    qid = int(item[0])\n",
    "    dev_data[qid] = {}\n",
    "    dev_data[qid]['q'] = list(map(int, item[1]))\n",
    "    dev_data[qid]['label'] = item[2]\n",
    "\n",
    "# create eval batch \n",
    "def process_eval_batch(qid, data):\n",
    "    qid_dict = data[qid]\n",
    "    qs = qid_dict['q']\n",
    "    batch_x = np.zeros(( len(qs)+1, 124, 200 ))\n",
    "    seq_len = np.zeros(len(qs)+1)\n",
    "    counter = 0\n",
    "    for qid_ in [qid] + qs:\n",
    "        title, body = context_repre[qid_]['t'], context_repre[qid_]['b']\n",
    "        if not body:\n",
    "            q_seq_len = len(title)\n",
    "        else:\n",
    "            q_seq_len = len(title) + len(body)\n",
    "        seq_len[counter] = q_seq_len\n",
    "        batch_x[counter, : q_seq_len] = contxt2vec(title, body)\n",
    "        counter += 1\n",
    "    return batch_x, seq_len\n",
    "\n",
    "def evaluate(embeddings): # (n x 240)\n",
    "    qs = embeddings[0]\n",
    "    qs_ = embeddings[1:]\n",
    "    cos_scores = cos_sim(qs.expand(len(embeddings)-1, 240), qs_)\n",
    "    return cos_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_annotations(path, K_neg=20, prune_pos_cnt=10):\n",
    "    lst = [ ]\n",
    "    with open(path) as fin:\n",
    "        for line in fin:\n",
    "            parts = line.split(\"\\t\")\n",
    "            pid, pos, neg = parts[:3]\n",
    "            pos = pos.split()\n",
    "            neg = neg.split()\n",
    "            if len(pos) == 0 or (len(pos) > prune_pos_cnt and prune_pos_cnt != -1): continue\n",
    "            if K_neg != -1:\n",
    "                np.random.shuffle(neg)\n",
    "                neg = neg[:K_neg]\n",
    "            s = set()\n",
    "            qids = [ ]\n",
    "            qlabels = [ ]\n",
    "            for q in neg:\n",
    "                if q not in s:\n",
    "                    qids.append(q)\n",
    "                    qlabels.append(0 if q not in pos else 1)\n",
    "                    s.add(q)\n",
    "            for q in pos:\n",
    "                if q not in s:\n",
    "                    qids.append(q)\n",
    "                    qlabels.append(1)\n",
    "                    s.add(q)\n",
    "            lst.append((pid, qids, qlabels))\n",
    "\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def precision(at, labels):\n",
    "    res = []\n",
    "    for item in labels:\n",
    "        tmp = item[:at]\n",
    "        res.append(np.sum(tmp) / at if len(tmp) != 0 else 0.0)\n",
    "    return sum(res)/len(res) if len(res) != 0 else 0.0\n",
    "\n",
    "def MAP(labels):\n",
    "    scores = []\n",
    "    missing_MAP = 0\n",
    "    for item in labels:\n",
    "        temp = []\n",
    "        count = 0.0\n",
    "        for i,val in enumerate(item):\n",
    "            \n",
    "            if val == 1:\n",
    "                count += 1.0\n",
    "                temp.append(count/(i+1))\n",
    "            if len(temp) > 0:\n",
    "                scores.append(sum(temp) / len(temp))\n",
    "            else:\n",
    "                missing_MAP += 1\n",
    "    return sum(scores)/len(scores) if len(scores) > 0 else 0.0\n",
    "    \n",
    "def MRR(labels):\n",
    "    scores = []\n",
    "    for item in labels:\n",
    "        for i,val in enumerate(item):\n",
    "            if val == 1:\n",
    "                scores.append(1.0/(i+1))\n",
    "                break\n",
    "    return sum(scores)/len(scores) if len(scores) > 0 else 0.0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Evaluation():\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def Precision(self, precision_at):\n",
    "        scores = []\n",
    "        for item in self.data:\n",
    "            temp = item[:precision_at]\n",
    "            if any(val==1 for val in item):\n",
    "                scores.append(sum([1 if val==1 else 0 for val in temp])*1.0 / len(temp) if len(temp) > 0 else 0.0)\n",
    "        return sum(scores)/len(scores) if len(scores) > 0 else 0.0\n",
    "\n",
    "    def MAP(self):\n",
    "        scores = []\n",
    "        missing_MAP = 0\n",
    "        for item in self.data:\n",
    "            temp = []\n",
    "            count = 0.0\n",
    "            for i,val in enumerate(item):\n",
    "                if val == 1:\n",
    "                    count += 1.0\n",
    "                    temp.append(count/(i+1))\n",
    "                if len(temp) > 0:\n",
    "                    scores.append(sum(temp) / len(temp))\n",
    "                else:\n",
    "                    missing_MAP += 1\n",
    "        return sum(scores)/len(scores) if len(scores) > 0 else 0.0\n",
    "\n",
    "    def MRR(self):\n",
    "        scores = []\n",
    "        for item in self.data:\n",
    "            for i,val in enumerate(item):\n",
    "                if val == 1:\n",
    "                    scores.append(1.0/(i+1))\n",
    "                    break\n",
    "\n",
    "        return sum(scores)/len(scores) if len(scores) > 0 else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EmbeddingLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, layer_type):\n",
    "        \n",
    "        super(EmbeddingLayer, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        if layer_type == 'lstm':\n",
    "            self.embedding_layer = nn.LSTM(input_size, hidden_size)#, batch_first=True)\n",
    "        elif layer_type == 'cnn':\n",
    "            pass\n",
    "        \n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return (Variable(torch.zeros(batch_size, 1, self.hidden_size)), Variable(torch.zeros(batch_size, 1, self.hidden_size)))\n",
    "\n",
    "    def forward(self, context, seq_len):\n",
    "        lstm_out, self.hidden = self.embedding_layer(context, (self.tanh(self.hidden[0]), self.tanh(self.hidden[1])))\n",
    "        mask = build_mask(seq_len)\n",
    "        embeddings = torch.sum(lstm_out * Variable(torch.FloatTensor(mask)), dim=1) \\\n",
    "            / Variable(torch.FloatTensor(np.sum(mask, axis=1)))\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cos_sim(qv, qv_):\n",
    "    return torch.sum(qv * qv_, dim=1) / (torch.sqrt(torch.sum(qv ** 2, dim=1)) * torch.sqrt(torch.sum(qv_ ** 2, dim=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.9449\n",
       " 0.9978\n",
       "[torch.FloatTensor of size 2]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim(torch.FloatTensor([[1,1,1,1],[2,3,4,5]]), torch.FloatTensor([[2,1,1,1],[3,4,5,6]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def criterion(embeddings):\n",
    "    \n",
    "    # a batch of embeddings\n",
    "    num_block = embeddings.size()[0] // 22\n",
    "    loss = 0\n",
    "    for i in range(num_block):\n",
    "        block_embeddings = embeddings[ i * 22: (i + 1) * 22 ]\n",
    "        qs = block_embeddings[0]\n",
    "        qs_ = block_embeddings[1:22]\n",
    "        cos_scores = cos_sim(qs.expand(21, 240), qs_)\n",
    "        pos_score = cos_scores[0]\n",
    "        neg_score = torch.max(cos_scores[1:])\n",
    "        \n",
    "        diff = neg_score - pos_score + 1 # margin=1\n",
    "        if diff.data[0] > 0:\n",
    "            loss += diff\n",
    "            \n",
    "    return loss / num_block # , cos_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_mask(seq_len):\n",
    "    mask = []\n",
    "    for i, s in enumerate(seq_len):\n",
    "        s_mask = np.zeros((124, 1))\n",
    "        s_mask[:int(s)] = np.ones((int(s), 1))\n",
    "        mask += [s_mask]\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mdl = EmbeddingLayer(200, 240, 'lstm')\n",
    "def train(layer_type, mdl, batch_size=64, num_epoch=100, eval=True):\n",
    "    \n",
    "    if layer_type == 'lstm':\n",
    "        embedding_layer = mdl\n",
    "    elif layer_type == 'cnn':\n",
    "        pass\n",
    "        \n",
    "    optimizer = torch.optim.Adam(embedding_layer.parameters(), lr=0.005)\n",
    "    \n",
    "    qids = list(train_idx_set.keys())\n",
    "    num_batch = len(qids) // batch_size\n",
    "    \n",
    "    for epoch in range(1, num_epoch + 1):\n",
    "        \n",
    "        for batch_idx in range(1, num_batch + 1):\n",
    "            \n",
    "            batch_x_qids = qids[(batch_idx - 1) * batch_size: batch_idx * batch_size]\n",
    "#             print(batch_x_qids)\n",
    "            start = time.time()\n",
    "            print ('processing batch {}'.format(batch_idx))\n",
    "            padded_batch_x, seq_len = process_batch(batch_x_qids, train_idx_set)\n",
    "#             print(padded_batch_x.shape, len(seq_len))\n",
    "            \n",
    "            \n",
    "            print ('processing batch costs:', time.time() - start)\n",
    "            embedding_layer.hidden = embedding_layer.init_hidden(padded_batch_x.shape[0])\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            start = time.time()\n",
    "            qs = Variable(torch.FloatTensor(padded_batch_x))\n",
    "            embeddings = embedding_layer(qs, seq_len)\n",
    "            print ('embedding costs:', time.time() - start)\n",
    "            start = time.time()\n",
    "            \n",
    "#             print(embeddings.size())\n",
    "            \n",
    "            \n",
    "            \n",
    "            print ('accumulating loss costs:', time.time() - start)\n",
    "            loss = criterion(embeddings)\n",
    "            \n",
    "            print ('-------------------------------')\n",
    "            print ('epoch:{}/{}, batch:{}/{}, loss:{}'.format(epoch, num_epoch, batch_idx, num_batch, loss.data[0]))\n",
    "            print ('-------------------------------')\n",
    "            start = time.time()\n",
    "            print('gradient befor back:', loss.grad)\n",
    "            loss.backward()\n",
    "            print('gradient after back:', loss.grad)\n",
    "            print ('backprop costs:', time.time() - start)\n",
    "            optimizer.step()\n",
    "            tmp = embedding_layer\n",
    "            if eval:\n",
    "                labels = []\n",
    "                for qid_ in dev_data.keys():\n",
    "                    dev_batch, dev_len = process_eval_batch(qid_, dev_data)\n",
    "                    embedding_layer.hidden = embedding_layer.init_hidden(dev_batch.shape[0])\n",
    "                    qs_ = Variable(torch.FloatTensor(dev_batch))\n",
    "                    embeddings = embedding_layer(qs_, dev_len)\n",
    "                    cos_scores = evaluate(embeddings)\n",
    "                    labels.append(np.array(dev_data[qid_]['label'])[np.argsort(cos_scores.data.numpy())][::-1])\n",
    "                print ('Dev Performance P@5', precision(5, labels))\n",
    "                print ('Dev Performance P@1', precision(1, labels))\n",
    "                print ('Dev Performance MAP', MAP(labels))\n",
    "                print ('Dev Performance MRR', MRR(labels))\n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "#     def MAP(self):\n",
    "#         scores = []\n",
    "#         missing_MAP = 0\n",
    "#         for item in self.data:\n",
    "#             temp = []\n",
    "#             count = 0.0\n",
    "#             for i,val in enumerate(item):\n",
    "#                 if val == 1:\n",
    "#                     count += 1.0\n",
    "#                     temp.append(count/(i+1))\n",
    "#                 if len(temp) > 0:\n",
    "#                     scores.append(sum(temp) / len(temp))\n",
    "#                 else:\n",
    "#                     missing_MAP += 1\n",
    "#         return sum(scores)/len(scores) if len(scores) > 0 else 0.0\n",
    "\n",
    "#     def MRR(self):\n",
    "#         scores = []\n",
    "#         for item in self.data:\n",
    "#             for i,val in enumerate(item):\n",
    "#                 if val == 1:\n",
    "#                     scores.append(1.0/(i+1))\n",
    "#                     break\n",
    "\n",
    "#         return sum(scores)/len(scores) if len(scores) > 0 else 0.0\n",
    "# scores = []\n",
    "# for qid_ in dev_data.keys():\n",
    "#     dev_batch, dev_len = process_eval_batch(qid_, dev_data)\n",
    "#     mdl.hidden = mdl.init_hidden(dev_batch.shape[0])\n",
    "#     qs_ = Variable(torch.FloatTensor(dev_batch))\n",
    "#     embeddings = mdl(qs_, dev_len)\n",
    "#     cos_scores = evaluate(embeddings)\n",
    "#     scores.append(cos_scores)\n",
    "\n",
    "# precision(5, scores, dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-184-a287e0687853>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mqs_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmdl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mcos_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mqid_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcos_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-7b8f3f5a8c66>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, context, seq_len)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mlstm_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_out\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m             \u001b[0;34m/\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0mflat_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         )\n\u001b[0;32m--> 162\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_packed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPackedSequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, *fargs, **fkwargs)\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutogradRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, weight, hidden)\u001b[0m\n\u001b[1;32m    242\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m         \u001b[0mnexth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_first\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, hidden, weight)\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_directions\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                 \u001b[0mhy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m                 \u001b[0mnext_hidden\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mall_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, hidden, weight)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0msteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreverse\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m             \u001b[0;31m# hack to handle LSTM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mLSTMCell\u001b[0;34m(input, hidden, w_ih, w_hh, b_ih, b_hh)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mingate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforgetgate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcellgate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutgate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mingate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mingate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0mforgetgate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforgetgate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mcellgate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcellgate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36msigmoid\u001b[0;34m(input)\u001b[0m\n\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_autograd_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSigmoid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/torch/autograd/_functions/pointwise.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, i, inplace)\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_for_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "labels = []\n",
    "qids = dev_data.keys()\n",
    "for qid_ in qids:\n",
    "    dev_batch, dev_len = process_eval_batch(qid_, dev_data)\n",
    "    mdl.hidden = mdl.init_hidden(dev_batch.shape[0])\n",
    "    qs_ = Variable(torch.FloatTensor(dev_batch))\n",
    "    embeddings = mdl(qs_, dev_len)\n",
    "    cos_scores = evaluate(embeddings)\n",
    "    labels.append(np.array(dev_data[qid_]['label'])[np.argsort(cos_scores.data.numpy())][::-1])\n",
    "print ('Dev Performance P@5', precision(5, labels))\n",
    "print ('Dev Performance P@1', precision(1, labels))\n",
    "print ('Dev Performance MAP', MAP(labels))\n",
    "print ('Dev Performance MRR', MRR(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20,)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Precision(self, precision_at):\n",
    "    scores = []\n",
    "    for item in self.data:\n",
    "        temp = item[:precision_at]\n",
    "        if any(val==1 for val in item):\n",
    "            scores.append(sum([1 if val==1 else 0 for val in temp])*1.0 / len(temp) if len(temp) > 0 else 0.0)\n",
    "    return sum(scores)/len(scores) if len(scores) > 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing batch 1\n",
      "processing batch costs: 0.3981361389160156\n",
      "embedding costs: 11.348710060119629\n",
      "accumulating loss costs: 1.1920928955078125e-06\n",
      "-------------------------------\n",
      "epoch:1/1, batch:1/10, loss:1.0000015497207642\n",
      "-------------------------------\n",
      "gradient befor back: None\n",
      "gradient after back: None\n",
      "backprop costs: 6.78510594367981\n",
      "Dev Performance P@5 0.289440993789\n",
      "Dev Performance P@1 0.304347826087\n",
      "Dev Performance MAP 0.5013439828151541\n",
      "Dev Performance MRR 0.5040220409319308\n",
      "processing batch 2\n",
      "processing batch costs: 0.47588086128234863\n",
      "embedding costs: 10.744858980178833\n",
      "accumulating loss costs: 9.5367431640625e-07\n",
      "-------------------------------\n",
      "epoch:1/1, batch:2/10, loss:0.9999990463256836\n",
      "-------------------------------\n",
      "gradient befor back: None\n",
      "gradient after back: None\n",
      "backprop costs: 6.25277304649353\n",
      "Dev Performance P@5 0.285714285714\n",
      "Dev Performance P@1 0.285714285714\n",
      "Dev Performance MAP 0.49561478558654337\n",
      "Dev Performance MRR 0.49428217826098725\n",
      "processing batch 3\n",
      "processing batch costs: 0.8194069862365723\n",
      "embedding costs: 26.6794011592865\n",
      "accumulating loss costs: 9.5367431640625e-07\n",
      "-------------------------------\n",
      "epoch:1/1, batch:3/10, loss:0.9999974966049194\n",
      "-------------------------------\n",
      "gradient befor back: None\n",
      "gradient after back: None\n",
      "backprop costs: 19.772520065307617\n",
      "Dev Performance P@5 0.288198757764\n",
      "Dev Performance P@1 0.291925465839\n",
      "Dev Performance MAP 0.4920104792791292\n",
      "Dev Performance MRR 0.4960113816480276\n",
      "processing batch 4\n",
      "processing batch costs: 0.4069070816040039\n",
      "embedding costs: 9.699919939041138\n",
      "accumulating loss costs: 9.5367431640625e-07\n",
      "-------------------------------\n",
      "epoch:1/1, batch:4/10, loss:0.9999990463256836\n",
      "-------------------------------\n",
      "gradient befor back: None\n",
      "gradient after back: None\n",
      "backprop costs: 5.166751861572266\n",
      "Dev Performance P@5 0.296894409938\n",
      "Dev Performance P@1 0.298136645963\n",
      "Dev Performance MAP 0.49780820601163833\n",
      "Dev Performance MRR 0.4966116577369776\n",
      "processing batch 5\n",
      "processing batch costs: 0.5352199077606201\n",
      "embedding costs: 15.010114908218384\n",
      "accumulating loss costs: 0.0\n",
      "-------------------------------\n",
      "epoch:1/1, batch:5/10, loss:1.0000003576278687\n",
      "-------------------------------\n",
      "gradient befor back: None\n",
      "gradient after back: None\n",
      "backprop costs: 9.270972967147827\n",
      "Dev Performance P@5 0.283229813665\n",
      "Dev Performance P@1 0.273291925466\n",
      "Dev Performance MAP 0.4834264092229788\n",
      "Dev Performance MRR 0.47609235647233444\n",
      "processing batch 6\n",
      "processing batch costs: 0.4309980869293213\n",
      "embedding costs: 11.007123947143555\n",
      "accumulating loss costs: 0.0\n",
      "-------------------------------\n",
      "epoch:1/1, batch:6/10, loss:1.0000016689300537\n",
      "-------------------------------\n",
      "gradient befor back: None\n",
      "gradient after back: None\n",
      "backprop costs: 6.65855598449707\n",
      "Dev Performance P@5 0.294409937888\n",
      "Dev Performance P@1 0.273291925466\n",
      "Dev Performance MAP 0.4850676949277749\n",
      "Dev Performance MRR 0.4799667490387257\n",
      "processing batch 7\n",
      "processing batch costs: 0.7643139362335205\n",
      "embedding costs: 23.59246516227722\n",
      "accumulating loss costs: 9.5367431640625e-07\n",
      "-------------------------------\n",
      "epoch:1/1, batch:7/10, loss:1.000001072883606\n",
      "-------------------------------\n",
      "gradient befor back: None\n",
      "gradient after back: None\n",
      "backprop costs: 19.283365964889526\n",
      "Dev Performance P@5 0.288198757764\n",
      "Dev Performance P@1 0.27950310559\n",
      "Dev Performance MAP 0.48677545770341984\n",
      "Dev Performance MRR 0.481746258082479\n",
      "processing batch 8\n",
      "processing batch costs: 0.4561150074005127\n",
      "embedding costs: 9.2969388961792\n",
      "accumulating loss costs: 0.0\n",
      "-------------------------------\n",
      "epoch:1/1, batch:8/10, loss:1.000001311302185\n",
      "-------------------------------\n",
      "gradient befor back: None\n",
      "gradient after back: None\n",
      "backprop costs: 5.15236496925354\n",
      "Dev Performance P@5 0.288198757764\n",
      "Dev Performance P@1 0.285714285714\n",
      "Dev Performance MAP 0.4856134726001709\n",
      "Dev Performance MRR 0.4869946587501344\n",
      "processing batch 9\n",
      "processing batch costs: 0.35426998138427734\n",
      "embedding costs: 7.751685857772827\n",
      "accumulating loss costs: 9.5367431640625e-07\n",
      "-------------------------------\n",
      "epoch:1/1, batch:9/10, loss:1.000002145767212\n",
      "-------------------------------\n",
      "gradient befor back: None\n",
      "gradient after back: None\n",
      "backprop costs: 4.315553903579712\n",
      "Dev Performance P@5 0.285714285714\n",
      "Dev Performance P@1 0.27950310559\n",
      "Dev Performance MAP 0.4903772952868103\n",
      "Dev Performance MRR 0.4857958747873753\n",
      "processing batch 10\n",
      "processing batch costs: 0.37608909606933594\n",
      "embedding costs: 9.631041049957275\n",
      "accumulating loss costs: 9.5367431640625e-07\n",
      "-------------------------------\n",
      "epoch:1/1, batch:10/10, loss:1.0000025033950806\n",
      "-------------------------------\n",
      "gradient befor back: None\n",
      "gradient after back: None\n",
      "backprop costs: 5.519411087036133\n",
      "Dev Performance P@5 0.283229813665\n",
      "Dev Performance P@1 0.304347826087\n",
      "Dev Performance MAP 0.495233109088336\n",
      "Dev Performance MRR 0.4968553811884967\n"
     ]
    }
   ],
   "source": [
    "mdl = EmbeddingLayer(200, 240, 'lstm')\n",
    "train('lstm', mdl, batch_size=25, num_epoch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('501754',\n",
       " ['399513',\n",
       "  '456861',\n",
       "  '144028',\n",
       "  '269913',\n",
       "  '491992',\n",
       "  '23596',\n",
       "  '428574',\n",
       "  '396630',\n",
       "  '50146',\n",
       "  '313023',\n",
       "  '419839',\n",
       "  '416899',\n",
       "  '438505',\n",
       "  '459246',\n",
       "  '23731',\n",
       "  '386606',\n",
       "  '218016',\n",
       "  '17630',\n",
       "  '462024',\n",
       "  '400442'],\n",
       " [1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_annotations('data/dev.txt')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161\n"
     ]
    }
   ],
   "source": [
    "tmp = 0\n",
    "for i in dev_data.keys():\n",
    "    tmp += 1\n",
    "print(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dev = read_annotations('data/dev.txt')\n",
    "dev_data = {}\n",
    "for item in dev:\n",
    "    qid = int(item[0])\n",
    "    dev_data[qid] = {}\n",
    "    dev_data[qid]['q'] = list(map(int, item[1]))\n",
    "    dev_data[qid]['label'] = item[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': [0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1],\n",
       " 'q': [428574,\n",
       "  400442,\n",
       "  218016,\n",
       "  491992,\n",
       "  419839,\n",
       "  17630,\n",
       "  144028,\n",
       "  399513,\n",
       "  416899,\n",
       "  23731,\n",
       "  459246,\n",
       "  386606,\n",
       "  23596,\n",
       "  438505,\n",
       "  462024,\n",
       "  50146,\n",
       "  396630,\n",
       "  313023,\n",
       "  269913,\n",
       "  456861]}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_data[501754]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dev = read_annotations('data/dev.txt')\n",
    "dev_data = {}\n",
    "for item in dev:\n",
    "    qid = int(item[0])\n",
    "    dev_data[qid] = {}\n",
    "    dev_data[qid]['q'] = list(map(int, item[1]))\n",
    "    dev_data[qid]['label'] = item[2]\n",
    "\n",
    "# create eval batch \n",
    "def process_eval_batch(qid, data):\n",
    "    qid_dict = data[qid]\n",
    "    qs = qid_dict['q']\n",
    "    batch_x = np.zeros(( len(qs)+1, 124, 200 ))\n",
    "    seq_len = np.zeros(len(qs)+1)\n",
    "    counter = 0\n",
    "    for qid_ in [qid] + qs:\n",
    "        title, body = context_repre[qid_]['t'], context_repre[qid_]['b']\n",
    "        if not body:\n",
    "            q_seq_len = len(title)\n",
    "        else:\n",
    "            q_seq_len = len(title) + len(body)\n",
    "        seq_len[counter] = q_seq_len\n",
    "        batch_x[counter, : q_seq_len] = contxt2vec(title, body)\n",
    "        counter += 1\n",
    "    return batch_x, seq_len\n",
    "\n",
    "def evaluate(embeddings): # (n x 240)\n",
    "    qs = embeddings[0]\n",
    "    qs_ = embeddings[1:]\n",
    "    cos_scores = cos_sim(qs.expand(len(embeddings)-1, 240), qs_)\n",
    "    return cos_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((21, 124, 200), (21,))"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_batch = process_eval_batch(501754, dev_data)\n",
    "dev_batch[0].shape, dev_batch[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
