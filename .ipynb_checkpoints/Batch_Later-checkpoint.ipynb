{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import string\n",
    "import numpy as np; np.random.seed(7)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# global\n",
    "HIDDEN_DIM = 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_map = {}\n",
    "with open('data/vectors_pruned.200.txt', 'r') as src:\n",
    "    src = src.read().strip().split('\\n')\n",
    "    for line in src:\n",
    "        wv = line.strip().split(' ')\n",
    "        word = wv.pop(0)\n",
    "        w2v_map[word] = np.array(list(map(float, wv)))\n",
    "\n",
    "w2i_map = {}\n",
    "w2v_matrix = np.zeros(( len((w2v_map.keys())), 200 ))\n",
    "for i, (key, val) in enumerate(w2v_map.items()):\n",
    "    w2i_map[key] = i\n",
    "    w2v_matrix[i] = val\n",
    "\n",
    "def w2v(w):\n",
    "    return w2v_matrix[w2i_map[w]]\n",
    "\n",
    "def sen2w(sen):\n",
    "    processed = []\n",
    "    sen = sen.strip().split()\n",
    "    if len(sen) > 100:\n",
    "        sen = sen[:100]\n",
    "    for w in sen:\n",
    "        #ignore date\n",
    "        if re.match(r'\\d{1,}-\\d{1,}-\\d{1,}', w):\n",
    "            continue\n",
    "        if re.match(r'\\d{1,}:\\d{1,}', w):\n",
    "            continue\n",
    "        \n",
    "        if w in w2i_map:\n",
    "            processed += [w]\n",
    "        else:\n",
    "            separated = re.findall(r\"[^\\W\\d_]+|\\d+|[=`%$\\^\\-@;\\[&_*>\\].<~|+\\d+]\", w)\n",
    "            if len(set(separated)) == 1:\n",
    "                continue\n",
    "            if separated.count('*') > 3 or separated.count('=') > 3:\n",
    "                continue\n",
    "            for separate_w in separated:\n",
    "                if separate_w in w2i_map:\n",
    "                    processed += [separate_w]\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fixed context len = 125\n",
    "context_repre = {}\n",
    "with open('data/text_tokenized.txt', 'r') as src:\n",
    "    src = src.read().strip().split('\\n')\n",
    "    for line in src:\n",
    "        context = line.strip().split('\\t')\n",
    "        qid = context.pop(0)\n",
    "        if len(context) == 1:\n",
    "            context_repre[int(qid)] = {'t': sen2w(context[0]), 'b': None}\n",
    "        else:\n",
    "            context_repre[int(qid)] = {'t':sen2w(context[0]), 'b': sen2w(context[1])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_set_pair_with_idx(df):\n",
    "    idx_set = {}\n",
    "    for idx, row in df.iterrows():\n",
    "        idx_set[row['Q']] = {'pos': np.array(list(map(int, row['Q+'].split(' ')))), \\\n",
    "                             'neg': np.array(list(map(int, row['Q-'].split(' '))))}\n",
    "    return idx_set\n",
    "\n",
    "train_df = pd.read_csv('data/train_random.txt', header=None, delimiter='\\t', names=['Q','Q+','Q-'])\n",
    "train_idx_set = build_set_pair_with_idx(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>Q+</th>\n",
       "      <th>Q-</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>262144</td>\n",
       "      <td>211039</td>\n",
       "      <td>227387 413633 113297 356390 256881 145638 2962...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>491522</td>\n",
       "      <td>65911</td>\n",
       "      <td>155119 402211 310669 383107 131731 299465 1633...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>240299</td>\n",
       "      <td>168608 390642</td>\n",
       "      <td>368007 70009 48077 376760 438005 228888 142340...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>196614</td>\n",
       "      <td>205184</td>\n",
       "      <td>334471 163710 376791 441664 159963 406360 4300...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>360457</td>\n",
       "      <td>321532</td>\n",
       "      <td>151863 501857 217578 470017 125838 31836 42066...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Q             Q+                                                 Q-\n",
       "0  262144         211039  227387 413633 113297 356390 256881 145638 2962...\n",
       "1  491522          65911  155119 402211 310669 383107 131731 299465 1633...\n",
       "2  240299  168608 390642  368007 70009 48077 376760 438005 228888 142340...\n",
       "3  196614         205184  334471 163710 376791 441664 159963 406360 4300...\n",
       "4  360457         321532  151863 501857 217578 470017 125838 31836 42066..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def contxt2vec(title, body=None):\n",
    "    \n",
    "    if body == None:\n",
    "        body = []\n",
    "    \n",
    "    title_v = np.zeros( (len(title), 200) )\n",
    "    \n",
    "    for i, t in enumerate(title):\n",
    "        title_v[i] = w2v(t)\n",
    "    \n",
    "    if len(body) > 0:\n",
    "        body_v = np.zeros( (len(body), 200) )\n",
    "        for i, b in enumerate(body):\n",
    "            body_v[i] = w2v(b)\n",
    "    \n",
    "        return title_v, body_v\n",
    "    \n",
    "    return title_v, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_contxt_batch(qids, idx_set, batch_first=False):\n",
    "    \n",
    "    batch_title, batch_body = [], []\n",
    "    max_title_len, max_body_len = 0, 0\n",
    "    title_len, body_len = [], []\n",
    "    \n",
    "    for qid in qids:\n",
    "        \n",
    "        q_title, q_body = context_repre[qid]['t'], context_repre[qid]['b']\n",
    "        q_pos = idx_set[qid]['pos']\n",
    "        \n",
    "        if len(q_pos) > 20:\n",
    "            q_pos = q_pos[:20]\n",
    "\n",
    "        for qid_pos in q_pos:\n",
    "            # query Q\n",
    "            title_len += [len(q_title)]\n",
    "            batch_title += [ q_title ]\n",
    "            max_title_len = max(max_title_len, len(q_title))\n",
    "            if not q_body:\n",
    "                body_len += [len(q_title)]\n",
    "                batch_body += [ q_title ]\n",
    "            else:\n",
    "                batch_body += [ q_body ]\n",
    "                body_len += [len(q_body)]\n",
    "                max_body_len = max(max_body_len, len(q_body))\n",
    "                \n",
    "            # pos Q\n",
    "            title, body = context_repre[qid_pos]['t'], context_repre[qid_pos]['b']\n",
    "            title_len += [len(title)]\n",
    "            batch_title += [ title ]\n",
    "            max_title_len = max(max_title_len, len(title))\n",
    "            if not body:\n",
    "                body_len += [len(title)]\n",
    "                batch_body += [ title ]\n",
    "            else:\n",
    "                batch_body += [ body ]\n",
    "                body_len += [len(body)]\n",
    "                max_body_len = max(max_body_len, len(body))\n",
    "            # neg Q\n",
    "            q_neg = idx_set[qid]['neg']\n",
    "            q_neg_sample_indices = np.random.choice(range(100), size=20)\n",
    "            q_random_neg = q_neg[q_neg_sample_indices]\n",
    "            \n",
    "            for qid_neg in q_random_neg:\n",
    "                title, body = context_repre[qid_neg]['t'], context_repre[qid_neg]['b']\n",
    "                title_len += [len(title)]\n",
    "                batch_title += [ title ]\n",
    "                max_title_len = max(max_title_len, len(title))\n",
    "                if not body:\n",
    "                    body_len += [len(title)]\n",
    "                    batch_body += [ title ]\n",
    "                else:\n",
    "                    batch_body += [ body ]\n",
    "                    body_len += [len(body)]\n",
    "                    max_body_len = max(max_body_len, len(body))\n",
    "    \n",
    "    if batch_first:\n",
    "        # for CNN\n",
    "        padded_batch_title = np.zeros(( len(batch_title), max_title_len, 200)) \n",
    "        padded_batch_body = np.zeros(( len(batch_body),  max_body_len, 200))\n",
    "        for i, (title, body) in enumerate(zip(batch_title, batch_body)):\n",
    "            title_repre, body_repre = contxt2vec(title, body)\n",
    "            padded_batch_title[i, :title_len[i]] = title_repre\n",
    "            padded_batch_body[i, :body_len[i]] = body_repre\n",
    "    else:\n",
    "        # for LSTM\n",
    "        # (max_seq_len, batch_size, feature_len)\n",
    "        padded_batch_title = np.zeros(( max_title_len, len(batch_title), 200)) \n",
    "        padded_batch_body = np.zeros(( max_body_len, len(batch_body),  200))\n",
    "        for i, (title, body) in enumerate(zip(batch_title, batch_body)):\n",
    "            title_repre, body_repre = contxt2vec(title, body)\n",
    "            padded_batch_title[:title_len[i], i] = title_repre\n",
    "            padded_batch_body[:body_len[i], i] = body_repre\n",
    "\n",
    "    return padded_batch_title, padded_batch_body, \\\n",
    "                np.array(title_len).reshape(-1,1), np.array(body_len).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_annotations(path, K_neg=20, prune_pos_cnt=20):\n",
    "    lst = [ ]\n",
    "    with open(path) as fin:\n",
    "        for line in fin:\n",
    "            parts = line.split(\"\\t\")\n",
    "            pid, pos, neg = parts[:3]\n",
    "            pos = pos.split()\n",
    "            neg = neg.split()\n",
    "            if len(pos) == 0 or (len(pos) > prune_pos_cnt and prune_pos_cnt != -1): continue\n",
    "            if K_neg != -1:\n",
    "                np.random.shuffle(neg)\n",
    "                neg = neg[:K_neg]\n",
    "            s = set()\n",
    "            qids = [ ]\n",
    "            qlabels = [ ]\n",
    "            for q in neg:\n",
    "                if q not in s:\n",
    "                    qids.append(q)\n",
    "                    qlabels.append(0 if q not in pos else 1)\n",
    "                    s.add(q)\n",
    "            for q in pos:\n",
    "                if q not in s:\n",
    "                    qids.append(q)\n",
    "                    qlabels.append(1)\n",
    "                    s.add(q)\n",
    "            lst.append((pid, qids, qlabels))\n",
    "\n",
    "    return lst\n",
    "\n",
    "def cos_sim(qv, qv_):\n",
    "    return torch.sum(qv * qv_, dim=1) / (torch.sqrt(torch.sum(qv ** 2, dim=1)) * torch.sqrt(torch.sum(qv_ ** 2, dim=1)))\n",
    "    \n",
    "# create eval batch \n",
    "def process_eval_batch(qid, data, batch_first=False):\n",
    "    qid_dict = data[qid]\n",
    "    qs = qid_dict['q']\n",
    "    max_title_len, max_body_len = 0, 0\n",
    "    title_len, body_len = [], []\n",
    "    batch_title, batch_body = [], []\n",
    "    for qid_ in [qid] + qs:\n",
    "        title, body = context_repre[qid_]['t'], context_repre[qid_]['b']\n",
    "        title_len += [len(title)]\n",
    "        batch_title += [ title ]\n",
    "        max_title_len = max(max_title_len, len(title))\n",
    "        if not body:\n",
    "            body_len += [len(title)]\n",
    "            batch_body += [ title ]\n",
    "        else:\n",
    "            batch_body += [ body ]\n",
    "            body_len += [len(body)]\n",
    "            max_body_len = max(max_body_len, len(body))\n",
    "            \n",
    "    if batch_first:\n",
    "        padded_batch_title = np.zeros(( len(batch_title), max_title_len, 200)) \n",
    "        padded_batch_body = np.zeros(( len(batch_body),  max_body_len, 200))\n",
    "        for i, (title, body) in enumerate(zip(batch_title, batch_body)):\n",
    "            title_repre, body_repre = contxt2vec(title, body)\n",
    "            padded_batch_title[i, :title_len[i]] = title_repre\n",
    "            padded_batch_body[i, :body_len[i]] = body_repre\n",
    "    else:\n",
    "        padded_batch_title = np.zeros(( max_title_len, len(batch_title), 200)) \n",
    "        padded_batch_body = np.zeros(( max_body_len, len(batch_body),  200))\n",
    "        for i, (title, body) in enumerate(zip(batch_title, batch_body)):\n",
    "            title_repre, body_repre = contxt2vec(title, body)\n",
    "            padded_batch_title[:title_len[i], i] = title_repre\n",
    "            padded_batch_body[:body_len[i], i] = body_repre\n",
    "    \n",
    "    return padded_batch_title, padded_batch_body, \\\n",
    "           np.array(title_len).reshape(-1,1), np.array(body_len).reshape(-1,1) \n",
    "    \n",
    "def evaluate(embeddings): # (n x 240)\n",
    "    qs = embeddings[0]\n",
    "    qs_ = embeddings[1:]\n",
    "    cos_scores = cos_sim(qs.expand(len(embeddings)-1, qs.size(0)), qs_)\n",
    "    return cos_scores\n",
    "\n",
    "def precision(at, labels):\n",
    "    res = []\n",
    "    for item in labels:\n",
    "        tmp = item[:at]\n",
    "        if any(val==1 for val in item):\n",
    "            res.append(np.sum(tmp) / len(tmp) if len(tmp) != 0 else 0.0)\n",
    "    return sum(res)/len(res) if len(res) != 0 else 0.0\n",
    "\n",
    "def MAP(labels):\n",
    "    scores = []\n",
    "    missing_MAP = 0\n",
    "    for item in labels:\n",
    "        temp = []\n",
    "        count = 0.0\n",
    "        for i,val in enumerate(item):\n",
    "            \n",
    "            if val == 1:\n",
    "                count += 1.0\n",
    "                temp.append(count/(i+1))\n",
    "            if len(temp) > 0:\n",
    "                scores.append(sum(temp) / len(temp))\n",
    "            else:\n",
    "                missing_MAP += 1\n",
    "    return sum(scores)/len(scores) if len(scores) > 0 else 0.0\n",
    "    \n",
    "def MRR(labels):\n",
    "    scores = []\n",
    "    for item in labels:\n",
    "        for i,val in enumerate(item):\n",
    "            if val == 1:\n",
    "                scores.append(1.0/(i+1))\n",
    "                break\n",
    "    return sum(scores)/len(scores) if len(scores) > 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEV SET\n",
    "dev = read_annotations('data/dev.txt')\n",
    "dev_data = {}\n",
    "for item in dev:\n",
    "    qid = int(item[0])\n",
    "    dev_data[qid] = {}\n",
    "    dev_data[qid]['q'] = list(map(int, item[1]))\n",
    "    dev_data[qid]['label'] = item[2]\n",
    "\n",
    "# TEST SET\n",
    "test = read_annotations('data/test.txt')\n",
    "test_data = {}\n",
    "for item in test:\n",
    "    qid = int(item[0])\n",
    "    test_data[qid] = {}\n",
    "    test_data[qid]['q'] = list(map(int, item[1]))\n",
    "    test_data[qid]['label'] = item[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_eval(embedding_layer, eval_name, batch_first=False):\n",
    "    \n",
    "    if eval_name == 'Dev':\n",
    "        eval_data = dev_data\n",
    "        eval_map = {}\n",
    "        for qid_ in dev_data.keys():\n",
    "            eval_map[qid_] = process_eval_batch(qid_, dev_data, batch_first=batch_first)\n",
    "            \n",
    "    elif eval_name == 'Test':\n",
    "        eval_data = test_data\n",
    "        eval_map = {}\n",
    "        for qid_ in test_data.keys():\n",
    "            eval_map[qid_] = process_eval_batch(qid_, test_data, batch_first=batch_first)\n",
    "    \n",
    "    labels = []\n",
    "    \n",
    "    for qid_ in eval_map.keys():\n",
    "        \n",
    "        eval_title_batch, eval_body_batch, eval_title_len, eval_body_len = eval_map[qid_] # process_eval_batch(qid_, eval_data)\n",
    "        embedding_layer.title_hidden = embedding_layer.init_hidden(eval_title_batch.shape[1])\n",
    "        embedding_layer.body_hidden = embedding_layer.init_hidden(eval_body_batch.shape[1])\n",
    "        eval_title_qs = Variable(torch.FloatTensor(eval_title_batch))\n",
    "        eval_body_qs = Variable(torch.FloatTensor(eval_body_batch))\n",
    "        embeddings = embedding_layer(eval_title_qs, eval_body_qs, eval_title_len, eval_body_len)\n",
    "        cos_scores = evaluate(embeddings)\n",
    "        labels.append(np.array(eval_data[qid_]['label'])[np.argsort(cos_scores.data.numpy())][::-1])\n",
    "    \n",
    "    print (eval_name + ' Performance MAP', MAP(labels))\n",
    "    print (eval_name + ' Performance MRR', MRR(labels))\n",
    "    print (eval_name + ' Performance P@1', precision(1, labels))\n",
    "    print (eval_name + ' Performance P@5', precision(5, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mask3d(seq_len, max_len):\n",
    "    mask = np.zeros((max_len, len(seq_len), 1))\n",
    "    for i, s in enumerate(seq_len):\n",
    "        # only one word\n",
    "        if int(s) == -1:\n",
    "            mask[0, i] = 1\n",
    "        # only two word\n",
    "        elif int(s) == 0:\n",
    "            mask[:2, i] = np.ones((2, 1))\n",
    "        else: \n",
    "            mask[:int(s), i] = np.ones((int(s), 1))\n",
    "    return mask\n",
    "\n",
    "def multi_margin_loss(hidden, margin=0.50):\n",
    "    \n",
    "    def loss_func(embeddings):\n",
    "        # a batch of embeddings\n",
    "        blocked_embeddings = embeddings.view(-1, 22, hidden)\n",
    "        q_vecs = blocked_embeddings[:,0,:]\n",
    "        \n",
    "        pos_vecs = blocked_embeddings[:,1,:]\n",
    "        neg_vecs = blocked_embeddings[:,2:,:]\n",
    "\n",
    "        pos_scores = torch.sum(q_vecs * pos_vecs, dim=1) / (torch.sqrt(torch.sum(q_vecs ** 2, dim=1)) \\\n",
    "                                                   * torch.sqrt(torch.sum(pos_vecs ** 2, dim=1)))\n",
    "        neg_scores = torch.sum(torch.unsqueeze(q_vecs, dim=1) * neg_vecs, dim=2) \\\n",
    "        / (torch.unsqueeze(torch.sqrt(torch.sum(q_vecs ** 2, dim=1)),dim=1) * torch.sqrt(torch.sum( neg_vecs ** 2, dim=2)))\n",
    "        neg_scores = torch.max(neg_scores, dim=1)[0]\n",
    "        \n",
    "        diff = neg_scores - pos_scores + margin\n",
    "        loss = torch.mean((diff > 0).float() * diff)\n",
    "        return loss\n",
    "\n",
    "    return loss_func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EmbeddingLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, layer_type, num_layer=1, kernel_size=3):\n",
    "        \n",
    "        super(EmbeddingLayer, self).__init__()\n",
    "        \n",
    "        self.num_layer = num_layer\n",
    "        \n",
    "        self.layer_type = layer_type\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.kernel_size = kernel_size\n",
    "        \n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "        if self.layer_type == 'lstm':\n",
    "            \n",
    "            self.embedding_layer = nn.LSTM(self.input_size, hidden_size, bidirectional=True)\n",
    "        \n",
    "        elif self.layer_type == 'cnn':\n",
    "\n",
    "            self.embedding_layer = nn.Sequential(\n",
    "                        nn.Conv1d(in_channels = self.input_size,\n",
    "                                  out_channels = self.hidden_size,\n",
    "                                  kernel_size = kernel_size),\n",
    "                        self.tanh)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return (Variable(torch.zeros(self.num_layer*2, batch_size, self.hidden_size)), \\\n",
    "                Variable(torch.zeros(self.num_layer*2, batch_size, self.hidden_size)))\n",
    "\n",
    "    def forward(self, title, body, title_len, body_len):\n",
    "        \n",
    "            \n",
    "        if self.layer_type == 'lstm':\n",
    "            \n",
    "            title_mask = Variable(torch.FloatTensor(build_mask3d(title_len, np.max(seq_len))))\n",
    "            body_mask = Variable(torch.FloatTensor(build_mask3d(body_len, np.max(body_len))))\n",
    "            \n",
    "            \n",
    "            title_out, self.title_hidden = self.embedding_layer(title, (self.tanh(self.title_hidden[0]), \\\n",
    "                                                                   self.tanh(self.title_hidden[1])))\n",
    "            body_out, self.body_hidden = self.embedding_layer(body, (self.tanh(self.body_hidden[0]), \\\n",
    "                                                                   self.tanh(self.body_hidden[1])))\n",
    "        \n",
    "        if self.layer_type == 'cnn':\n",
    "            # batch first input\n",
    "            title_mask = Variable(torch.FloatTensor(build_mask3d(title_len - self.kernel_size + 1,\\\n",
    "                                                                 np.max(title_len) - self.kernel_size + 1)))\n",
    "            body_mask = Variable(torch.FloatTensor(build_mask3d(body_len - self.kernel_size + 1, \\\n",
    "                                                                np.max(body_len) - self.kernel_size + 1)))\n",
    "            \n",
    "            title = torch.transpose(title, 1, 2)\n",
    "            body = torch.transpose(body, 1, 2)\n",
    "#             title = title.view(-1, title.size(2), title.size(1))\n",
    "#             body = body.view(-1, body.size(2), body.size(1))\n",
    "#             print ('after transpose:', title.size())\n",
    "            title_out =  self.embedding_layer(title)\n",
    "            body_out =  self.embedding_layer(body)\n",
    "#             print ('after embedding:', title_out.size())\n",
    "            title_out = torch.transpose(title_out, 1, 2)\n",
    "            body_out = torch.transpose(body_out, 1, 2)\n",
    "#             print ('after transpose:', title_out.size())\n",
    "            title_out = torch.transpose(title_out, 0, 1)\n",
    "            body_out = torch.transpose(body_out, 0, 1)\n",
    "#             print ('after transpose:', title_out.size())\n",
    "#             title_out = title_out.view(title_out.size(1), -1, title_out.size(2))\n",
    "#             body_out = body_out.view(body_out.size(1), -1, body_out.size(2))\n",
    "#             print ('after view embedding :', title_out.size())\n",
    "        \n",
    "        #print ('mask size:', title_mask.size())\n",
    "        title_embeddings = torch.sum(title_out * title_mask, dim=0) / torch.sum(title_mask, dim=0)\n",
    "        body_embeddings = torch.sum(body_out * body_mask, dim=0) / torch.sum(body_mask, dim=0)\n",
    "        embeddings = ( title_embeddings + body_embeddings ) / 2\n",
    "        \n",
    "        return embeddings#title_embeddings, body_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_model(mdl, path):\n",
    "    # saving model params\n",
    "    torch.save(mdl.state_dict(), path)\n",
    "\n",
    "def restore_model(mdl_skeleton, path):\n",
    "    # restoring params to the mdl skeleton\n",
    "    mdl_skeleton.load_state_dict(torch.load(path))\n",
    "    return mdl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(layer_type, embedding_layer, batch_size=25, num_epoch=100, id_set=train_idx_set, eval=True):\n",
    "    \n",
    "    if layer_type == 'lstm':\n",
    "        \n",
    "        embedding_layer = EmbeddingLayer(200, 240, 'lstm')\n",
    "        criterion = multi_margin_loss(hidden=embedding_layer.hidden_size * 2)\n",
    "    \n",
    "    elif layer_type == 'cnn':\n",
    "        \n",
    "        embedding_layer = EmbeddingLayer(200, 667, 'cnn')\n",
    "        criterion = multi_margin_loss(hidden=embedding_layer.hidden_size)\n",
    "    \n",
    "    \n",
    "    optimizer = torch.optim.Adam(embedding_layer.parameters(), lr=0.001)\n",
    "    \n",
    "    qids = list(id_set.keys())\n",
    "    num_batch = len(qids) // batch_size\n",
    "    \n",
    "    for epoch in range(1, num_epoch + 1):\n",
    "        \n",
    "        for batch_idx in range(1, num_batch + 1):\n",
    "            \n",
    "            batch_x_qids = qids[ ( batch_idx - 1 ) * batch_size: batch_idx * batch_size ]\n",
    "            \n",
    "            if layer_type == 'lstm':\n",
    "                batch_title, batch_body, title_len, body_len = process_contxt_batch(batch_x_qids, \\\n",
    "                                                                                train_idx_set)\n",
    "                embedding_layer.title_hidden = embedding_layer.init_hidden(batch_title.shape[1])\n",
    "                embedding_layer.body_hidden = embedding_layer.init_hidden(batch_body.shape[1])\n",
    "            else:\n",
    "                batch_title, batch_body, title_len, body_len = process_contxt_batch(batch_x_qids, \\\n",
    "                                                                                train_idx_set, batch_first=True)\n",
    "            \n",
    "            title_qs = Variable(torch.FloatTensor(batch_title))\n",
    "            body_qs = Variable(torch.FloatTensor(batch_body))\n",
    "            \n",
    "            embeddings = embedding_layer(title_qs, body_qs, title_len, body_len)\n",
    "\n",
    "            loss = criterion(embeddings)\n",
    "\n",
    "            print ('epoch:{}/{}, batch:{}/{}, loss:{}'.format(epoch, num_epoch, batch_idx, num_batch, loss.data[0]))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "            if eval and batch_idx % 5 == 0: # lstm for now\n",
    "                print ('evaluating ....')\n",
    "                if layer_type == 'lstm':\n",
    "                    do_eval(embedding_layer, 'Dev')\n",
    "                    print ('------------------')\n",
    "                    do_eval(embedding_layer, 'Test')\n",
    "                elif layer_type == 'cnn':\n",
    "                    do_eval(embedding_layer, 'Dev', batch_first=True)\n",
    "                    print ('------------------')\n",
    "                    do_eval(embedding_layer, 'Test', batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = EmbeddingLayer(200, 667, 'cnn') # loss margin = 0.5\n",
    "train('cnn', model, batch_size=25, num_epoch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_model(model, 'models/lstm_bi_epoch=4.5_margin=.5_hidden=120')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1/10, batch:1/508, loss:0.06204039976000786\n",
      "epoch:1/10, batch:2/508, loss:0.06698644161224365\n",
      "epoch:1/10, batch:3/508, loss:0.08204396069049835\n",
      "epoch:1/10, batch:4/508, loss:0.07271836698055267\n",
      "epoch:1/10, batch:5/508, loss:0.11348847299814224\n",
      "epoch:1/10, batch:6/508, loss:0.12347452342510223\n",
      "epoch:1/10, batch:7/508, loss:0.07170307636260986\n",
      "epoch:1/10, batch:8/508, loss:0.05405590310692787\n",
      "epoch:1/10, batch:9/508, loss:0.13060428202152252\n",
      "epoch:1/10, batch:10/508, loss:0.07260049134492874\n",
      "epoch:1/10, batch:11/508, loss:0.08073266595602036\n",
      "epoch:1/10, batch:12/508, loss:0.14527538418769836\n",
      "epoch:1/10, batch:13/508, loss:0.05466698110103607\n",
      "epoch:1/10, batch:14/508, loss:0.08360808342695236\n",
      "epoch:1/10, batch:15/508, loss:0.0836087018251419\n",
      "epoch:1/10, batch:16/508, loss:0.09609977155923843\n",
      "epoch:1/10, batch:17/508, loss:0.10398983210325241\n",
      "epoch:1/10, batch:18/508, loss:0.1288212686777115\n",
      "epoch:1/10, batch:19/508, loss:0.03824116662144661\n",
      "epoch:1/10, batch:20/508, loss:0.034473661333322525\n",
      "epoch:1/10, batch:21/508, loss:0.09659384936094284\n",
      "epoch:1/10, batch:22/508, loss:0.07781846076250076\n",
      "epoch:1/10, batch:23/508, loss:0.08880175650119781\n",
      "epoch:1/10, batch:24/508, loss:0.1002836674451828\n",
      "epoch:1/10, batch:25/508, loss:0.09663750976324081\n",
      "evaluating ....\n",
      "Dev Performance P@5 0.403726708075\n",
      "Dev Performance P@1 0.546583850932\n",
      "Dev Performance MAP 0.6264974227024006\n",
      "Dev Performance MRR 0.6723516674329609\n",
      "------------------\n",
      "Test Performance P@5 0.368553459119\n",
      "Test Performance P@1 0.540880503145\n",
      "Test Performance MAP 0.6399132663804957\n",
      "Test Performance MRR 0.6856381793571468\n",
      "epoch:1/10, batch:26/508, loss:0.13447436690330505\n",
      "epoch:1/10, batch:27/508, loss:0.08450022339820862\n",
      "epoch:1/10, batch:28/508, loss:0.09064411371946335\n",
      "epoch:1/10, batch:29/508, loss:0.10594522953033447\n",
      "epoch:1/10, batch:30/508, loss:0.10714061558246613\n",
      "epoch:1/10, batch:31/508, loss:0.12672273814678192\n",
      "epoch:1/10, batch:32/508, loss:0.10365843772888184\n",
      "epoch:1/10, batch:33/508, loss:0.09424255043268204\n",
      "epoch:1/10, batch:34/508, loss:0.10167635977268219\n",
      "epoch:1/10, batch:35/508, loss:0.09326761215925217\n",
      "epoch:1/10, batch:36/508, loss:0.11956734955310822\n",
      "epoch:1/10, batch:37/508, loss:0.08456160128116608\n",
      "epoch:1/10, batch:38/508, loss:0.09245448559522629\n",
      "epoch:1/10, batch:39/508, loss:0.09253036975860596\n",
      "epoch:1/10, batch:40/508, loss:0.10610334575176239\n",
      "epoch:1/10, batch:41/508, loss:0.07033271342515945\n",
      "epoch:1/10, batch:42/508, loss:0.1148577407002449\n",
      "epoch:1/10, batch:43/508, loss:0.13309550285339355\n",
      "epoch:1/10, batch:44/508, loss:0.0906161218881607\n",
      "epoch:1/10, batch:45/508, loss:0.06436991691589355\n",
      "epoch:1/10, batch:46/508, loss:0.106843002140522\n",
      "epoch:1/10, batch:47/508, loss:0.10574543476104736\n",
      "epoch:1/10, batch:48/508, loss:0.10789930075407028\n",
      "epoch:1/10, batch:49/508, loss:0.0982334092259407\n",
      "epoch:1/10, batch:50/508, loss:0.0789995789527893\n",
      "evaluating ....\n",
      "Dev Performance P@5 0.396273291925\n",
      "Dev Performance P@1 0.546583850932\n",
      "Dev Performance MAP 0.6317550376985139\n",
      "Dev Performance MRR 0.6712358689295116\n",
      "------------------\n",
      "Test Performance P@5 0.37106918239\n",
      "Test Performance P@1 0.553459119497\n",
      "Test Performance MAP 0.6430936512210027\n",
      "Test Performance MRR 0.6920103885595446\n",
      "epoch:1/10, batch:51/508, loss:0.08062314987182617\n",
      "epoch:1/10, batch:52/508, loss:0.06334889680147171\n",
      "epoch:1/10, batch:53/508, loss:0.0863930881023407\n",
      "epoch:1/10, batch:54/508, loss:0.08468839526176453\n",
      "epoch:1/10, batch:55/508, loss:0.09255748987197876\n",
      "epoch:1/10, batch:56/508, loss:0.07438299059867859\n",
      "epoch:1/10, batch:57/508, loss:0.10391288995742798\n",
      "epoch:1/10, batch:58/508, loss:0.10485994815826416\n",
      "epoch:1/10, batch:59/508, loss:0.1350363790988922\n",
      "epoch:1/10, batch:60/508, loss:0.11643565446138382\n",
      "epoch:1/10, batch:61/508, loss:0.08424875140190125\n",
      "epoch:1/10, batch:62/508, loss:0.08558327704668045\n",
      "epoch:1/10, batch:63/508, loss:0.1042114719748497\n",
      "epoch:1/10, batch:64/508, loss:0.1378486305475235\n",
      "epoch:1/10, batch:65/508, loss:0.10690349340438843\n",
      "epoch:1/10, batch:66/508, loss:0.07734782993793488\n",
      "epoch:1/10, batch:67/508, loss:0.08245107531547546\n",
      "epoch:1/10, batch:68/508, loss:0.06217960640788078\n",
      "epoch:1/10, batch:69/508, loss:0.07909134775400162\n",
      "epoch:1/10, batch:70/508, loss:0.06453556567430496\n",
      "epoch:1/10, batch:71/508, loss:0.10103791952133179\n",
      "epoch:1/10, batch:72/508, loss:0.0864960253238678\n",
      "epoch:1/10, batch:73/508, loss:0.07632187008857727\n",
      "epoch:1/10, batch:74/508, loss:0.09861849993467331\n",
      "epoch:1/10, batch:75/508, loss:0.10911431163549423\n",
      "evaluating ....\n",
      "Dev Performance P@5 0.398757763975\n",
      "Dev Performance P@1 0.552795031056\n",
      "Dev Performance MAP 0.6319955412091983\n",
      "Dev Performance MRR 0.6741629057622845\n",
      "------------------\n",
      "Test Performance P@5 0.367295597484\n",
      "Test Performance P@1 0.540880503145\n",
      "Test Performance MAP 0.6350623449130176\n",
      "Test Performance MRR 0.6831656518846192\n",
      "epoch:1/10, batch:76/508, loss:0.13024400174617767\n",
      "epoch:1/10, batch:77/508, loss:0.14584314823150635\n",
      "epoch:1/10, batch:78/508, loss:0.05074673891067505\n",
      "epoch:1/10, batch:79/508, loss:0.0887855663895607\n",
      "epoch:1/10, batch:80/508, loss:0.1306835263967514\n",
      "epoch:1/10, batch:81/508, loss:0.08532267063856125\n",
      "epoch:1/10, batch:82/508, loss:0.05905339494347572\n",
      "epoch:1/10, batch:83/508, loss:0.1268201619386673\n",
      "epoch:1/10, batch:84/508, loss:0.09161476045846939\n",
      "epoch:1/10, batch:85/508, loss:0.049834124743938446\n",
      "epoch:1/10, batch:86/508, loss:0.12088322639465332\n",
      "epoch:1/10, batch:87/508, loss:0.09998011589050293\n",
      "epoch:1/10, batch:88/508, loss:0.0831763744354248\n",
      "epoch:1/10, batch:89/508, loss:0.08975543826818466\n",
      "epoch:1/10, batch:90/508, loss:0.06727343797683716\n",
      "epoch:1/10, batch:91/508, loss:0.07930073142051697\n",
      "epoch:1/10, batch:92/508, loss:0.15656134486198425\n",
      "epoch:1/10, batch:93/508, loss:0.12136391550302505\n",
      "epoch:1/10, batch:94/508, loss:0.0762263759970665\n",
      "epoch:1/10, batch:95/508, loss:0.05167662352323532\n",
      "epoch:1/10, batch:96/508, loss:0.08286251872777939\n",
      "epoch:1/10, batch:97/508, loss:0.11194101721048355\n",
      "epoch:1/10, batch:98/508, loss:0.09669239073991776\n",
      "epoch:1/10, batch:99/508, loss:0.056707851588726044\n",
      "epoch:1/10, batch:100/508, loss:0.12711378931999207\n",
      "evaluating ....\n",
      "Dev Performance P@5 0.401242236025\n",
      "Dev Performance P@1 0.534161490683\n",
      "Dev Performance MAP 0.6322392796794208\n",
      "Dev Performance MRR 0.665773818902244\n",
      "------------------\n",
      "Test Performance P@5 0.377358490566\n",
      "Test Performance P@1 0.553459119497\n",
      "Test Performance MAP 0.6373235356280916\n",
      "Test Performance MRR 0.6909422044724925\n",
      "epoch:1/10, batch:101/508, loss:0.13125580549240112\n",
      "epoch:1/10, batch:102/508, loss:0.05956730246543884\n",
      "epoch:1/10, batch:103/508, loss:0.1365373581647873\n",
      "epoch:1/10, batch:104/508, loss:0.1930154711008072\n",
      "epoch:1/10, batch:105/508, loss:0.10307034105062485\n",
      "epoch:1/10, batch:106/508, loss:0.08513674139976501\n",
      "epoch:1/10, batch:107/508, loss:0.09382668882608414\n",
      "epoch:1/10, batch:108/508, loss:0.11004967242479324\n",
      "epoch:1/10, batch:109/508, loss:0.09546678513288498\n",
      "epoch:1/10, batch:110/508, loss:0.09889370203018188\n",
      "epoch:1/10, batch:111/508, loss:0.09249966591596603\n",
      "epoch:1/10, batch:112/508, loss:0.11335443705320358\n",
      "epoch:1/10, batch:113/508, loss:0.07320060580968857\n",
      "epoch:1/10, batch:114/508, loss:0.11562378704547882\n",
      "epoch:1/10, batch:115/508, loss:0.10292332619428635\n",
      "epoch:1/10, batch:116/508, loss:0.08245676010847092\n",
      "epoch:1/10, batch:117/508, loss:0.1132686510682106\n",
      "epoch:1/10, batch:118/508, loss:0.08106248080730438\n",
      "epoch:1/10, batch:119/508, loss:0.0560537688434124\n",
      "epoch:1/10, batch:120/508, loss:0.11922872811555862\n",
      "epoch:1/10, batch:121/508, loss:0.07680200785398483\n",
      "epoch:1/10, batch:122/508, loss:0.1209745705127716\n",
      "epoch:1/10, batch:123/508, loss:0.10672225803136826\n",
      "epoch:1/10, batch:124/508, loss:0.052520908415317535\n",
      "epoch:1/10, batch:125/508, loss:0.07695029675960541\n",
      "evaluating ....\n",
      "Dev Performance P@5 0.40248447205\n",
      "Dev Performance P@1 0.509316770186\n",
      "Dev Performance MAP 0.6200647195599989\n",
      "Dev Performance MRR 0.6509461552939816\n",
      "------------------\n",
      "Test Performance P@5 0.379874213836\n",
      "Test Performance P@1 0.553459119497\n",
      "Test Performance MAP 0.6346629004432877\n",
      "Test Performance MRR 0.6909233847913093\n",
      "epoch:1/10, batch:126/508, loss:0.0894295871257782\n",
      "epoch:1/10, batch:127/508, loss:0.07956308126449585\n",
      "epoch:1/10, batch:128/508, loss:0.07538924366235733\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1/10, batch:129/508, loss:0.07693225145339966\n",
      "epoch:1/10, batch:130/508, loss:0.13850216567516327\n",
      "epoch:1/10, batch:131/508, loss:0.1792794018983841\n",
      "epoch:1/10, batch:132/508, loss:0.09958145767450333\n",
      "epoch:1/10, batch:133/508, loss:0.08008226007223129\n",
      "epoch:1/10, batch:134/508, loss:0.1331702023744583\n",
      "epoch:1/10, batch:135/508, loss:0.07687525451183319\n",
      "epoch:1/10, batch:136/508, loss:0.08159011602401733\n",
      "epoch:1/10, batch:137/508, loss:0.08214867860078812\n",
      "epoch:1/10, batch:138/508, loss:0.08653660118579865\n",
      "epoch:1/10, batch:139/508, loss:0.08022867143154144\n",
      "epoch:1/10, batch:140/508, loss:0.0887611135840416\n",
      "epoch:1/10, batch:141/508, loss:0.12374649196863174\n",
      "epoch:1/10, batch:142/508, loss:0.08957083523273468\n",
      "epoch:1/10, batch:143/508, loss:0.07674036175012589\n",
      "epoch:1/10, batch:144/508, loss:0.07492111623287201\n",
      "epoch:1/10, batch:145/508, loss:0.06743984669446945\n",
      "epoch:1/10, batch:146/508, loss:0.09372374415397644\n",
      "epoch:1/10, batch:147/508, loss:0.07385239005088806\n",
      "epoch:1/10, batch:148/508, loss:0.0731256827712059\n",
      "epoch:1/10, batch:149/508, loss:0.08190370351076126\n",
      "epoch:1/10, batch:150/508, loss:0.07792524248361588\n",
      "evaluating ....\n",
      "Dev Performance P@5 0.39751552795\n",
      "Dev Performance P@1 0.509316770186\n",
      "Dev Performance MAP 0.6152610100714928\n",
      "Dev Performance MRR 0.650499382381918\n",
      "------------------\n",
      "Test Performance P@5 0.372327044025\n",
      "Test Performance P@1 0.540880503145\n",
      "Test Performance MAP 0.6338418638511351\n",
      "Test Performance MRR 0.6842842168313867\n",
      "epoch:1/10, batch:151/508, loss:0.1624450981616974\n",
      "epoch:1/10, batch:152/508, loss:0.10843338817358017\n",
      "epoch:1/10, batch:153/508, loss:0.11153457313776016\n",
      "epoch:1/10, batch:154/508, loss:0.11004538834095001\n",
      "epoch:1/10, batch:155/508, loss:0.10926898568868637\n",
      "epoch:1/10, batch:156/508, loss:0.09694527834653854\n",
      "epoch:1/10, batch:157/508, loss:0.07450110465288162\n",
      "epoch:1/10, batch:158/508, loss:0.11218985915184021\n",
      "epoch:1/10, batch:159/508, loss:0.1028914749622345\n",
      "epoch:1/10, batch:160/508, loss:0.09550872445106506\n",
      "epoch:1/10, batch:161/508, loss:0.10349217802286148\n",
      "epoch:1/10, batch:162/508, loss:0.1310771256685257\n",
      "epoch:1/10, batch:163/508, loss:0.11152733862400055\n",
      "epoch:1/10, batch:164/508, loss:0.13888685405254364\n",
      "epoch:1/10, batch:165/508, loss:0.08619707077741623\n",
      "epoch:1/10, batch:166/508, loss:0.1276395618915558\n",
      "epoch:1/10, batch:167/508, loss:0.08837952464818954\n",
      "epoch:1/10, batch:168/508, loss:0.08712615072727203\n",
      "epoch:1/10, batch:169/508, loss:0.053584251552820206\n",
      "epoch:1/10, batch:170/508, loss:0.09967946261167526\n",
      "epoch:1/10, batch:171/508, loss:0.11760330200195312\n",
      "epoch:1/10, batch:172/508, loss:0.09466119855642319\n",
      "epoch:1/10, batch:173/508, loss:0.10036542266607285\n",
      "epoch:1/10, batch:174/508, loss:0.0862615779042244\n",
      "epoch:1/10, batch:175/508, loss:0.0821487307548523\n",
      "evaluating ....\n",
      "Dev Performance P@5 0.40248447205\n",
      "Dev Performance P@1 0.521739130435\n",
      "Dev Performance MAP 0.6259343678962637\n",
      "Dev Performance MRR 0.6547749265916969\n",
      "------------------\n",
      "Test Performance P@5 0.363522012579\n",
      "Test Performance P@1 0.553459119497\n",
      "Test Performance MAP 0.6363696793853503\n",
      "Test Performance MRR 0.6946979351150155\n",
      "epoch:1/10, batch:176/508, loss:0.0974736139178276\n",
      "epoch:1/10, batch:177/508, loss:0.07315541803836823\n",
      "epoch:1/10, batch:178/508, loss:0.09990042448043823\n",
      "epoch:1/10, batch:179/508, loss:0.0783471167087555\n",
      "epoch:1/10, batch:180/508, loss:0.132586270570755\n",
      "epoch:1/10, batch:181/508, loss:0.10325029492378235\n",
      "epoch:1/10, batch:182/508, loss:0.06321092694997787\n",
      "epoch:1/10, batch:183/508, loss:0.13183388113975525\n",
      "epoch:1/10, batch:184/508, loss:0.09698713570833206\n",
      "epoch:1/10, batch:185/508, loss:0.11108685284852982\n",
      "epoch:1/10, batch:186/508, loss:0.0925474464893341\n",
      "epoch:1/10, batch:187/508, loss:0.16167770326137543\n",
      "epoch:1/10, batch:188/508, loss:0.10035780072212219\n",
      "epoch:1/10, batch:189/508, loss:0.08594485372304916\n",
      "epoch:1/10, batch:190/508, loss:0.09673699736595154\n",
      "epoch:1/10, batch:191/508, loss:0.09460017085075378\n",
      "epoch:1/10, batch:192/508, loss:0.1279289722442627\n",
      "epoch:1/10, batch:193/508, loss:0.06262768805027008\n",
      "epoch:1/10, batch:194/508, loss:0.08235662430524826\n",
      "epoch:1/10, batch:195/508, loss:0.10781591385602951\n",
      "epoch:1/10, batch:196/508, loss:0.0631660670042038\n",
      "epoch:1/10, batch:197/508, loss:0.11381223052740097\n",
      "epoch:1/10, batch:198/508, loss:0.11841119825839996\n",
      "epoch:1/10, batch:199/508, loss:0.11672071367502213\n",
      "epoch:1/10, batch:200/508, loss:0.09759689122438431\n",
      "evaluating ....\n",
      "Dev Performance P@5 0.40248447205\n",
      "Dev Performance P@1 0.527950310559\n",
      "Dev Performance MAP 0.6259260528065057\n",
      "Dev Performance MRR 0.6635609671049933\n",
      "------------------\n",
      "Test Performance P@5 0.358490566038\n",
      "Test Performance P@1 0.522012578616\n",
      "Test Performance MAP 0.6242936390826697\n",
      "Test Performance MRR 0.6757284288536113\n",
      "epoch:1/10, batch:201/508, loss:0.13990454375743866\n",
      "epoch:1/10, batch:202/508, loss:0.1074666902422905\n",
      "epoch:1/10, batch:203/508, loss:0.0748860314488411\n",
      "epoch:1/10, batch:204/508, loss:0.08867499977350235\n",
      "epoch:1/10, batch:205/508, loss:0.08859983831644058\n",
      "epoch:1/10, batch:206/508, loss:0.12255876511335373\n",
      "epoch:1/10, batch:207/508, loss:0.07415802776813507\n",
      "epoch:1/10, batch:208/508, loss:0.08056632429361343\n",
      "epoch:1/10, batch:209/508, loss:0.06468657404184341\n",
      "epoch:1/10, batch:210/508, loss:0.13608482480049133\n",
      "epoch:1/10, batch:211/508, loss:0.10920488834381104\n",
      "epoch:1/10, batch:212/508, loss:0.15231971442699432\n",
      "epoch:1/10, batch:213/508, loss:0.054703712463378906\n",
      "epoch:1/10, batch:214/508, loss:0.10963921993970871\n",
      "epoch:1/10, batch:215/508, loss:0.10852828621864319\n",
      "epoch:1/10, batch:216/508, loss:0.0546637624502182\n",
      "epoch:1/10, batch:217/508, loss:0.09320856630802155\n",
      "epoch:1/10, batch:218/508, loss:0.09688805788755417\n",
      "epoch:1/10, batch:219/508, loss:0.04873654991388321\n",
      "epoch:1/10, batch:220/508, loss:0.06656661629676819\n",
      "epoch:1/10, batch:221/508, loss:0.07249920815229416\n",
      "epoch:1/10, batch:222/508, loss:0.10229188203811646\n",
      "epoch:1/10, batch:223/508, loss:0.1331184208393097\n",
      "epoch:1/10, batch:224/508, loss:0.07479099929332733\n",
      "epoch:1/10, batch:225/508, loss:0.09535697102546692\n",
      "evaluating ....\n",
      "Dev Performance P@5 0.403726708075\n",
      "Dev Performance P@1 0.540372670807\n",
      "Dev Performance MAP 0.6345699973814173\n",
      "Dev Performance MRR 0.6674000688752241\n",
      "------------------\n",
      "Test Performance P@5 0.362264150943\n",
      "Test Performance P@1 0.503144654088\n",
      "Test Performance MAP 0.6247515137985375\n",
      "Test Performance MRR 0.6669377847133557\n",
      "epoch:1/10, batch:226/508, loss:0.06968484818935394\n",
      "epoch:1/10, batch:227/508, loss:0.07189162820577621\n",
      "epoch:1/10, batch:228/508, loss:0.09150063991546631\n",
      "epoch:1/10, batch:229/508, loss:0.14240628480911255\n",
      "epoch:1/10, batch:230/508, loss:0.05970524251461029\n",
      "epoch:1/10, batch:231/508, loss:0.07065564393997192\n",
      "epoch:1/10, batch:232/508, loss:0.1421992927789688\n",
      "epoch:1/10, batch:233/508, loss:0.12315484136343002\n",
      "epoch:1/10, batch:234/508, loss:0.05538032948970795\n",
      "epoch:1/10, batch:235/508, loss:0.12702953815460205\n",
      "epoch:1/10, batch:236/508, loss:0.08818347007036209\n",
      "epoch:1/10, batch:237/508, loss:0.10781201720237732\n",
      "epoch:1/10, batch:238/508, loss:0.14951004087924957\n",
      "epoch:1/10, batch:239/508, loss:0.11266972869634628\n",
      "epoch:1/10, batch:240/508, loss:0.13263975083827972\n",
      "epoch:1/10, batch:241/508, loss:0.08920857310295105\n",
      "epoch:1/10, batch:242/508, loss:0.08240243047475815\n",
      "epoch:1/10, batch:243/508, loss:0.06847730278968811\n",
      "epoch:1/10, batch:244/508, loss:0.09919171035289764\n",
      "epoch:1/10, batch:245/508, loss:0.036874182522296906\n",
      "epoch:1/10, batch:246/508, loss:0.09153592586517334\n",
      "epoch:1/10, batch:247/508, loss:0.07544687390327454\n",
      "epoch:1/10, batch:248/508, loss:0.06987521797418594\n",
      "epoch:1/10, batch:249/508, loss:0.09178340435028076\n",
      "epoch:1/10, batch:250/508, loss:0.1385684460401535\n",
      "evaluating ....\n",
      "Dev Performance P@5 0.393788819876\n",
      "Dev Performance P@1 0.534161490683\n",
      "Dev Performance MAP 0.6306999173851299\n",
      "Dev Performance MRR 0.6617561380486116\n",
      "------------------\n",
      "Test Performance P@5 0.366037735849\n",
      "Test Performance P@1 0.522012578616\n",
      "Test Performance MAP 0.6315066408085728\n",
      "Test Performance MRR 0.672320365095936\n",
      "epoch:1/10, batch:251/508, loss:0.06447917968034744\n",
      "epoch:1/10, batch:252/508, loss:0.05694538354873657\n",
      "epoch:1/10, batch:253/508, loss:0.11975753307342529\n",
      "epoch:1/10, batch:254/508, loss:0.10379277914762497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1/10, batch:255/508, loss:0.06415783613920212\n",
      "epoch:1/10, batch:256/508, loss:0.11833930760622025\n",
      "epoch:1/10, batch:257/508, loss:0.1142667680978775\n",
      "epoch:1/10, batch:258/508, loss:0.12293071299791336\n",
      "epoch:1/10, batch:259/508, loss:0.13567987084388733\n",
      "epoch:1/10, batch:260/508, loss:0.13079805672168732\n",
      "epoch:1/10, batch:261/508, loss:0.0925537720322609\n",
      "epoch:1/10, batch:262/508, loss:0.04757543280720711\n",
      "epoch:1/10, batch:263/508, loss:0.07988748699426651\n",
      "epoch:1/10, batch:264/508, loss:0.14379945397377014\n",
      "epoch:1/10, batch:265/508, loss:0.055405762046575546\n",
      "epoch:1/10, batch:266/508, loss:0.11227753013372421\n",
      "epoch:1/10, batch:267/508, loss:0.11142270267009735\n",
      "epoch:1/10, batch:268/508, loss:0.11688664555549622\n",
      "epoch:1/10, batch:269/508, loss:0.07529149204492569\n",
      "epoch:1/10, batch:270/508, loss:0.09170898795127869\n",
      "epoch:1/10, batch:271/508, loss:0.11453507095575333\n",
      "epoch:1/10, batch:272/508, loss:0.07868916541337967\n",
      "epoch:1/10, batch:273/508, loss:0.07169362157583237\n",
      "epoch:1/10, batch:274/508, loss:0.0596579946577549\n",
      "epoch:1/10, batch:275/508, loss:0.07881525158882141\n",
      "evaluating ....\n",
      "Dev Performance P@5 0.390062111801\n",
      "Dev Performance P@1 0.540372670807\n",
      "Dev Performance MAP 0.6327591935259509\n",
      "Dev Performance MRR 0.6693389081587839\n",
      "------------------\n",
      "Test Performance P@5 0.363522012579\n",
      "Test Performance P@1 0.540880503145\n",
      "Test Performance MAP 0.6416920774834587\n",
      "Test Performance MRR 0.6854914707198717\n",
      "epoch:1/10, batch:276/508, loss:0.08226849883794785\n",
      "epoch:1/10, batch:277/508, loss:0.1152099221944809\n",
      "epoch:1/10, batch:278/508, loss:0.09281931072473526\n",
      "epoch:1/10, batch:279/508, loss:0.06164008751511574\n",
      "epoch:1/10, batch:280/508, loss:0.08368285745382309\n",
      "epoch:1/10, batch:281/508, loss:0.0432741641998291\n",
      "epoch:1/10, batch:282/508, loss:0.09423283487558365\n",
      "epoch:1/10, batch:283/508, loss:0.09790488332509995\n",
      "epoch:1/10, batch:284/508, loss:0.08785443753004074\n",
      "epoch:1/10, batch:285/508, loss:0.1145646944642067\n",
      "epoch:1/10, batch:286/508, loss:0.09085416793823242\n",
      "epoch:1/10, batch:287/508, loss:0.08456757664680481\n",
      "epoch:1/10, batch:288/508, loss:0.055509988218545914\n",
      "epoch:1/10, batch:289/508, loss:0.1195489689707756\n",
      "epoch:1/10, batch:290/508, loss:0.07321751862764359\n",
      "epoch:1/10, batch:291/508, loss:0.10535091161727905\n",
      "epoch:1/10, batch:292/508, loss:0.0860862284898758\n",
      "epoch:1/10, batch:293/508, loss:0.11577928811311722\n",
      "epoch:1/10, batch:294/508, loss:0.11330744624137878\n",
      "epoch:1/10, batch:295/508, loss:0.10578912496566772\n",
      "epoch:1/10, batch:296/508, loss:0.08704356104135513\n",
      "epoch:1/10, batch:297/508, loss:0.10163417458534241\n",
      "epoch:1/10, batch:298/508, loss:0.11783529818058014\n",
      "epoch:1/10, batch:299/508, loss:0.09636133164167404\n",
      "epoch:1/10, batch:300/508, loss:0.09384574741125107\n",
      "evaluating ....\n",
      "Dev Performance P@5 0.393788819876\n",
      "Dev Performance P@1 0.534161490683\n",
      "Dev Performance MAP 0.6291466861397834\n",
      "Dev Performance MRR 0.6665545893030363\n",
      "------------------\n",
      "Test Performance P@5 0.372327044025\n",
      "Test Performance P@1 0.547169811321\n",
      "Test Performance MAP 0.6377153489493214\n",
      "Test Performance MRR 0.6869080398251204\n",
      "epoch:1/10, batch:301/508, loss:0.14816628396511078\n",
      "epoch:1/10, batch:302/508, loss:0.06777849793434143\n",
      "epoch:1/10, batch:303/508, loss:0.10971201211214066\n",
      "epoch:1/10, batch:304/508, loss:0.12385252863168716\n",
      "epoch:1/10, batch:305/508, loss:0.11284875869750977\n",
      "epoch:1/10, batch:306/508, loss:0.07754956185817719\n",
      "epoch:1/10, batch:307/508, loss:0.07397990673780441\n",
      "epoch:1/10, batch:308/508, loss:0.09293182939291\n",
      "epoch:1/10, batch:309/508, loss:0.10540611296892166\n",
      "epoch:1/10, batch:310/508, loss:0.07447684556245804\n",
      "epoch:1/10, batch:311/508, loss:0.07883291691541672\n",
      "epoch:1/10, batch:312/508, loss:0.10305911302566528\n",
      "epoch:1/10, batch:313/508, loss:0.07122624665498734\n",
      "epoch:1/10, batch:314/508, loss:0.06796037405729294\n",
      "epoch:1/10, batch:315/508, loss:0.10510843992233276\n",
      "epoch:1/10, batch:316/508, loss:0.11927977204322815\n",
      "epoch:1/10, batch:317/508, loss:0.16670529544353485\n",
      "epoch:1/10, batch:318/508, loss:0.10255254060029984\n",
      "epoch:1/10, batch:319/508, loss:0.10608616471290588\n",
      "epoch:1/10, batch:320/508, loss:0.08450935781002045\n",
      "epoch:1/10, batch:321/508, loss:0.08520770072937012\n",
      "epoch:1/10, batch:322/508, loss:0.07506417483091354\n",
      "epoch:1/10, batch:323/508, loss:0.08651882410049438\n",
      "epoch:1/10, batch:324/508, loss:0.113801509141922\n",
      "epoch:1/10, batch:325/508, loss:0.09859250485897064\n",
      "evaluating ....\n",
      "Dev Performance P@5 0.388819875776\n",
      "Dev Performance P@1 0.503105590062\n",
      "Dev Performance MAP 0.6145820400245012\n",
      "Dev Performance MRR 0.6462430624423682\n",
      "------------------\n",
      "Test Performance P@5 0.37106918239\n",
      "Test Performance P@1 0.547169811321\n",
      "Test Performance MAP 0.6372899849623698\n",
      "Test Performance MRR 0.6837746166445273\n",
      "epoch:1/10, batch:326/508, loss:0.07642469555139542\n",
      "epoch:1/10, batch:327/508, loss:0.08500109612941742\n",
      "epoch:1/10, batch:328/508, loss:0.08344951272010803\n",
      "epoch:1/10, batch:329/508, loss:0.10230544954538345\n",
      "epoch:1/10, batch:330/508, loss:0.13010594248771667\n",
      "epoch:1/10, batch:331/508, loss:0.12112222611904144\n",
      "epoch:1/10, batch:332/508, loss:0.10692213475704193\n",
      "epoch:1/10, batch:333/508, loss:0.10823137313127518\n",
      "epoch:1/10, batch:334/508, loss:0.1593254655599594\n",
      "epoch:1/10, batch:335/508, loss:0.07663996517658234\n",
      "epoch:1/10, batch:336/508, loss:0.0775010883808136\n",
      "epoch:1/10, batch:337/508, loss:0.09196067601442337\n",
      "epoch:1/10, batch:338/508, loss:0.0944981500506401\n",
      "epoch:1/10, batch:339/508, loss:0.05747395008802414\n",
      "epoch:1/10, batch:340/508, loss:0.09366963803768158\n",
      "epoch:1/10, batch:341/508, loss:0.10907474160194397\n",
      "epoch:1/10, batch:342/508, loss:0.08733305335044861\n",
      "epoch:1/10, batch:343/508, loss:0.17843356728553772\n",
      "epoch:1/10, batch:344/508, loss:0.08832713961601257\n",
      "epoch:1/10, batch:345/508, loss:0.16761134564876556\n",
      "epoch:1/10, batch:346/508, loss:0.051258787512779236\n",
      "epoch:1/10, batch:347/508, loss:0.09421289712190628\n",
      "epoch:1/10, batch:348/508, loss:0.11243560910224915\n",
      "epoch:1/10, batch:349/508, loss:0.11298510432243347\n",
      "epoch:1/10, batch:350/508, loss:0.13147388398647308\n",
      "evaluating ....\n",
      "Dev Performance P@5 0.393788819876\n",
      "Dev Performance P@1 0.509316770186\n",
      "Dev Performance MAP 0.6154263255526277\n",
      "Dev Performance MRR 0.6556854291326341\n",
      "------------------\n",
      "Test Performance P@5 0.37106918239\n",
      "Test Performance P@1 0.534591194969\n",
      "Test Performance MAP 0.6391548314648258\n",
      "Test Performance MRR 0.6805302657309201\n",
      "epoch:1/10, batch:351/508, loss:0.10716447234153748\n",
      "epoch:1/10, batch:352/508, loss:0.09357216954231262\n",
      "epoch:1/10, batch:353/508, loss:0.08933387696743011\n",
      "epoch:1/10, batch:354/508, loss:0.09180402755737305\n",
      "epoch:1/10, batch:355/508, loss:0.1461373120546341\n",
      "epoch:1/10, batch:356/508, loss:0.1257781982421875\n",
      "epoch:1/10, batch:357/508, loss:0.08339527994394302\n",
      "epoch:1/10, batch:358/508, loss:0.08890422433614731\n",
      "epoch:1/10, batch:359/508, loss:0.09692772477865219\n",
      "epoch:1/10, batch:360/508, loss:0.07889004796743393\n",
      "epoch:1/10, batch:361/508, loss:0.09892306476831436\n",
      "epoch:1/10, batch:362/508, loss:0.06454852223396301\n",
      "epoch:1/10, batch:363/508, loss:0.12997353076934814\n",
      "epoch:1/10, batch:364/508, loss:0.1631273776292801\n",
      "epoch:1/10, batch:365/508, loss:0.08251228928565979\n",
      "epoch:1/10, batch:366/508, loss:0.07014946639537811\n",
      "epoch:1/10, batch:367/508, loss:0.10118184238672256\n",
      "epoch:1/10, batch:368/508, loss:0.07613415271043777\n",
      "epoch:1/10, batch:369/508, loss:0.11859096586704254\n",
      "epoch:1/10, batch:370/508, loss:0.10052816569805145\n",
      "epoch:1/10, batch:371/508, loss:0.1160757765173912\n",
      "epoch:1/10, batch:372/508, loss:0.11541244387626648\n",
      "epoch:1/10, batch:373/508, loss:0.10327842086553574\n",
      "epoch:1/10, batch:374/508, loss:0.12329556792974472\n",
      "epoch:1/10, batch:375/508, loss:0.05470722168684006\n",
      "evaluating ....\n",
      "Dev Performance P@5 0.4\n",
      "Dev Performance P@1 0.496894409938\n",
      "Dev Performance MAP 0.6075673662572748\n",
      "Dev Performance MRR 0.6398255151863118\n",
      "------------------\n",
      "Test Performance P@5 0.369811320755\n",
      "Test Performance P@1 0.559748427673\n",
      "Test Performance MAP 0.6395953633254509\n",
      "Test Performance MRR 0.6937720091890895\n",
      "epoch:1/10, batch:376/508, loss:0.1994979828596115\n",
      "epoch:1/10, batch:377/508, loss:0.08845891058444977\n",
      "epoch:1/10, batch:378/508, loss:0.08019281178712845\n",
      "epoch:1/10, batch:379/508, loss:0.0711992010474205\n",
      "epoch:1/10, batch:380/508, loss:0.10963355004787445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1/10, batch:381/508, loss:0.05859388783574104\n",
      "epoch:1/10, batch:382/508, loss:0.1103350892663002\n",
      "epoch:1/10, batch:383/508, loss:0.08043211698532104\n",
      "epoch:1/10, batch:384/508, loss:0.06245983764529228\n",
      "epoch:1/10, batch:385/508, loss:0.11905223876237869\n",
      "epoch:1/10, batch:386/508, loss:0.07976298034191132\n",
      "epoch:1/10, batch:387/508, loss:0.0702722817659378\n",
      "epoch:1/10, batch:388/508, loss:0.09807973355054855\n",
      "epoch:1/10, batch:389/508, loss:0.13630197942256927\n",
      "epoch:1/10, batch:390/508, loss:0.06581863760948181\n",
      "epoch:1/10, batch:391/508, loss:0.06663747876882553\n",
      "epoch:1/10, batch:392/508, loss:0.08755485713481903\n",
      "epoch:1/10, batch:393/508, loss:0.11267269402742386\n",
      "epoch:1/10, batch:394/508, loss:0.10319242626428604\n",
      "epoch:1/10, batch:395/508, loss:0.17395727336406708\n",
      "epoch:1/10, batch:396/508, loss:0.1303078532218933\n",
      "epoch:1/10, batch:397/508, loss:0.07981802523136139\n",
      "epoch:1/10, batch:398/508, loss:0.07730397582054138\n",
      "epoch:1/10, batch:399/508, loss:0.14149557054042816\n",
      "epoch:1/10, batch:400/508, loss:0.12315766513347626\n",
      "evaluating ....\n",
      "Dev Performance P@5 0.395031055901\n",
      "Dev Performance P@1 0.496894409938\n",
      "Dev Performance MAP 0.6058301246500685\n",
      "Dev Performance MRR 0.6452599875462498\n",
      "------------------\n",
      "Test Performance P@5 0.37106918239\n",
      "Test Performance P@1 0.553459119497\n",
      "Test Performance MAP 0.6387575776758749\n",
      "Test Performance MRR 0.6895438680816038\n",
      "epoch:1/10, batch:401/508, loss:0.11837024986743927\n",
      "epoch:1/10, batch:402/508, loss:0.05888724699616432\n",
      "epoch:1/10, batch:403/508, loss:0.13006164133548737\n",
      "epoch:1/10, batch:404/508, loss:0.1686321347951889\n",
      "epoch:1/10, batch:405/508, loss:0.06937120854854584\n",
      "epoch:1/10, batch:406/508, loss:0.10224567353725433\n",
      "epoch:1/10, batch:407/508, loss:0.05967991426587105\n",
      "epoch:1/10, batch:408/508, loss:0.12808305025100708\n",
      "epoch:1/10, batch:409/508, loss:0.08562085032463074\n",
      "epoch:1/10, batch:410/508, loss:0.11354140937328339\n",
      "epoch:1/10, batch:411/508, loss:0.08273926377296448\n",
      "epoch:1/10, batch:412/508, loss:0.08542955666780472\n",
      "epoch:1/10, batch:413/508, loss:0.08456280082464218\n",
      "epoch:1/10, batch:414/508, loss:0.11509141325950623\n",
      "epoch:1/10, batch:415/508, loss:0.09231362491846085\n",
      "epoch:1/10, batch:416/508, loss:0.11966162174940109\n",
      "epoch:1/10, batch:417/508, loss:0.12701845169067383\n",
      "epoch:1/10, batch:418/508, loss:0.0661483183503151\n",
      "epoch:1/10, batch:419/508, loss:0.11187730729579926\n",
      "epoch:1/10, batch:420/508, loss:0.08338803052902222\n",
      "epoch:1/10, batch:421/508, loss:0.07053614407777786\n",
      "epoch:1/10, batch:422/508, loss:0.0929945558309555\n",
      "epoch:1/10, batch:423/508, loss:0.10476625710725784\n",
      "epoch:1/10, batch:424/508, loss:0.12665851414203644\n",
      "epoch:1/10, batch:425/508, loss:0.08253790438175201\n",
      "evaluating ....\n",
      "Dev Performance P@5 0.39751552795\n",
      "Dev Performance P@1 0.515527950311\n",
      "Dev Performance MAP 0.617642680485363\n",
      "Dev Performance MRR 0.6544663781190999\n",
      "------------------\n",
      "Test Performance P@5 0.367295597484\n",
      "Test Performance P@1 0.509433962264\n",
      "Test Performance MAP 0.6235824862704501\n",
      "Test Performance MRR 0.6637411734978765\n",
      "epoch:1/10, batch:426/508, loss:0.1250309944152832\n",
      "epoch:1/10, batch:427/508, loss:0.10237441956996918\n",
      "epoch:1/10, batch:428/508, loss:0.05496278405189514\n",
      "epoch:1/10, batch:429/508, loss:0.09450282156467438\n",
      "epoch:1/10, batch:430/508, loss:0.10918121784925461\n",
      "epoch:1/10, batch:431/508, loss:0.0900556743144989\n",
      "epoch:1/10, batch:432/508, loss:0.07696489989757538\n",
      "epoch:1/10, batch:433/508, loss:0.13119779527187347\n",
      "epoch:1/10, batch:434/508, loss:0.10728594660758972\n",
      "epoch:1/10, batch:435/508, loss:0.07828196883201599\n",
      "epoch:1/10, batch:436/508, loss:0.13936762511730194\n",
      "epoch:1/10, batch:437/508, loss:0.15792220830917358\n",
      "epoch:1/10, batch:438/508, loss:0.09188902378082275\n",
      "epoch:1/10, batch:439/508, loss:0.08271249383687973\n",
      "epoch:1/10, batch:440/508, loss:0.08671700954437256\n",
      "epoch:1/10, batch:441/508, loss:0.10332031548023224\n",
      "epoch:1/10, batch:442/508, loss:0.16569675505161285\n",
      "epoch:1/10, batch:443/508, loss:0.08391588181257248\n",
      "epoch:1/10, batch:444/508, loss:0.1087866798043251\n",
      "epoch:1/10, batch:445/508, loss:0.12024001032114029\n",
      "epoch:1/10, batch:446/508, loss:0.08655011653900146\n",
      "epoch:1/10, batch:447/508, loss:0.10059767216444016\n",
      "epoch:1/10, batch:448/508, loss:0.07434983551502228\n",
      "epoch:1/10, batch:449/508, loss:0.08601171523332596\n",
      "epoch:1/10, batch:450/508, loss:0.05607173964381218\n",
      "evaluating ....\n",
      "Dev Performance P@5 0.39751552795\n",
      "Dev Performance P@1 0.540372670807\n",
      "Dev Performance MAP 0.6268474810383652\n",
      "Dev Performance MRR 0.6670033296586091\n",
      "------------------\n",
      "Test Performance P@5 0.366037735849\n",
      "Test Performance P@1 0.528301886792\n",
      "Test Performance MAP 0.6365618977431653\n",
      "Test Performance MRR 0.6834649158631283\n",
      "epoch:1/10, batch:451/508, loss:0.10967805981636047\n",
      "epoch:1/10, batch:452/508, loss:0.07550826668739319\n",
      "epoch:1/10, batch:453/508, loss:0.14838644862174988\n",
      "epoch:1/10, batch:454/508, loss:0.09776127338409424\n",
      "epoch:1/10, batch:455/508, loss:0.06876566261053085\n",
      "epoch:1/10, batch:456/508, loss:0.09670577943325043\n",
      "epoch:1/10, batch:457/508, loss:0.08377943187952042\n",
      "epoch:1/10, batch:458/508, loss:0.0904071256518364\n",
      "epoch:1/10, batch:459/508, loss:0.08577993512153625\n",
      "epoch:1/10, batch:460/508, loss:0.14836235344409943\n",
      "epoch:1/10, batch:461/508, loss:0.07486501336097717\n",
      "epoch:1/10, batch:462/508, loss:0.08990495651960373\n",
      "epoch:1/10, batch:463/508, loss:0.11242879927158356\n",
      "epoch:1/10, batch:464/508, loss:0.08045274764299393\n",
      "epoch:1/10, batch:465/508, loss:0.09903677552938461\n",
      "epoch:1/10, batch:466/508, loss:0.10722941160202026\n",
      "epoch:1/10, batch:467/508, loss:0.10600501298904419\n",
      "epoch:1/10, batch:468/508, loss:0.10172662138938904\n",
      "epoch:1/10, batch:469/508, loss:0.09449373185634613\n",
      "epoch:1/10, batch:470/508, loss:0.10592979192733765\n",
      "epoch:1/10, batch:471/508, loss:0.09737037867307663\n",
      "epoch:1/10, batch:472/508, loss:0.1127714216709137\n",
      "epoch:1/10, batch:473/508, loss:0.11091208457946777\n",
      "epoch:1/10, batch:474/508, loss:0.06139582395553589\n",
      "epoch:1/10, batch:475/508, loss:0.06611786037683487\n",
      "evaluating ....\n",
      "Dev Performance P@5 0.398757763975\n",
      "Dev Performance P@1 0.534161490683\n",
      "Dev Performance MAP 0.6148099259207116\n",
      "Dev Performance MRR 0.6605567422063586\n",
      "------------------\n",
      "Test Performance P@5 0.372327044025\n",
      "Test Performance P@1 0.547169811321\n",
      "Test Performance MAP 0.6362077158055849\n",
      "Test Performance MRR 0.6891670284917555\n",
      "epoch:1/10, batch:476/508, loss:0.09776248037815094\n",
      "epoch:1/10, batch:477/508, loss:0.11659985780715942\n",
      "epoch:1/10, batch:478/508, loss:0.09308762848377228\n",
      "epoch:1/10, batch:479/508, loss:0.07372993230819702\n",
      "epoch:1/10, batch:480/508, loss:0.06303109228610992\n",
      "epoch:1/10, batch:481/508, loss:0.08141070604324341\n",
      "epoch:1/10, batch:482/508, loss:0.16243454813957214\n",
      "epoch:1/10, batch:483/508, loss:0.1456579864025116\n",
      "epoch:1/10, batch:484/508, loss:0.057357266545295715\n",
      "epoch:1/10, batch:485/508, loss:0.03755265846848488\n",
      "epoch:1/10, batch:486/508, loss:0.08830154687166214\n",
      "epoch:1/10, batch:487/508, loss:0.0937575101852417\n",
      "epoch:1/10, batch:488/508, loss:0.1548076868057251\n",
      "epoch:1/10, batch:489/508, loss:0.10728415846824646\n",
      "epoch:1/10, batch:490/508, loss:0.11865005642175674\n",
      "epoch:1/10, batch:491/508, loss:0.09125390648841858\n",
      "epoch:1/10, batch:492/508, loss:0.07812958210706711\n",
      "epoch:1/10, batch:493/508, loss:0.08429903537034988\n",
      "epoch:1/10, batch:494/508, loss:0.08683633804321289\n",
      "epoch:1/10, batch:495/508, loss:0.10924214124679565\n",
      "epoch:1/10, batch:496/508, loss:0.13982021808624268\n",
      "epoch:1/10, batch:497/508, loss:0.08618638664484024\n",
      "epoch:1/10, batch:498/508, loss:0.0685567855834961\n",
      "epoch:1/10, batch:499/508, loss:0.10091830790042877\n",
      "epoch:1/10, batch:500/508, loss:0.10691659152507782\n",
      "evaluating ....\n",
      "Dev Performance P@5 0.4\n",
      "Dev Performance P@1 0.515527950311\n",
      "Dev Performance MAP 0.6191719875684701\n",
      "Dev Performance MRR 0.6569621914844785\n",
      "------------------\n",
      "Test Performance P@5 0.369811320755\n",
      "Test Performance P@1 0.522012578616\n",
      "Test Performance MAP 0.6312904534290196\n",
      "Test Performance MRR 0.6747322613757569\n",
      "epoch:1/10, batch:501/508, loss:0.05582166463136673\n",
      "epoch:1/10, batch:502/508, loss:0.13156992197036743\n",
      "epoch:1/10, batch:503/508, loss:0.09172121435403824\n",
      "epoch:1/10, batch:504/508, loss:0.08156996965408325\n",
      "epoch:1/10, batch:505/508, loss:0.15214143693447113\n",
      "epoch:1/10, batch:506/508, loss:0.09394927322864532\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1/10, batch:507/508, loss:0.08553814888000488\n",
      "epoch:1/10, batch:508/508, loss:0.06603405624628067\n",
      "epoch:2/10, batch:1/508, loss:0.0494852289557457\n",
      "epoch:2/10, batch:2/508, loss:0.09168995916843414\n",
      "epoch:2/10, batch:3/508, loss:0.1366259753704071\n",
      "epoch:2/10, batch:4/508, loss:0.06433810293674469\n",
      "epoch:2/10, batch:5/508, loss:0.11617773026227951\n",
      "epoch:2/10, batch:6/508, loss:0.10645077377557755\n",
      "epoch:2/10, batch:7/508, loss:0.06478830426931381\n",
      "epoch:2/10, batch:8/508, loss:0.057500630617141724\n",
      "epoch:2/10, batch:9/508, loss:0.09177706390619278\n",
      "epoch:2/10, batch:10/508, loss:0.05528249964118004\n",
      "epoch:2/10, batch:11/508, loss:0.08676209300756454\n",
      "epoch:2/10, batch:12/508, loss:0.13217806816101074\n",
      "epoch:2/10, batch:13/508, loss:0.039482150226831436\n",
      "epoch:2/10, batch:14/508, loss:0.08643946796655655\n",
      "epoch:2/10, batch:15/508, loss:0.06677962094545364\n",
      "epoch:2/10, batch:16/508, loss:0.06086498126387596\n",
      "epoch:2/10, batch:17/508, loss:0.09507714956998825\n",
      "epoch:2/10, batch:18/508, loss:0.11380793899297714\n",
      "epoch:2/10, batch:19/508, loss:0.04666818305850029\n",
      "epoch:2/10, batch:20/508, loss:0.057266030460596085\n",
      "epoch:2/10, batch:21/508, loss:0.0672847330570221\n",
      "epoch:2/10, batch:22/508, loss:0.07443054765462875\n",
      "epoch:2/10, batch:23/508, loss:0.0779985561966896\n",
      "epoch:2/10, batch:24/508, loss:0.08234412223100662\n",
      "epoch:2/10, batch:25/508, loss:0.06520631164312363\n",
      "evaluating ....\n",
      "Dev Performance P@5 0.398757763975\n",
      "Dev Performance P@1 0.540372670807\n",
      "Dev Performance MAP 0.6244728072223983\n",
      "Dev Performance MRR 0.6644885221748575\n",
      "------------------\n",
      "Test Performance P@5 0.372327044025\n",
      "Test Performance P@1 0.509433962264\n",
      "Test Performance MAP 0.6260328976371611\n",
      "Test Performance MRR 0.6714494211515064\n",
      "epoch:2/10, batch:26/508, loss:0.0807989090681076\n",
      "epoch:2/10, batch:27/508, loss:0.11255399882793427\n",
      "epoch:2/10, batch:28/508, loss:0.09776899963617325\n",
      "epoch:2/10, batch:29/508, loss:0.07382635027170181\n",
      "epoch:2/10, batch:30/508, loss:0.11887882649898529\n",
      "epoch:2/10, batch:31/508, loss:0.09537364542484283\n",
      "epoch:2/10, batch:32/508, loss:0.12578260898590088\n",
      "epoch:2/10, batch:33/508, loss:0.09201749414205551\n",
      "epoch:2/10, batch:34/508, loss:0.08207583427429199\n",
      "epoch:2/10, batch:35/508, loss:0.08587627112865448\n",
      "epoch:2/10, batch:36/508, loss:0.10125857591629028\n",
      "epoch:2/10, batch:37/508, loss:0.0720670223236084\n",
      "epoch:2/10, batch:38/508, loss:0.0940249115228653\n",
      "epoch:2/10, batch:39/508, loss:0.06687455624341965\n",
      "epoch:2/10, batch:40/508, loss:0.06111526116728783\n",
      "epoch:2/10, batch:41/508, loss:0.07293219119310379\n",
      "epoch:2/10, batch:42/508, loss:0.10222618281841278\n",
      "epoch:2/10, batch:43/508, loss:0.10493946820497513\n",
      "epoch:2/10, batch:44/508, loss:0.0833999440073967\n",
      "epoch:2/10, batch:45/508, loss:0.06425470113754272\n",
      "epoch:2/10, batch:46/508, loss:0.07838782668113708\n",
      "epoch:2/10, batch:47/508, loss:0.115348681807518\n",
      "epoch:2/10, batch:48/508, loss:0.11875244230031967\n",
      "epoch:2/10, batch:49/508, loss:0.08406629413366318\n",
      "epoch:2/10, batch:50/508, loss:0.05369865149259567\n",
      "evaluating ....\n",
      "Dev Performance P@5 0.39751552795\n",
      "Dev Performance P@1 0.527950310559\n",
      "Dev Performance MAP 0.6238889898825349\n",
      "Dev Performance MRR 0.66053357443109\n",
      "------------------\n",
      "Test Performance P@5 0.37358490566\n",
      "Test Performance P@1 0.540880503145\n",
      "Test Performance MAP 0.6364376562801574\n",
      "Test Performance MRR 0.6910613899690363\n",
      "epoch:2/10, batch:51/508, loss:0.05779200419783592\n",
      "epoch:2/10, batch:52/508, loss:0.037133775651454926\n",
      "epoch:2/10, batch:53/508, loss:0.09366869926452637\n",
      "epoch:2/10, batch:54/508, loss:0.074076808989048\n",
      "epoch:2/10, batch:55/508, loss:0.07186826318502426\n",
      "epoch:2/10, batch:56/508, loss:0.04781918227672577\n",
      "epoch:2/10, batch:57/508, loss:0.11173315346240997\n",
      "epoch:2/10, batch:58/508, loss:0.08209291100502014\n",
      "epoch:2/10, batch:59/508, loss:0.10264778882265091\n",
      "epoch:2/10, batch:60/508, loss:0.08833697438240051\n",
      "epoch:2/10, batch:61/508, loss:0.0942598357796669\n",
      "epoch:2/10, batch:62/508, loss:0.07479603588581085\n",
      "epoch:2/10, batch:63/508, loss:0.09394000470638275\n",
      "epoch:2/10, batch:64/508, loss:0.12235111743211746\n",
      "epoch:2/10, batch:65/508, loss:0.09793756157159805\n",
      "epoch:2/10, batch:66/508, loss:0.0790124386548996\n",
      "epoch:2/10, batch:67/508, loss:0.07406698167324066\n",
      "epoch:2/10, batch:68/508, loss:0.07375633716583252\n",
      "epoch:2/10, batch:69/508, loss:0.05559702217578888\n",
      "epoch:2/10, batch:70/508, loss:0.06789081543684006\n",
      "epoch:2/10, batch:71/508, loss:0.0990455225110054\n",
      "epoch:2/10, batch:72/508, loss:0.07365027070045471\n",
      "epoch:2/10, batch:73/508, loss:0.07880620658397675\n",
      "epoch:2/10, batch:74/508, loss:0.09211309999227524\n",
      "epoch:2/10, batch:75/508, loss:0.09839669615030289\n",
      "evaluating ....\n",
      "Dev Performance P@5 0.396273291925\n",
      "Dev Performance P@1 0.55900621118\n",
      "Dev Performance MAP 0.6377799035636932\n",
      "Dev Performance MRR 0.674984225472899\n",
      "------------------\n",
      "Test Performance P@5 0.378616352201\n",
      "Test Performance P@1 0.547169811321\n",
      "Test Performance MAP 0.639069097972119\n",
      "Test Performance MRR 0.6886445774976064\n",
      "epoch:2/10, batch:76/508, loss:0.11905147135257721\n",
      "epoch:2/10, batch:77/508, loss:0.14102624356746674\n",
      "epoch:2/10, batch:78/508, loss:0.07363811135292053\n",
      "epoch:2/10, batch:79/508, loss:0.11119291931390762\n",
      "epoch:2/10, batch:80/508, loss:0.10394217073917389\n",
      "epoch:2/10, batch:81/508, loss:0.06397148966789246\n",
      "epoch:2/10, batch:82/508, loss:0.06368187814950943\n",
      "epoch:2/10, batch:83/508, loss:0.10095935314893723\n",
      "epoch:2/10, batch:84/508, loss:0.09607250988483429\n",
      "epoch:2/10, batch:85/508, loss:0.04952017962932587\n",
      "epoch:2/10, batch:86/508, loss:0.11339393258094788\n",
      "epoch:2/10, batch:87/508, loss:0.09580808132886887\n",
      "epoch:2/10, batch:88/508, loss:0.07970412075519562\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-161-a9cbc0121e4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel_margin_p3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEmbeddingLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHIDDEN_DIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lstm'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'lstm'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-160-36ef405c2c05>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(layer_type, embedding_layer, batch_size, num_epoch, id_set, eval)\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mbody_qs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_body\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#, requires_grad=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle_qs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody_qs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;31m#             blocked_embeddings = embeddings.view(-1, 22, HIDDEN_DIM * 2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m#             q_vecs = blocked_embeddings[:,0,:]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-134-98d371bbc73f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, title, body, title_len, body_len)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mtitle_lstm_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle_hidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m                                                                    \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle_hidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0mbody_lstm_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody_hidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m                                                                    \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody_hidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0mflat_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         )\n\u001b[0;32m--> 162\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_packed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPackedSequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, *fargs, **fkwargs)\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutogradRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, weight, hidden)\u001b[0m\n\u001b[1;32m    242\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m         \u001b[0mnexth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_first\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, hidden, weight)\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_directions\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                 \u001b[0mhy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m                 \u001b[0mnext_hidden\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mall_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, hidden, weight)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0msteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreverse\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m             \u001b[0;31m# hack to handle LSTM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mLSTMCell\u001b[0;34m(input, hidden, w_ih, w_hh, b_ih, b_hh)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mingate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mingate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mforgetgate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforgetgate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mcellgate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcellgate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0moutgate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutgate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mtanh\u001b[0;34m(input)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_autograd_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTanh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/autograd/_functions/pointwise.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, i, inplace)\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_for_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_margin_p3 = EmbeddingLayer(200, HIDDEN_DIM, 'lstm')\n",
    "train('lstm', model, batch_size=25, num_epoch=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "qids = list(train_idx_set.keys())[:25]\n",
    "t, b, tl, bl = process_contxt_batch(qids, train_idx_set, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = EmbeddingLayer(200, 667, 'cnn')\n",
    "criterion = multi_margin_loss(hidden=embedding_layer.hidden_size)\n",
    "# embedding_layer.title_hidden = embedding_layer.init_hidden(t.shape[1])\n",
    "# embedding_layer.body_hidden = embedding_layer.init_hidden(b.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([902, 30, 200])"
      ]
     },
     "execution_count": 556,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_qs = Variable(torch.FloatTensor(t))\n",
    "body_qs = Variable(torch.FloatTensor(b))\n",
    "title_qs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after transpose: torch.Size([902, 200, 30])\n",
      "after embedding: torch.Size([902, 667, 28])\n",
      "after transpose: torch.Size([902, 28, 667])\n",
      "after transpose: torch.Size([28, 902, 667])\n"
     ]
    }
   ],
   "source": [
    "emb = embedding_layer(title_qs, body_qs, tl, bl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmask = Variable(torch.FloatTensor(build_mask3d(tl - 3 + 1, np.max(tl) - 3 + 1)))\n",
    "# bmask = Variable(torch.FloatTensor(build_mask3d(bl - 3 + 1, np.max(bl) - 3 + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # te \n",
    "# # tmask = Variable(torch.FloatTensor(build_mask3d(tl - 3 + 1, np.max(tl) - 3 + 1)))\n",
    "# # bmask = Variable(torch.FloatTensor(build_mask3d(bl - 3 + 1, np.max(bl) - 3 + 1)))\n",
    "# temb = (torch.sum(torch.transpose(te, 0,1) * tmask, dim=0) / torch.sum(tmask, dim=0))\n",
    "# bemb = (torch.sum(torch.transpose(be, 0,1) * bmask, dim=0) / torch.sum(bmask, dim=0))\n",
    "# emb = (temb + bemb) / 2\n",
    "# # torch.sum(te * tmask, dim=1)[17*22: 18*22]\n",
    "# # te[17*22:18*22] * tmask[17*22:18*22]\n",
    "# # tmask[17*22:18*22], \n",
    "# # tl[17*22:18*22]\n",
    "# criterion(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target = Variable(torch.LongTensor([1,1] + [0]*20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = torch.nn.MultiMarginLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 1.0260\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(embeddings[:22], target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "blocked_embeddings = embeddings.view(-1, 22, 240)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q_vecs = blocked_embeddings[:,0,:]\n",
    "pos_vecs = blocked_embeddings[:,1,:]\n",
    "neg_vecs = blocked_embeddings[:,2:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_scores = torch.sum(q_vecs * pos_vecs, dim=1) / (torch.sqrt(torch.sum(q_vecs ** 2, dim=1)) \\\n",
    "                                    * torch.sqrt(torch.sum(pos_vecs ** 2, dim=1)))\n",
    "neg_scores = torch.sum(torch.unsqueeze(q_vecs, dim=1) * neg_vecs, dim=2) \\\n",
    "/ (torch.unsqueeze(torch.sqrt(torch.sum(q_vecs ** 2, dim=1)), dim=1) * torch.sqrt(torch.sum( neg_vecs ** 2, dim=2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Variable containing:\n",
       "  0.9956\n",
       " [torch.FloatTensor of size 1], Variable containing:\n",
       "  0.9919\n",
       "  0.9907\n",
       "  0.9932\n",
       "  0.9865\n",
       "  0.9866\n",
       "  0.9890\n",
       "  0.9923\n",
       "  0.9835\n",
       "  0.9876\n",
       "  0.9871\n",
       "  0.9884\n",
       "  0.9846\n",
       "  0.9868\n",
       "  0.9855\n",
       "  0.9879\n",
       "  0.9884\n",
       "  0.9886\n",
       "  0.9864\n",
       "  0.9892\n",
       "  0.9893\n",
       " [torch.FloatTensor of size 20])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_scores[0], neg_scores[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q_vecs = blocked_embeddings[:,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pn_vecs = blocked_embeddings[:,1:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores = torch.sum(torch.unsqueeze(q_vecs, dim=1) * pn_vecs, dim=2) \\\n",
    "/ (torch.unsqueeze(torch.sqrt(torch.sum(q_vecs ** 2, dim=1)), dim=1) * torch.sqrt(torch.sum( pn_vecs ** 2, dim=2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "\n",
       "Columns 0 to 9 \n",
       " 0.9956  0.9919  0.9907  0.9932  0.9865  0.9866  0.9890  0.9923  0.9835  0.9876\n",
       " 0.9903  0.9830  0.9841  0.9789  0.9809  0.9809  0.9718  0.9705  0.9728  0.9721\n",
       " 0.9930  0.9897  0.9855  0.9897  0.9890  0.9820  0.9798  0.9854  0.9760  0.9834\n",
       " 0.9867  0.9819  0.9823  0.9876  0.9819  0.9851  0.9798  0.9759  0.9894  0.9874\n",
       " 0.9920  0.9864  0.9870  0.9819  0.9849  0.9806  0.9822  0.9809  0.9870  0.9651\n",
       " 0.9929  0.9910  0.9900  0.9828  0.9876  0.9922  0.9914  0.9906  0.9916  0.9895\n",
       " 0.9964  0.9934  0.9812  0.9916  0.9866  0.9881  0.9790  0.9898  0.9876  0.9864\n",
       " 0.9941  0.9806  0.9755  0.9790  0.9821  0.9760  0.9773  0.9871  0.9782  0.9830\n",
       " 0.9853  0.9871  0.9801  0.9824  0.9804  0.9677  0.9870  0.9842  0.9805  0.9844\n",
       " 0.9833  0.9861  0.9755  0.9871  0.9849  0.9815  0.9762  0.9896  0.9809  0.9874\n",
       " 0.9978  0.9925  0.9840  0.9818  0.9911  0.9924  0.9908  0.9857  0.9918  0.9911\n",
       " 0.9929  0.9882  0.9847  0.9853  0.9857  0.9858  0.9930  0.9908  0.9923  0.9881\n",
       " 0.9881  0.9908  0.9887  0.9878  0.9877  0.9852  0.9876  0.9875  0.9878  0.9873\n",
       " 0.9941  0.9718  0.9880  0.9866  0.9894  0.9810  0.9894  0.9928  0.9832  0.9922\n",
       " 0.9857  0.9832  0.9874  0.9907  0.9878  0.9881  0.9878  0.9877  0.9826  0.9878\n",
       " 0.9917  0.9776  0.9906  0.9915  0.9870  0.9858  0.9824  0.9867  0.9892  0.9889\n",
       " 0.9926  0.9806  0.9822  0.9721  0.9717  0.9754  0.9817  0.9772  0.9818  0.9768\n",
       " 0.9888  0.9916  0.9767  0.9896  0.9891  0.9891  0.9799  0.9851  0.9910  0.9912\n",
       " 0.9904  0.9853  0.9862  0.9824  0.9775  0.9788  0.9832  0.9874  0.9832  0.9833\n",
       " 0.9882  0.9857  0.9847  0.9908  0.9834  0.9855  0.9839  0.9839  0.9882  0.9851\n",
       " 0.9869  0.9797  0.9802  0.9834  0.9892  0.9775  0.9922  0.9856  0.9932  0.9829\n",
       " 0.9936  0.9886  0.9883  0.9793  0.9771  0.9819  0.9905  0.9857  0.9895  0.9874\n",
       " 0.9854  0.9784  0.9733  0.9772  0.9780  0.9729  0.9887  0.9831  0.9861  0.9847\n",
       " 0.9885  0.9808  0.9821  0.9888  0.9886  0.9861  0.9833  0.9899  0.9807  0.9873\n",
       " 0.9842  0.9899  0.9851  0.9729  0.9902  0.9812  0.9837  0.9799  0.9812  0.9857\n",
       " 0.9853  0.9888  0.9779  0.9866  0.9866  0.9916  0.9811  0.9771  0.9847  0.9886\n",
       " 0.9909  0.9790  0.9820  0.9906  0.9866  0.9908  0.9673  0.9829  0.9872  0.9835\n",
       " 0.9866  0.9818  0.9803  0.9812  0.9809  0.9847  0.9906  0.9926  0.9909  0.9745\n",
       " 0.9815  0.9877  0.9837  0.9788  0.9866  0.9877  0.9877  0.9812  0.9913  0.9811\n",
       " 0.9858  0.9673  0.9847  0.9902  0.9821  0.9809  0.9851  0.9812  0.9890  0.9866\n",
       " 0.9911  0.9729  0.9885  0.9861  0.9897  0.9782  0.9837  0.9857  0.9812  0.9745\n",
       " 0.9903  0.9838  0.9899  0.9748  0.9886  0.9713  0.9820  0.9838  0.9818  0.9748\n",
       " 0.9882  0.9818  0.9872  0.9782  0.9812  0.9861  0.9909  0.9908  0.9873  0.9916\n",
       " 0.9842  0.9752  0.9680  0.9735  0.9854  0.9790  0.9808  0.9716  0.9726  0.9759\n",
       " 0.9915  0.9843  0.9775  0.9790  0.9812  0.9893  0.9862  0.9866  0.9866  0.9809\n",
       " 0.9950  0.9826  0.9882  0.9827  0.9793  0.9836  0.9804  0.9810  0.9793  0.9884\n",
       " 0.9865  0.9839  0.9710  0.9776  0.9841  0.9857  0.9797  0.9875  0.9776  0.9736\n",
       " 0.9956  0.9832  0.9713  0.9912  0.9799  0.9856  0.9938  0.9798  0.9867  0.9828\n",
       " 0.9914  0.9853  0.9910  0.9896  0.9877  0.9839  0.9824  0.9932  0.9916  0.9873\n",
       " 0.9823  0.9819  0.9822  0.9857  0.9762  0.9808  0.9783  0.9827  0.9859  0.9828\n",
       " 0.9927  0.9923  0.9855  0.9806  0.9878  0.9864  0.9898  0.9923  0.9790  0.9872\n",
       "\n",
       "Columns 10 to 19 \n",
       " 0.9871  0.9884  0.9846  0.9868  0.9855  0.9879  0.9884  0.9886  0.9864  0.9892\n",
       " 0.9819  0.9744  0.9797  0.9801  0.9940  0.9867  0.9652  0.9769  0.9825  0.9850\n",
       " 0.9878  0.9796  0.9871  0.9772  0.9796  0.9794  0.9899  0.9790  0.9796  0.9872\n",
       " 0.9782  0.9870  0.9811  0.9772  0.9798  0.9794  0.9897  0.9845  0.9764  0.9835\n",
       " 0.9838  0.9855  0.9809  0.9844  0.9840  0.9851  0.9771  0.9920  0.9783  0.9895\n",
       " 0.9837  0.9914  0.9886  0.9836  0.9745  0.9870  0.9915  0.9818  0.9917  0.9907\n",
       " 0.9838  0.9906  0.9919  0.9866  0.9802  0.9887  0.9832  0.9741  0.9787  0.9827\n",
       " 0.9801  0.9771  0.9771  0.9769  0.9858  0.9782  0.9729  0.9769  0.9754  0.9768\n",
       " 0.9775  0.9831  0.9808  0.9747  0.9785  0.9808  0.9747  0.9750  0.9770  0.9747\n",
       " 0.9708  0.9786  0.9779  0.9766  0.9832  0.9849  0.9662  0.9838  0.9839  0.9799\n",
       " 0.9840  0.9930  0.9871  0.9818  0.9883  0.9901  0.9910  0.9869  0.9840  0.9929\n",
       " 0.9833  0.9882  0.9694  0.9882  0.9863  0.9885  0.9847  0.9882  0.9899  0.9887\n",
       " 0.9866  0.9922  0.9929  0.9913  0.9886  0.9790  0.9852  0.9861  0.9866  0.9813\n",
       " 0.9903  0.9848  0.9848  0.9918  0.9911  0.9826  0.9878  0.9853  0.9894  0.9908\n",
       " 0.9877  0.9877  0.9832  0.9884  0.9902  0.9846  0.9853  0.9735  0.9873  0.9928\n",
       " 0.9951  0.9906  0.9824  0.9828  0.9926  0.9914  0.9903  0.9920  0.9873  0.9920\n",
       " 0.9730  0.9798  0.9780  0.9706  0.9792  0.9819  0.9763  0.9794  0.9730  0.9782\n",
       " 0.9893  0.9862  0.9876  0.9874  0.9871  0.9847  0.9877  0.9846  0.9828  0.9905\n",
       " 0.9877  0.9872  0.9853  0.9877  0.9884  0.9809  0.9872  0.9815  0.9872  0.9804\n",
       " 0.9839  0.9872  0.9825  0.9908  0.9786  0.9771  0.9874  0.9797  0.9837  0.9830\n",
       " 0.9879  0.9897  0.9811  0.9908  0.9874  0.9851  0.9860  0.9893  0.9860  0.9932\n",
       " 0.9886  0.9838  0.9859  0.9814  0.9786  0.9895  0.9874  0.9852  0.9893  0.9882\n",
       " 0.9821  0.9877  0.9812  0.9771  0.9812  0.9837  0.9779  0.9793  0.9810  0.9917\n",
       " 0.9873  0.9918  0.9713  0.9916  0.9897  0.9866  0.9810  0.9835  0.9890  0.9916\n",
       " 0.9851  0.9866  0.9917  0.9755  0.9706  0.9873  0.9835  0.9877  0.9877  0.9868\n",
       " 0.9830  0.9906  0.9713  0.9896  0.9916  0.9830  0.9814  0.9889  0.9745  0.9814\n",
       " 0.9713  0.9861  0.9873  0.9830  0.9820  0.9917  0.9807  0.9888  0.9783  0.9772\n",
       " 0.9812  0.9892  0.9897  0.9713  0.9812  0.9913  0.9871  0.9886  0.9812  0.9851\n",
       " 0.9812  0.9790  0.9851  0.9837  0.9888  0.9748  0.9873  0.9818  0.9818  0.9713\n",
       " 0.9829  0.9816  0.9888  0.9729  0.9868  0.9811  0.9902  0.9818  0.9808  0.9888\n",
       " 0.9888  0.9908  0.9847  0.9863  0.9909  0.9837  0.9771  0.9906  0.9799  0.9812\n",
       " 0.9887  0.9873  0.9783  0.9811  0.9812  0.9801  0.9811  0.9771  0.9748  0.9706\n",
       " 0.9908  0.9916  0.9866  0.9866  0.9908  0.9863  0.9866  0.9808  0.9896  0.9918\n",
       " 0.9761  0.9820  0.9822  0.9761  0.9791  0.9707  0.9766  0.9792  0.9765  0.9762\n",
       " 0.9811  0.9847  0.9808  0.9755  0.9826  0.9705  0.9848  0.9836  0.9729  0.9925\n",
       " 0.9864  0.9729  0.9867  0.9867  0.9884  0.9794  0.9916  0.9790  0.9508  0.9803\n",
       " 0.9852  0.9787  0.9764  0.9864  0.9854  0.9787  0.9829  0.9852  0.9796  0.9826\n",
       " 0.9909  0.9730  0.9743  0.9867  0.9805  0.9874  0.9791  0.9906  0.9879  0.9912\n",
       " 0.9787  0.9899  0.9865  0.9856  0.9896  0.9813  0.9894  0.9865  0.9866  0.9799\n",
       " 0.9860  0.9810  0.9830  0.9824  0.9800  0.9848  0.9840  0.9824  0.9783  0.9868\n",
       " 0.9854  0.9846  0.9893  0.9916  0.9903  0.9870  0.9891  0.9857  0.9876  0.9790\n",
       "\n",
       "Columns 20 to 20 \n",
       " 0.9893\n",
       " 0.9780\n",
       " 0.9779\n",
       " 0.9760\n",
       " 0.9915\n",
       " 0.9877\n",
       " 0.9893\n",
       " 0.9798\n",
       " 0.9782\n",
       " 0.9813\n",
       " 0.9809\n",
       " 0.9861\n",
       " 0.9926\n",
       " 0.9932\n",
       " 0.9898\n",
       " 0.9881\n",
       " 0.9736\n",
       " 0.9881\n",
       " 0.9860\n",
       " 0.9786\n",
       " 0.9795\n",
       " 0.9847\n",
       " 0.9808\n",
       " 0.9829\n",
       " 0.9875\n",
       " 0.9809\n",
       " 0.9847\n",
       " 0.9877\n",
       " 0.9793\n",
       " 0.9830\n",
       " 0.9877\n",
       " 0.9855\n",
       " 0.9899\n",
       " 0.9808\n",
       " 0.9878\n",
       " 0.9775\n",
       " 0.9840\n",
       " 0.9853\n",
       " 0.9917\n",
       " 0.9762\n",
       " 0.9885\n",
       "[torch.FloatTensor of size 41x21]"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = torch.nn.MultiMarginLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.9469\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = Variable(torch.zeros(scores.size(0)).type(torch.LongTensor)) \n",
    "criterion(scores, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
