{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import string\n",
    "import numpy as np; np.random.seed(7)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loadings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word 2 vec repre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_map = {}\n",
    "with open('data/vectors_pruned.200.txt', 'r') as src:\n",
    "    src = src.read().strip().split('\\n')\n",
    "    for line in src:\n",
    "        wv = line.strip().split(' ')\n",
    "        word = wv.pop(0)\n",
    "        w2v_map[word] = np.array(list(map(float, wv)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2i_map = {}\n",
    "for i, key in enumerate(w2v_map.keys()):\n",
    "    w2i_map[key] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pkl.dump(w2i_map, open('data/word_idx_map.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### map Q idx to context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_matrix = np.zeros(( len((w2v_map.keys())), 200 ))\n",
    "counter = 0\n",
    "for _, val in w2v_map.items():\n",
    "    w2v_matrix[counter] = val\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pkl.dump(w2i_map, open('data/w2v_matrix.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def w2v(w):\n",
    "    return w2v_matrix[w2i_map[w]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sen2w(sen):\n",
    "    processed = []\n",
    "    sen = re.sub(r'[!#\\'(),/:?\\{}]', ' ', sen).strip().split()\n",
    "    if len(sen) > 100:\n",
    "        sen = sen[:100]\n",
    "    for w in sen:\n",
    "        # ignore date\n",
    "        if re.match(r'\\d{1,}-\\d{1,}-\\d{1,}', w):\n",
    "            continue\n",
    "        if re.match(r'\\d{1,}:\\d{1,}', w):\n",
    "            continue\n",
    "        if w in w2i_map:\n",
    "            processed += [w]\n",
    "        else:\n",
    "            separated = re.findall(r\"[^\\W\\d_]+|\\d+|[=`%$\\^\\-@;\\[&_*>\\].<~|+\\d+]\", w)\n",
    "            if len(set(separated)) == 1:\n",
    "                continue\n",
    "            if separated.count('*') > 3 or separated.count('=') > 3:\n",
    "                continue\n",
    "            for separate_w in separated:\n",
    "                if separate_w in w2i_map:\n",
    "                    processed += [separate_w]\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fixed context len = 125\n",
    "context_repre = {}\n",
    "with open('data/text_tokenized.txt', 'r') as src:\n",
    "    src = src.read().strip().split('\\n')\n",
    "    for line in src:\n",
    "        context = line.strip().split('\\t')\n",
    "        qid = context.pop(0)\n",
    "        if len(context) == 1:\n",
    "            context_repre[int(qid)] = {'t': sen2w(context[0]), 'b': None}\n",
    "        else:\n",
    "            len_title = len(context[0])\n",
    "            if len_title >= 125:\n",
    "                context_repre[int(qid)] = {'t':sen2w(context[0])[:125], 'b': None}\n",
    "            else:\n",
    "                context_repre[int(qid)] = {'t':sen2w(context[0]), 'b': sen2w(context[1])[:125 - len_title]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### len of context ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_lens = []\n",
    "for k, v in context_repre.items():\n",
    "    t, b = v['t'], v['b']\n",
    "    if not v['b']:\n",
    "        b = []\n",
    "    all_lens += [len(t)+len(b)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 633,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all(np.array(all_lens) < 125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/train_random.txt', header=None, delimiter='\\t', names=['Q','Q+','Q-'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>Q+</th>\n",
       "      <th>Q-</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>262144</td>\n",
       "      <td>211039</td>\n",
       "      <td>227387 413633 113297 356390 256881 145638 2962...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>491522</td>\n",
       "      <td>65911</td>\n",
       "      <td>155119 402211 310669 383107 131731 299465 1633...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>240299</td>\n",
       "      <td>168608 390642</td>\n",
       "      <td>368007 70009 48077 376760 438005 228888 142340...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>196614</td>\n",
       "      <td>205184</td>\n",
       "      <td>334471 163710 376791 441664 159963 406360 4300...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>360457</td>\n",
       "      <td>321532</td>\n",
       "      <td>151863 501857 217578 470017 125838 31836 42066...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Q             Q+                                                 Q-\n",
       "0  262144         211039  227387 413633 113297 356390 256881 145638 2962...\n",
       "1  491522          65911  155119 402211 310669 383107 131731 299465 1633...\n",
       "2  240299  168608 390642  368007 70009 48077 376760 438005 228888 142340...\n",
       "3  196614         205184  334471 163710 376791 441664 159963 406360 4300...\n",
       "4  360457         321532  151863 501857 217578 470017 125838 31836 42066..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_set_pair_with_idx(df):\n",
    "    idx_set = {}\n",
    "    for idx, row in df.iterrows():\n",
    "        idx_set[row['Q']] = {'pos': np.array(list(map(int, row['Q+'].split(' ')))), \\\n",
    "                             'neg': np.array(list(map(int, row['Q-'].split(' '))))}\n",
    "    return idx_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_idx_set = build_set_pair_with_idx(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def contxt2vec(title, body=None):\n",
    "    \n",
    "    if body == None:\n",
    "        body = []\n",
    "    \n",
    "    v = np.zeros( (len(title) + len(body), 200) )\n",
    "    counter = 0\n",
    "    \n",
    "    for t in title:\n",
    "        v[counter] = w2v(t)\n",
    "        counter += 1\n",
    "\n",
    "    for b in body:\n",
    "        v[counter] = w2v(b)\n",
    "        counter += 1\n",
    "    \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_batch(qids, idx_set):\n",
    "    \n",
    "    total_pos_len = 0\n",
    "    for qid in qids:\n",
    "        total_pos_len += len(idx_set[qid]['pos'])\n",
    "    \n",
    "    # per batch element x: vstack, [query_Q x 1; pos_Q x 1; neg_Q x 20]\n",
    "    # per batch element y: vstack, [query_Q=-1; pos_Q=1; neg_Q=0]\n",
    "    batch_x = np.zeros(( total_pos_len * 22, 124, 200 ))\n",
    "    seq_len = np.zeros(total_pos_len * 22)\n",
    "    \n",
    "    counter = 0\n",
    "    for qid in qids:\n",
    "        \n",
    "        q_title, q_body = context_repre[qid]['t'], context_repre[qid]['b']\n",
    "        q_pos = idx_set[qid]['pos']\n",
    "\n",
    "        # usually one sample\n",
    "        for qid_pos in q_pos:\n",
    "            title, body = context_repre[qid_pos]['t'], context_repre[qid_pos]['b']\n",
    "            # query Q\n",
    "            if not q_body:\n",
    "                q_seq_len = len(q_title)\n",
    "            else:\n",
    "                q_seq_len = len(q_title) + len(q_body)\n",
    "            seq_len[counter] = q_seq_len\n",
    "            batch_x[counter, :q_seq_len] = contxt2vec(q_title, q_body)\n",
    "            counter += 1\n",
    "            # pos Q\n",
    "            if not body:\n",
    "                pos_q_seq_len = len(title)\n",
    "            else:\n",
    "                pos_q_seq_len = len(title) + len(body)\n",
    "            seq_len[counter] = pos_q_seq_len\n",
    "            batch_x[counter, :pos_q_seq_len] = contxt2vec(title, body)\n",
    "            counter += 1\n",
    "        \n",
    "            q_neg = idx_set[qid]['neg']\n",
    "            q_neg_sample_indices = np.random.choice(range(100), size=20)\n",
    "            q_random_neg = q_neg[q_neg_sample_indices]\n",
    "            # neg Q\n",
    "            for qid_neg in q_random_neg:\n",
    "                title, body = context_repre[qid_neg]['t'], context_repre[qid_neg]['b']\n",
    "                if not body:\n",
    "                    neg_q_seq_len = len(title)\n",
    "                else:\n",
    "                    neg_q_seq_len = len(title) + len(body)\n",
    "                seq_len[counter] = neg_q_seq_len\n",
    "                batch_x[counter, : neg_q_seq_len] = contxt2vec(title, body)\n",
    "                counter += 1\n",
    "    \n",
    "    return batch_x, seq_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_annotations(path, K_neg=20, prune_pos_cnt=10):\n",
    "    lst = [ ]\n",
    "    with open(path) as fin:\n",
    "        for line in fin:\n",
    "            parts = line.split(\"\\t\")\n",
    "            pid, pos, neg = parts[:3]\n",
    "            pos = pos.split()\n",
    "            neg = neg.split()\n",
    "            if len(pos) == 0 or (len(pos) > prune_pos_cnt and prune_pos_cnt != -1): continue\n",
    "            if K_neg != -1:\n",
    "                np.random.shuffle(neg)\n",
    "                neg = neg[:K_neg]\n",
    "            s = set()\n",
    "            qids = [ ]\n",
    "            qlabels = [ ]\n",
    "            for q in neg:\n",
    "                if q not in s:\n",
    "                    qids.append(q)\n",
    "                    qlabels.append(0 if q not in pos else 1)\n",
    "                    s.add(q)\n",
    "            for q in pos:\n",
    "                if q not in s:\n",
    "                    qids.append(q)\n",
    "                    qlabels.append(1)\n",
    "                    s.add(q)\n",
    "            lst.append((pid, qids, qlabels))\n",
    "\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Evaluation():\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def Precision(self, precision_at):\n",
    "        scores = []\n",
    "        for item in self.data:\n",
    "            temp = item[:precision_at]\n",
    "            if any(val==1 for val in item):\n",
    "                scores.append(sum([1 if val==1 else 0 for val in temp])*1.0 / len(temp) if len(temp) > 0 else 0.0)\n",
    "        return sum(scores)/len(scores) if len(scores) > 0 else 0.0\n",
    "\n",
    "    def MAP(self):\n",
    "        scores = []\n",
    "        missing_MAP = 0\n",
    "        for item in self.data:\n",
    "            temp = []\n",
    "            count = 0.0\n",
    "            for i,val in enumerate(item):\n",
    "                if val == 1:\n",
    "                    count += 1.0\n",
    "                    temp.append(count/(i+1))\n",
    "                if len(temp) > 0:\n",
    "                    scores.append(sum(temp) / len(temp))\n",
    "                else:\n",
    "                    missing_MAP += 1\n",
    "        return sum(scores)/len(scores) if len(scores) > 0 else 0.0\n",
    "\n",
    "    def MRR(self):\n",
    "        scores = []\n",
    "        for item in self.data:\n",
    "            for i,val in enumerate(item):\n",
    "                if val == 1:\n",
    "                    scores.append(1.0/(i+1))\n",
    "                    break\n",
    "\n",
    "        return sum(scores)/len(scores) if len(scores) > 0 else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EmbeddingLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, layer_type):\n",
    "        \n",
    "        super(EmbeddingLayer, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        if layer_type == 'lstm':\n",
    "            self.embedding_layer = nn.LSTM(input_size, hidden_size)\n",
    "        elif layer_type == 'cnn':\n",
    "            pass\n",
    "        \n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return (Variable(torch.zeros(batch_size, 1, self.hidden_size)), Variable(torch.zeros(batch_size, 1, self.hidden_size)))\n",
    "\n",
    "    def forward(self, context, seq_len):\n",
    "        lstm_out, self.hidden = self.embedding_layer(context, (self.tanh(self.hidden[0]), self.tanh(self.hidden[1])))\n",
    "        mask = build_mask(seq_len)\n",
    "        embeddings = torch.sum(lstm_out * Variable(torch.FloatTensor(mask)), dim=1) \\\n",
    "            / Variable(torch.FloatTensor(np.sum(mask, axis=1)))\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cos_sim(qv, qv_):\n",
    "    return torch.sum(qv * qv_, dim=1) / (torch.sqrt(torch.sum(qv ** 2, dim=1)) * torch.sqrt(torch.sum(qv_ ** 2, dim=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def criterion(embeddings):\n",
    "    \n",
    "    # a batch of embeddings\n",
    "    num_block = embeddings.size()[0] // 22\n",
    "    loss = 0\n",
    "    for i in range(num_block):\n",
    "        block_embeddings = embeddings[ i * 22: (i + 1) * 22 ]\n",
    "        qs = block_embeddings[0]\n",
    "        qs_ = block_embeddings[1:22]\n",
    "        pos_score = cos_sim(qs.expand(21, 240), qs_)[0]\n",
    "        neg_score = torch.max(cos_sim(qs.expand(21, 240), qs_)[1:])\n",
    "        diff = neg_score - pos_score + 1 # margin=1\n",
    "        if diff.data[0] > 0:\n",
    "            loss += diff\n",
    "            \n",
    "    return loss / num_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_mask(seq_len):\n",
    "    mask = []\n",
    "    for i, s in enumerate(seq_len):\n",
    "        s_mask = np.zeros((124, 1))\n",
    "        s_mask[:int(s)] = np.ones((int(s), 1))\n",
    "        mask += [s_mask]\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(layer_type, batch_size=64, num_epoch=100):\n",
    "    \n",
    "    if layer_type == 'lstm':\n",
    "        embedding_layer = EmbeddingLayer(200, 240, 'lstm')\n",
    "    elif layer_type == 'cnn':\n",
    "        pass\n",
    "        \n",
    "    optimizer = torch.optim.Adam(embedding_layer.parameters(), lr=0.005)\n",
    "    \n",
    "    qids = list(train_idx_set.keys())\n",
    "    num_batch = len(qids) // batch_size\n",
    "    \n",
    "    for epoch in range(1, num_epoch + 1):\n",
    "        \n",
    "        for batch_idx in range(1, num_batch + 1):\n",
    "            \n",
    "            batch_x_qids = qids[batch_idx * batch_size: (batch_idx + 1) * batch_size]\n",
    "            start = time.time()\n",
    "            print ('processing batch {}'.format(batch_idx))\n",
    "            padded_batch_x, seq_len = process_batch(batch_x_qids, train_idx_set)\n",
    "            print ('processing batch costs:', time.time() - start)\n",
    "            embedding_layer.hidden = embedding_layer.init_hidden(padded_batch_x.shape[0])\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            start = time.time()\n",
    "            qs = Variable(torch.FloatTensor(padded_batch_x))\n",
    "            embeddings = embedding_layer(qs, seq_len)\n",
    "            print ('embedding costs:', time.time() - start)\n",
    "            start = time.time()\n",
    "            \n",
    "            print ('accumulating loss costs:', time.time() - start)\n",
    "            loss = criterion(embeddings)\n",
    "            print ('-------------------------------')\n",
    "            print ('epoch:{}/{}, batch:{}/{}, loss:{}'.format(epoch, num_epoch, batch_idx, num_batch, loss.data[0]))\n",
    "            print ('-------------------------------')\n",
    "            start = time.time()\n",
    "            loss.backward()\n",
    "            print ('backprop costs:', time.time() - start)\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing batch 1\n",
      "processing batch costs: 0.830629825592041\n",
      "embedding costs: 14.15979790687561\n",
      "accumulating loss costs: 0.0\n",
      "-------------------------------\n",
      "epoch:1/1, batch:1/198, loss:0.999708354473114\n",
      "-------------------------------\n",
      "backprop costs: 21.249325275421143\n",
      "processing batch 2\n",
      "processing batch costs: 1.7010469436645508\n",
      "embedding costs: 14.345101833343506\n",
      "accumulating loss costs: 9.5367431640625e-07\n",
      "-------------------------------\n",
      "epoch:1/1, batch:2/198, loss:0.9993116855621338\n",
      "-------------------------------\n",
      "backprop costs: 17.478061199188232\n",
      "processing batch 3\n",
      "processing batch costs: 1.0079529285430908\n",
      "embedding costs: 9.926378965377808\n",
      "accumulating loss costs: 9.5367431640625e-07\n",
      "-------------------------------\n",
      "epoch:1/1, batch:3/198, loss:0.9964604377746582\n",
      "-------------------------------\n",
      "backprop costs: 14.163267135620117\n",
      "processing batch 4\n",
      "processing batch costs: 1.2058498859405518\n",
      "embedding costs: 20.66390609741211\n",
      "accumulating loss costs: 9.5367431640625e-07\n",
      "-------------------------------\n",
      "epoch:1/1, batch:4/198, loss:0.9942623972892761\n",
      "-------------------------------\n",
      "backprop costs: 43.88836884498596\n",
      "processing batch 5\n",
      "processing batch costs: 1.691204309463501\n",
      "embedding costs: 15.34352707862854\n",
      "accumulating loss costs: 1.1920928955078125e-06\n",
      "-------------------------------\n",
      "epoch:1/1, batch:5/198, loss:0.9917752146720886\n",
      "-------------------------------\n",
      "backprop costs: 26.043922185897827\n",
      "processing batch 6\n",
      "processing batch costs: 1.6226890087127686\n",
      "embedding costs: 38.78587508201599\n",
      "accumulating loss costs: 9.5367431640625e-07\n",
      "-------------------------------\n",
      "epoch:1/1, batch:6/198, loss:0.9770793914794922\n",
      "-------------------------------\n",
      "backprop costs: 65.93780183792114\n",
      "processing batch 7\n",
      "processing batch costs: 1.8173749446868896\n",
      "embedding costs: 10.76349687576294\n",
      "accumulating loss costs: 9.5367431640625e-07\n",
      "-------------------------------\n",
      "epoch:1/1, batch:7/198, loss:0.9589095115661621\n",
      "-------------------------------\n",
      "backprop costs: 14.704532861709595\n",
      "processing batch 8\n",
      "processing batch costs: 1.0809593200683594\n",
      "embedding costs: 10.868113994598389\n",
      "accumulating loss costs: 9.5367431640625e-07\n",
      "-------------------------------\n",
      "epoch:1/1, batch:8/198, loss:0.9527587890625\n",
      "-------------------------------\n",
      "backprop costs: 17.74914002418518\n",
      "processing batch 9\n",
      "processing batch costs: 0.655864953994751\n",
      "embedding costs: 11.175625801086426\n",
      "accumulating loss costs: 1.1920928955078125e-06\n",
      "-------------------------------\n",
      "epoch:1/1, batch:9/198, loss:0.95338374376297\n",
      "-------------------------------\n",
      "backprop costs: 14.934036016464233\n",
      "processing batch 10\n",
      "processing batch costs: 0.6302359104156494\n",
      "embedding costs: 8.707222700119019\n",
      "accumulating loss costs: 0.0\n",
      "-------------------------------\n",
      "epoch:1/1, batch:10/198, loss:0.9570054411888123\n",
      "-------------------------------\n",
      "backprop costs: 15.71939492225647\n",
      "processing batch 11\n",
      "processing batch costs: 0.6783020496368408\n",
      "embedding costs: 7.456454038619995\n",
      "accumulating loss costs: 9.5367431640625e-07\n",
      "-------------------------------\n",
      "epoch:1/1, batch:11/198, loss:0.9466806650161743\n",
      "-------------------------------\n",
      "backprop costs: 12.728395938873291\n",
      "processing batch 12\n",
      "processing batch costs: 0.5661449432373047\n",
      "embedding costs: 10.564661026000977\n",
      "accumulating loss costs: 9.5367431640625e-07\n",
      "-------------------------------\n",
      "epoch:1/1, batch:12/198, loss:0.9515613913536072\n",
      "-------------------------------\n",
      "backprop costs: 16.36846399307251\n",
      "processing batch 13\n",
      "processing batch costs: 0.556981086730957\n",
      "embedding costs: 9.228953123092651\n",
      "accumulating loss costs: 9.5367431640625e-07\n",
      "-------------------------------\n",
      "epoch:1/1, batch:13/198, loss:0.9250264167785645\n",
      "-------------------------------\n",
      "backprop costs: 10.375940084457397\n",
      "processing batch 14\n",
      "processing batch costs: 0.6103420257568359\n",
      "embedding costs: 14.866384744644165\n",
      "accumulating loss costs: 9.5367431640625e-07\n",
      "-------------------------------\n",
      "epoch:1/1, batch:14/198, loss:0.886458694934845\n",
      "-------------------------------\n",
      "backprop costs: 20.19615888595581\n",
      "processing batch 15\n",
      "processing batch costs: 0.73897385597229\n",
      "embedding costs: 7.07091498374939\n",
      "accumulating loss costs: 9.5367431640625e-07\n",
      "-------------------------------\n",
      "epoch:1/1, batch:15/198, loss:0.8736528754234314\n",
      "-------------------------------\n",
      "backprop costs: 8.405752182006836\n",
      "processing batch 16\n",
      "processing batch costs: 0.5263972282409668\n",
      "embedding costs: 8.970813035964966\n",
      "accumulating loss costs: 0.0\n",
      "-------------------------------\n",
      "epoch:1/1, batch:16/198, loss:0.8877140879631042\n",
      "-------------------------------\n",
      "backprop costs: 9.160334825515747\n",
      "processing batch 17\n",
      "processing batch costs: 0.856935977935791\n",
      "embedding costs: 11.133815050125122\n",
      "accumulating loss costs: 9.5367431640625e-07\n",
      "-------------------------------\n",
      "epoch:1/1, batch:17/198, loss:0.8553863167762756\n",
      "-------------------------------\n",
      "backprop costs: 12.694546222686768\n",
      "processing batch 18\n",
      "processing batch costs: 0.9567840099334717\n",
      "embedding costs: 17.807588815689087\n",
      "accumulating loss costs: 9.5367431640625e-07\n",
      "-------------------------------\n",
      "epoch:1/1, batch:18/198, loss:0.8655185699462891\n",
      "-------------------------------\n",
      "backprop costs: 19.22684907913208\n",
      "processing batch 19\n",
      "processing batch costs: 0.7306549549102783\n",
      "embedding costs: 11.05019211769104\n",
      "accumulating loss costs: 0.0\n",
      "-------------------------------\n",
      "epoch:1/1, batch:19/198, loss:0.8599542379379272\n",
      "-------------------------------\n",
      "backprop costs: 10.21573805809021\n",
      "processing batch 20\n",
      "processing batch costs: 0.77227783203125\n",
      "embedding costs: 7.3091959953308105\n",
      "accumulating loss costs: 9.5367431640625e-07\n",
      "-------------------------------\n",
      "epoch:1/1, batch:20/198, loss:0.8114031553268433\n",
      "-------------------------------\n",
      "backprop costs: 6.595167875289917\n",
      "processing batch 21\n",
      "processing batch costs: 0.5358879566192627\n",
      "embedding costs: 10.264069080352783\n",
      "accumulating loss costs: 0.0\n",
      "-------------------------------\n",
      "epoch:1/1, batch:21/198, loss:0.7911403775215149\n",
      "-------------------------------\n",
      "backprop costs: 11.995519876480103\n",
      "processing batch 22\n",
      "processing batch costs: 0.50596022605896\n",
      "embedding costs: 7.241318941116333\n",
      "accumulating loss costs: 0.0\n",
      "-------------------------------\n",
      "epoch:1/1, batch:22/198, loss:0.8348995447158813\n",
      "-------------------------------\n",
      "backprop costs: 8.77648401260376\n",
      "processing batch 23\n",
      "processing batch costs: 0.7921130657196045\n",
      "embedding costs: 22.954845905303955\n",
      "accumulating loss costs: 9.5367431640625e-07\n",
      "-------------------------------\n",
      "epoch:1/1, batch:23/198, loss:0.9148140549659729\n",
      "-------------------------------\n",
      "backprop costs: 31.905334949493408\n",
      "processing batch 24\n",
      "processing batch costs: 1.9795849323272705\n",
      "embedding costs: 15.70223593711853\n",
      "accumulating loss costs: 9.5367431640625e-07\n",
      "-------------------------------\n",
      "epoch:1/1, batch:24/198, loss:0.8680198788642883\n",
      "-------------------------------\n",
      "backprop costs: 16.105175971984863\n",
      "processing batch 25\n",
      "processing batch costs: 0.8507118225097656\n",
      "embedding costs: 10.316379070281982\n",
      "accumulating loss costs: 4.0531158447265625e-06\n",
      "-------------------------------\n",
      "epoch:1/1, batch:25/198, loss:0.8417959213256836\n",
      "-------------------------------\n",
      "backprop costs: 15.723248720169067\n",
      "processing batch 26\n",
      "processing batch costs: 0.8501098155975342\n",
      "embedding costs: 19.90740418434143\n",
      "accumulating loss costs: 9.5367431640625e-07\n",
      "-------------------------------\n",
      "epoch:1/1, batch:26/198, loss:0.8022318482398987\n",
      "-------------------------------\n",
      "backprop costs: 24.811227798461914\n",
      "processing batch 27\n",
      "processing batch costs: 1.758493185043335\n",
      "embedding costs: 77.5648410320282\n",
      "accumulating loss costs: 1.1920928955078125e-06\n",
      "-------------------------------\n",
      "epoch:1/1, batch:27/198, loss:0.8204713463783264\n",
      "-------------------------------\n",
      "backprop costs: 63.51009678840637\n",
      "processing batch 28\n",
      "processing batch costs: 2.308095932006836\n",
      "embedding costs: 8.737639904022217\n",
      "accumulating loss costs: 0.0\n",
      "-------------------------------\n",
      "epoch:1/1, batch:28/198, loss:0.8064692616462708\n",
      "-------------------------------\n",
      "backprop costs: 10.733599185943604\n",
      "processing batch 29\n",
      "processing batch costs: 2.2256550788879395\n",
      "embedding costs: 19.170889139175415\n",
      "accumulating loss costs: 1.1920928955078125e-06\n",
      "-------------------------------\n",
      "epoch:1/1, batch:29/198, loss:0.8060630559921265\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backprop costs: 23.66404891014099\n",
      "processing batch 30\n",
      "processing batch costs: 1.5806190967559814\n",
      "embedding costs: 12.456974983215332\n",
      "accumulating loss costs: 0.0\n",
      "-------------------------------\n",
      "epoch:1/1, batch:30/198, loss:0.8005266785621643\n",
      "-------------------------------\n",
      "backprop costs: 16.482194185256958\n",
      "processing batch 31\n",
      "processing batch costs: 0.9081857204437256\n",
      "embedding costs: 14.906774997711182\n",
      "accumulating loss costs: 3.0994415283203125e-06\n",
      "-------------------------------\n",
      "epoch:1/1, batch:31/198, loss:0.7624027729034424\n",
      "-------------------------------\n",
      "backprop costs: 16.6115620136261\n",
      "processing batch 32\n",
      "processing batch costs: 1.6157338619232178\n",
      "embedding costs: 14.416923999786377\n",
      "accumulating loss costs: 1.9073486328125e-06\n",
      "-------------------------------\n",
      "epoch:1/1, batch:32/198, loss:0.7573146224021912\n",
      "-------------------------------\n",
      "backprop costs: 12.165377140045166\n",
      "processing batch 33\n",
      "processing batch costs: 0.9758009910583496\n",
      "embedding costs: 9.29592490196228\n",
      "accumulating loss costs: 0.0\n",
      "-------------------------------\n",
      "epoch:1/1, batch:33/198, loss:0.7606881260871887\n",
      "-------------------------------\n",
      "backprop costs: 13.515182971954346\n",
      "processing batch 34\n",
      "processing batch costs: 1.543529987335205\n",
      "embedding costs: 53.82455086708069\n",
      "accumulating loss costs: 1.1920928955078125e-06\n",
      "-------------------------------\n",
      "epoch:1/1, batch:34/198, loss:0.7542509436607361\n",
      "-------------------------------\n",
      "backprop costs: 55.89412975311279\n",
      "processing batch 35\n",
      "processing batch costs: 2.435828924179077\n",
      "embedding costs: 11.114928960800171\n",
      "accumulating loss costs: 9.5367431640625e-07\n",
      "-------------------------------\n",
      "epoch:1/1, batch:35/198, loss:0.7751114368438721\n",
      "-------------------------------\n",
      "backprop costs: 10.630244970321655\n",
      "processing batch 36\n",
      "processing batch costs: 1.6022138595581055\n",
      "embedding costs: 11.570871591567993\n",
      "accumulating loss costs: 1.1920928955078125e-06\n",
      "-------------------------------\n",
      "epoch:1/1, batch:36/198, loss:0.804598331451416\n",
      "-------------------------------\n",
      "backprop costs: 9.527941942214966\n",
      "processing batch 37\n",
      "processing batch costs: 1.1529781818389893\n",
      "embedding costs: 21.813347101211548\n",
      "accumulating loss costs: 7.152557373046875e-07\n",
      "-------------------------------\n",
      "epoch:1/1, batch:37/198, loss:0.7395151853561401\n",
      "-------------------------------\n",
      "backprop costs: 22.191646099090576\n",
      "processing batch 38\n",
      "processing batch costs: 1.8603756427764893\n",
      "embedding costs: 62.590384006500244\n",
      "accumulating loss costs: 7.152557373046875e-07\n",
      "-------------------------------\n",
      "epoch:1/1, batch:38/198, loss:0.8805357217788696\n",
      "-------------------------------\n",
      "backprop costs: 57.181419134140015\n",
      "processing batch 39\n",
      "processing batch costs: 2.531749725341797\n",
      "embedding costs: 31.15703797340393\n",
      "accumulating loss costs: 0.0\n",
      "-------------------------------\n",
      "epoch:1/1, batch:39/198, loss:0.7883772253990173\n",
      "-------------------------------\n",
      "backprop costs: 36.56443524360657\n",
      "processing batch 40\n",
      "processing batch costs: 3.280236005783081\n",
      "embedding costs: 93.47957396507263\n",
      "accumulating loss costs: 5.9604644775390625e-06\n",
      "-------------------------------\n",
      "epoch:1/1, batch:40/198, loss:0.8459410071372986\n",
      "-------------------------------\n",
      "backprop costs: 82.22426223754883\n",
      "processing batch 41\n",
      "processing batch costs: 1.6178770065307617\n",
      "embedding costs: 11.108144044876099\n",
      "accumulating loss costs: 9.5367431640625e-07\n",
      "-------------------------------\n",
      "epoch:1/1, batch:41/198, loss:0.7779085636138916\n",
      "-------------------------------\n",
      "backprop costs: 10.52247405052185\n",
      "processing batch 42\n",
      "processing batch costs: 1.9360623359680176\n",
      "embedding costs: 12.21970510482788\n",
      "accumulating loss costs: 0.0\n",
      "-------------------------------\n",
      "epoch:1/1, batch:42/198, loss:0.7802762985229492\n",
      "-------------------------------\n",
      "backprop costs: 9.762087345123291\n",
      "processing batch 43\n",
      "processing batch costs: 2.5077059268951416\n",
      "embedding costs: 75.44351410865784\n",
      "accumulating loss costs: 9.5367431640625e-07\n",
      "-------------------------------\n",
      "epoch:1/1, batch:43/198, loss:0.7533915638923645\n",
      "-------------------------------\n",
      "backprop costs: 90.79126405715942\n",
      "processing batch 44\n",
      "processing batch costs: 2.0894100666046143\n",
      "embedding costs: 9.257567167282104\n",
      "accumulating loss costs: 1.1920928955078125e-06\n",
      "-------------------------------\n",
      "epoch:1/1, batch:44/198, loss:0.7537009119987488\n",
      "-------------------------------\n",
      "backprop costs: 9.441864013671875\n",
      "processing batch 45\n",
      "processing batch costs: 2.0637059211730957\n",
      "embedding costs: 25.489181995391846\n",
      "accumulating loss costs: 9.5367431640625e-07\n",
      "-------------------------------\n",
      "epoch:1/1, batch:45/198, loss:0.732707142829895\n",
      "-------------------------------\n",
      "backprop costs: 30.79476571083069\n",
      "processing batch 46\n",
      "processing batch costs: 1.7770359516143799\n",
      "embedding costs: 8.25810718536377\n",
      "accumulating loss costs: 9.5367431640625e-07\n",
      "-------------------------------\n",
      "epoch:1/1, batch:46/198, loss:0.7302281260490417\n",
      "-------------------------------\n",
      "backprop costs: 10.866809844970703\n",
      "processing batch 47\n",
      "processing batch costs: 0.8387598991394043\n",
      "embedding costs: 10.481457948684692\n",
      "accumulating loss costs: 1.1920928955078125e-06\n",
      "-------------------------------\n",
      "epoch:1/1, batch:47/198, loss:0.740914523601532\n",
      "-------------------------------\n",
      "backprop costs: 13.526196002960205\n",
      "processing batch 48\n",
      "processing batch costs: 0.7859659194946289\n",
      "embedding costs: 8.148698091506958\n",
      "accumulating loss costs: 0.0\n",
      "-------------------------------\n",
      "epoch:1/1, batch:48/198, loss:0.7519862055778503\n",
      "-------------------------------\n",
      "backprop costs: 7.783859014511108\n",
      "processing batch 49\n",
      "processing batch costs: 0.9758238792419434\n",
      "embedding costs: 7.59058690071106\n",
      "accumulating loss costs: 9.5367431640625e-07\n",
      "-------------------------------\n",
      "epoch:1/1, batch:49/198, loss:0.7229378819465637\n",
      "-------------------------------\n",
      "backprop costs: 10.450732707977295\n",
      "processing batch 50\n",
      "processing batch costs: 0.8300869464874268\n",
      "embedding costs: 10.735196113586426\n",
      "accumulating loss costs: 1.1920928955078125e-06\n",
      "-------------------------------\n",
      "epoch:1/1, batch:50/198, loss:0.7161046862602234\n",
      "-------------------------------\n",
      "backprop costs: 12.549033880233765\n",
      "processing batch 51\n",
      "processing batch costs: 1.1614410877227783\n",
      "embedding costs: 19.17154574394226\n",
      "accumulating loss costs: 0.0\n",
      "-------------------------------\n",
      "epoch:1/1, batch:51/198, loss:0.7247236967086792\n",
      "-------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-65754f1aca40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'lstm'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-68-4f8eab6c5a5c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(layer_type, batch_size, num_epoch)\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'-------------------------------'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'backprop costs:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \"\"\"\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 98\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train('lstm', num_epoch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
